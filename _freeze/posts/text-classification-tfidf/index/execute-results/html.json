{
  "hash": "a12c63019a08c2f045c3d7d4539bb633",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"How Machines Comprehend Language<br><sub>Bonus: Text-Classification</sub>\"\nauthor: \"Jonathan Dekermanjian\"\ndate: \"2025-06-28\"\ncategories: [Natural Language Processing]\ncode-annotations: below\njupyter: python3\nexecute: \n  cache: true\n---\n\n# Overview\n\nWe expand on the hands-on example of the last post and show how we can leverage TF-IDF sparse embeddings to build a multinomial logistic regression classifier.\n\n# Introduction\n\nIn the previous [post](/posts/embeddings_deepdive/index.qmd) we created TF-IDF embeddings using a toy dataset and with the help of some dimensionality reduction plotted our embeddings in three-dimensional space. We showed that the TF-IDF embeddings held enough information within them to allow us to distinguish between the article classes.\n\nIn this post we are going to build a classifier using the Bayesian framework to classify an out-of-sample subset of our texts and we are going to also inspect which words are most influential in predicting class membership. \n\nFinally, we are also going to go over some important assumptions that multiclass classification inherently make and look at ways to get around these limitations.\n\n# Multinomial Logistic Regression\n\nMultinomial Logistic Regression (MLR) is the generalization of Logistic Regression to $k$ classes. The goal of MLR is to predict the probability that a sample $i$ belongs to some class $k$. More formally, suppose we have $y \\in \\{0, 1, ... K - 1\\}$ where $K$ are the total number of **mutually exclusive** classes. \n\nWe want to estimate $P(y = k|\\mathbf{x})$. Where $\\mathbf{x}$ is our input features in $\\mathbf{x} \\in \\mathbb{R}^{D}$ for one observation.\n\nWe condition our probability estimates for each class $k$ on a linear function of the input features \n$$\nlogits_{k} = \\alpha_{k} + \\mathbf{x}^{T}\\pmb{\\beta}_{k}\n$$ \n\nand transform the outputs to probability space, $P(y = k | \\mathbf{x}) \\in (0, 1)$, by applying the softmax function\n\n$$\nP(y = k | \\mathbf{x}) = \\frac{exp(logits_{k})}{\\sum_{j=0}^{K-1}exp(logits_{j})}\n$$\n\nThe softmax ensures that the sum of the class probabilities sum to one $\\sum_{k} P(y = k | \\mathbf{x}) = 1$ this property also makes the softmax invariant to additive shifts. This means the softmax applied to our logits vector \n\n$$\n\\textbf{logits} = \\begin{bmatrix} logits_{0} && logits_{1} & ... & logits_{k-1} \\end{bmatrix}\n$$\n\nplus some constant $c$ results in the same solution\n\n$$\nsoftmax(\\textbf{logits}) = softmax(\\textbf{logits} + c)\n$$\n\nbecause we have multiple solutions the model parameters are not identifiable. In order to induce identifiability we need to either\n\n1. Constrain one class as a reference class $\\alpha_{k} = 0$ and $\\mathbf{\\beta}_{k} = 0$ or\n2. Constrain the parameters of all classes to sum to zero $\\sum_{k} \\mathbf{\\beta}_{k} = 0$ \n\n:::{.callout-tip}\n\nIn this post we use the sum to zero constrain as it makes it simpler to inspect the weights of all the classes. If you constrain the model using the reference class approach you will need to further compute the weights of the reference class post model fitting.\n\n:::\n\nThe likelihood function for a single observation is just the predicted probability for the true class\n\n$$\nP(y_{i} | \\hat{\\textbf{y}_{i}}) = \\hat{y}_{y_{i}}\n$$\n\nwhere\n\n$$\n\\hat{\\textbf{y}_{i}} = \\begin{bmatrix} \\hat{y}_{i0} && \\hat{y}_{i1} && ... \\hat{y}_{iK-1} \\end{bmatrix}\n$$\n\nif we represent $y_{i}$ as a one-hot encoded vector where all the entries are zero except for the entry representing the true class then we have\n\n$$\nP(y_{i} | \\hat{\\textbf{y}_{i}}) = \\prod_{k=0}^{K-1}(\\hat{y}_{ik})^{y_{ik}}\n$$\n\nthis is simpler to represent as the log-likelihood\n\n$$\nlogP(y_{i} | \\hat{\\textbf{y}_{i}}) = \\sum_{k=0}^{K-1}y_{ik} \\cdot log\\hat{y}_{ik}\n$$\n\nand because the one-hot vector representation of $y_{i}$ for all classes not the correct class is zero, we can simplify our log-likelihood to \n\n$$\nlogP(y_{i} | \\hat{\\textbf{y}_{i}}) = log\\hat{y}_{ic}\n$$\n\nwhere $c$ here represents the correct class.\n\nBefore wrapping up this section, I want to go over an important property of MLR that is often overlooked and that is the Independence of Irrelevant Alternatives (IIA). What IIA posits is that the odds ratio between two class choices are independent of other \"irrelevant\" alternatives. Mathematically this says\n\n$$\n\\frac{P(y = i | \\mathbf{x})}{P(y = j | \\mathbf{x})} = exp(logits_{i} - logits_{j})\n$$\n\nwhich in many cases might not be realistic. \n\nFor example, say you are trying to predict the probabilities for three different candidates of winning an election. IIA implies that the odds ratio between any two candidates is independent of the third. However, we know that this can't be true due to the nature of elections as a vote for a particular candidate takes away probability of winning from another. \n\n# Sparse Matrix Multiplication\n\nComing back to the classification problem at hand. Our input features are the TF-IDF embeddings that we generated in the previous post. Now, we could just use it as is, however, that would be extremely inefficient because most of the matrix multiplications will be zeros. \n\nInstead of dense matrix vector multiplication which is defined as\n\n$$\ny_{i} = \\sum_{j=1}^{n}\\textbf{A}_{ij}x_{j}\n$$\n\nwe are going to use sparse matrix vector multiplication defined as\n\n$$\ny_{i} = \\sum_{j \\in NZ_{i}}^{n}\\textbf{A}_{ij}x_{j}\n$$\n\nwhere $NZ_{i}$ is the set of column indexes where $A_{ij} \\ne 0$. Essentially, we skip the zero multiplications and summations. \n\nIn practice, however, we often represent sparse matrices in compressed sparse row (CSR) format or (CSC) for compressed columns. Since we are going to be working with CSR let's look at how that format is set up and then we can talk about how matrix vector multiplication works with CSR.\n\nThe CSR format takes a matrix $A \\in \\mathbb{R}^{m \\times n}$ and breaks it apart into three one-dimensional vectors\n\n1. Data: This is a vector of all the non-zeros elements in the matrix entered starting from the top-left of the matrix working to the bottom-right\n2. Indices: This is a vector that maps to column indices in the original matrix for each non-zero element in our data vector\n3. Index Pointer (Indptr): This is a vector that stores information about the row boundaries (when a row starts and stops)\n\nLet's take a look at an example:\n\nLet \n\n$$\nA = \\begin{bmatrix} \n    0 && 0 && 1 && 0 \\\\\n    4 && 0 && 2 && 0 \\\\\n    0 && 2 && 0 && 0\n    \\end{bmatrix}\n$$\n\nTurning this into CSR format we have\n\n$$\ndata = \\begin{bmatrix}\n       1 && 4 && 2 && 2\n       \\end{bmatrix}\n$$\n\n$$\nindices = \\begin{bmatrix}\n           2 && 0 && 2 && 1\n           \\end{bmatrix}\n$$\n\nand \n\n$$\nindptr = \\begin{bmatrix}\n          0 && 1 && 3 && 4\n          \\end{bmatrix}\n$$\n\nLet's discuss how `indptr` is constructed because that is not clear just by looking at it.\n\nWe scan through the sparse matrix row-wise and add the number of non-zero elements starting at index zero. Then that value becomes your next starting position. \n\nSo in our example starting at the first row\n\n$$\n\\text{initial index} = 0\n$$\n\n$$\n\\text{first row number non-zero} = 1\n$$\n\n$$\n\\text{next index} = 0 + 1\n$$\n\nThe second row we now have \n\n$$\n\\text{current index} = 1\n$$\n\n$$\n\\text{second row number non-zero} = 2\n$$\n\n$$\n\\text{next index} = 2 + 1 = 3\n$$\n\nFinally, the third and last row we have \n\n$$\n\\text{current index} = 3\n$$\n\n$$\n\\text{third row number non-zero} = 1\n$$\n\n$$\n\\text{next index} = 3 + 1 = 4\n$$\n\nwhich results in \n\n$$\nindptr = \\begin{bmatrix}\n          0 && 1 && 3 && 4\n          \\end{bmatrix}\n$$\n\nalright, then! Let's walk through how we can perform matrix vector multiplication using a CSR matrix.\n\nLet's re-use our $\\textbf{A}$ from above and let's say our vector is\n\n$$\n\\textbf{x} = \\begin{bmatrix}\n              1 \\\\\n              2 \\\\\n              3 \\\\\n              4 \\\\\n             \\end{bmatrix}\n$$\n\nand we want to compute\n\n$$\ny = \\textbf{A} \\cdot \\textbf{x}\n$$\n\nwe loop over the rows $i$ in $\\textbf{A}$ computing\n\n$$\ny_{i} = \\sum_{k=\\text{indptr[i]}}^{\\text{indptr[i + 1] - 1}} \\text{data[k]} \\cdot \\textbf{x}\\text{[indices[k]]}\n$$\n\nLet's walk through it step by step:\n\n---\n\nwe start with $\\colorbox{blue}{\\color{white}{i = 0}}$ \n\nwhich gives us $\\colorbox{blue}{\\color{white}{k = indptr[i=0] = 0}}$ \n\nand $\\colorbox{blue}{\\color{white}{indptr[0 + 1] - 1 = 1 - 1 = 0}}$ \n\nthis means our summation for $\\colorbox{blue}{\\color{white}{i=0}}$ \n\nis $\\bbox[blue]{\\color{white}{\\sum_{k=0}^{0}}}$ \n\njust one element. \n\nWith $\\colorbox{blue}{\\color{white}{k=0}}$ we have \n\n$$\n\\colorbox{blue}{\\color{white}{data[k=0] = 1}}\n$$ \n\nand \n\n$$\n\\colorbox{blue}{\\color{white}{\\textbf{x}[indices[k=0]] = x[2] = 3}}\n$$ \n\nmultiplying them together we have $\\bbox[blue]{\\color{white}{1 \\times 3 = 3}}$ for the first element in our resultant vector.\n\n---\n\nNext, we increment $\\colorbox{blue}{\\color{white}{i = 1}}$ \n\nwhich gives us $\\colorbox{blue}{\\color{white}{k = indptr[i=1] = 1}}$ \n\nand $\\colorbox{blue}{\\color{white}{indptr[1 + 1] - 1 = 3 - 1 = 2}}$ \n\nand now our sum is $\\bbox[blue]{\\color{white}{\\sum_{k=1}^{2}}}$ \n\nwhich implies we have two elements in our summation. \n\nFirst, for $\\colorbox{blue}{\\color{white}{k=1}}$ we have \n\n$$\n\\colorbox{blue}{\\color{white}{data[k=1] = 4}}\n$$ \n\nand \n\n$$\n\\colorbox{blue}{\\color{white}{\\textbf{x}[indices[k=1]] = x[0] = 1}}\n$$ \n\nmultiplying them together we have $\\bbox[blue]{\\color{white}{4 \\times 1 = 4}}$. \n\nSecond, for $\\colorbox{blue}{\\color{white}{k=2}}$ we have \n\n$$\n\\colorbox{blue}{\\color{white}{data[k=2] = 2}}\n$$ \n\nand \n\n$$\n\\colorbox{blue}{\\color{white}{\\textbf{x}[indices[k=2]] = x[2] = 3}}\n$$ \n\nmultiplying them together we have $\\bbox[blue]{\\color{white}{2 \\times 3 = 6}}$ and our second element in our resultant vector is  $\\colorbox{blue}{\\color{white}{4 + 6 = 10}}$\n\n---\n\nFinally, we increment $\\colorbox{blue}{\\color{white}{i = 2}}$ \n\nwhich gives us $\\colorbox{blue}{\\color{white}{k = indptr[i=2] = 3}}$ \n\nand $\\colorbox{blue}{\\color{white}{indptr[2 + 1] - 1 = 4 - 1 = 3}}$ \n\nand now our sum is $\\bbox[blue]{\\color{white}{\\sum_{k=3}^{3}}}$ \n\nwhich implies we have one element in our summation. \n\nFor $\\colorbox{blue}{\\color{white}{k=3}}$ we have \n\n$$\n\\colorbox{blue}{\\color{white}{data[k=3] = 2}}\n$$ \n\nand \n\n$$\n\\colorbox{blue}{\\color{white}{\\textbf{x}[indices[k=3]] = x[1] = 2}}\n$$ \n\nmultiplying them together we have $\\bbox[blue]{\\color{white}{2 \\times 2 = 4}}$ for the last element in our resultant vector.\n\nThus, our resultant vector is \n\n$$\ny = \\textbf{A} \\cdot \\textbf{x} = \\begin{bmatrix} 3 \\\\ 10 \\\\ 4 \\end{bmatrix}\n$$\n\n---\n\n# Model Definition\n\nWe will start with the \"vanilla\" MLR specification that assumes IIA. Admittedly, our problem of classifying text using the TF-IDF embeddings most likely satisfies IIA but for demonstration purposes we will implement a model that explicitly deals with IIA. \n\n## Vanilla MLR:\n\n$$\ny \\sim Categorical(\\textbf{p})\n$$\n\n$$\n\\textbf{p} = softmax(\\pmb{\\alpha} + \\pmb{\\beta}\\textbf{X})\n$$\n\n$$\n\\pmb{\\alpha} \\sim Normal(0, 1)\n$$\n\n$$\n\\pmb{\\beta} \\sim Normal(0, 0.5)\n$$\n\nAlso remember that we are constraining our model such that\n\n$$\n\\sum_{k=1}^{K}\\alpha_{k} = 0\n$$\n\n$$\n\\sum_{k=1}^{K}\\beta_{kd} = 0\n$$\n\nWhere we have $k$ classes and $d$ features.\n\n:::{.callout-caution}\n\nWe discussed the two different ways to impose identifiability to our model and talked about the nice property of having all the coefficients available to us using the sum to zero constraint. It is important to mention that the drawback is that this method is slower because you are explicitly estimating those coefficients.\n\n:::\n\n## Structured Covariance MLR\n\nIn order to address IIA we need to model the correlations between class-wise coefficients. We can model it like\n\n$$\ny \\sim Categorical(\\textbf{p})\n$$\n\n$$\n\\textbf{p} = softmax(\\pmb{\\alpha} + \\pmb{\\beta}\\textbf{X})\n$$\n\n$$\n\\pmb{\\alpha} \\sim Normal(0, 1)\n$$\n\n$$\n\\pmb{\\beta} \\sim MVNormal(\\textbf{0}, \\pmb{\\Sigma}_{class})\n$$\n\n$$\n\\pmb{\\Sigma}_{class} \\sim \\textbf{LKJ}(2.0)\n$$\n\nIn this model we specifically model the covariance between the coefficients across classes by placing an $LKJ(\\eta)$ prior on the covariance matrix. The $\\eta$ parameter adjusts our prior belief on how strong/weak we expect the coefficients to be correlated. \n\nWhere \n\n- $\\eta = 1$ places a uniform distribution over the correlation matrices\n- $\\eta \\lt 1$ places more weight over the coefficients being correlated and\n- $\\eta \\gt 1$ places lesser weight over the coefficients being correlated\n\nIn our example we set $\\eta = 2.0$ because our prior belief is that in this particular example the correlations between class coefficients will be small.\n\n# Results\n\n## Setup\n\nLet's go ahead and pull the code from the prior post that pulls in the toy dataset and generates the TF-IDF embeddings.\n\n::: {#9df2b6bc .cell execution_count=2}\n``` {.python .cell-code}\n# imports\nimport re\nfrom collections import Counter\nfrom typing import Literal\n\nimport arviz as az\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport plotly.graph_objects as go\nimport pymc as pm\nimport pytensor.sparse as spt\nimport pytensor.tensor as pt\nimport xarray as xr\nfrom numpy.typing import NDArray\nfrom scipy import stats\nfrom scipy.sparse import csr_matrix\nfrom sklearn.datasets import fetch_20newsgroups\nfrom sklearn.metrics import classification_report, confusion_matrix\n```\n:::\n\n\n::: {#abe5a51b .cell execution_count=3}\n``` {.python .cell-code}\n# Set a seed for reproducibility\nseed = sum(map(ord, \"multinomialLR\"))\nrng = np.random.default_rng(seed)\n```\n:::\n\n\n::: {#d0b55a61 .cell execution_count=4}\n``` {.python .cell-code}\n# TF-IDF Functions\ndef tokenize(text: str, TOKEN_PATTERN = re.compile(r\"(?u)\\b\\w\\w+\\b\")) -> list[str]:\n    tokens = TOKEN_PATTERN.findall(text.lower())\n    return tokens\n\ndef tf_log(count):\n    return 1 + np.log10(count)\n\ndef idf(doc_count, num_documents):\n    return np.log10(num_documents / doc_count)\n\ndef tf_idf(vocabulary: list, tokens: list[list], mode: Literal['fit', 'transform'] = \"fit\") -> NDArray:\n    \"\"\"\n    Creates sparse TF-IDF embeddings from a given vocabulary and the tokens of documents\n\n    Parameters\n    ----------\n    vocabulary: list\n        The total unique words in your training set corpus\n    tokens: list[list]\n        a list of documents' list of tokens\n    mode: Literal\n        whether you are fitting on a new corpus or transforming to an already existing corpus. default (fit)\n\n    Returns\n    -------\n    NDArray\n        The sparse embeddings in a dense representation\n    \"\"\"\n\n    doc_term_counts = [Counter(doc) for doc in tokens]\n\n    # Compute TF matrix: rows = terms, columns = documents\n    term_frequencies = []\n    for term in vocabulary:\n        term_frequencies.append([\n            tf_log(doc.get(term, 0)) if doc.get(term, 0) > 0 else 0\n            for doc in doc_term_counts\n        ])\n    term_frequencies = np.array(term_frequencies)\n\n    num_documents = term_frequencies.shape[1]\n\n    document_frequencies = np.sum(term_frequencies > 0, axis=1)\n\n    if mode == \"fit\":\n        inverse_document_frequencies = []\n        for doc_count in document_frequencies:\n            inverse_document_frequencies.append(idf(doc_count, num_documents))\n\n        inverse_document_frequencies = np.array(inverse_document_frequencies)\n    else:\n        inverse_document_frequencies = []\n        for doc_count in document_frequencies:\n            if doc_count > 0:\n                inverse_document_frequencies.append(idf(doc_count, num_documents))\n            else:\n                inverse_document_frequencies.append(0)\n        inverse_document_frequencies = np.array(inverse_document_frequencies)\n\n    return term_frequencies * inverse_document_frequencies[:, None]\n```\n:::\n\n\n::: {#f2f0296d .cell execution_count=5}\n``` {.python .cell-code}\n# Pull in toy data\ncategories = [\n    \"sci.med\",\n    \"comp.graphics\",\n    \"sci.space\",\n]\n\nnewsgroups_train = fetch_20newsgroups(subset='train', categories=categories)\nnewsgroups_test = fetch_20newsgroups(subset='test', categories=categories)\n```\n:::\n\n\n::: {#1b0a888e .cell execution_count=6}\n``` {.python .cell-code}\n# Create training dataset TF-IDF\ntrain_tokens = [tokenize(text) for text in newsgroups_train.data]\nvocabulary = sorted(set(sum(train_tokens, [])))\ntrain_tf_idf = tf_idf(vocabulary=vocabulary, tokens=train_tokens, mode='fit')\n\n# Create testing dataset TF-IDF\ntest_tokens = [tokenize(text) for text in newsgroups_test.data]\ntest_tf_idf = tf_idf(vocabulary=vocabulary, tokens=test_tokens, mode='transform')\n\n# Create X,y sets\nX_train = train_tf_idf.T\ny_train = newsgroups_train.target\nX_test = test_tf_idf.T\ny_test = newsgroups_test.target\n```\n:::\n\n\nHere we will convert our sparse matrices from the dense representation with all the zeros to the CSR representation that we discussed above.\n\n::: {#223a3f3e .cell execution_count=7}\n``` {.python .cell-code}\n# Create Sparse CSR Matrices\nX_train_sparse = csr_matrix(X_train)\nX_test_sparse = csr_matrix(X_test)\n```\n:::\n\n\n## Vanilla MLR\n\nAlright, with our TF-IDF embeddings created and converted to CSR we are ready to start building our model. Let's start with the typical \"vanilla\" MLR model that assumes IIA. We are going to be doing all of this in a Bayesian framework which gives us the flexibility to extend this \"vanilla\" model to include more complex relationships. \n\nNotice that we are enforcing identifiability by enducing a sum to zero over the parameters across the classes and notice that we are computing our logits using a sparse matrix vector multiplication algorithm.\n\n::: {#85f0071e .cell execution_count=8}\n``` {.python .cell-code}\n# MLR Model specification\ncoords = {\n    \"classes\": newsgroups_test.target_names,\n    \"features\": vocabulary,\n    \"num_obs\": np.arange(X_train.shape[0])\n}\nwith pm.Model(coords=coords) as sparse_model:\n    # Data containers\n    y = pm.Data(\"y\", y_train, dims=\"num_obs\")\n    X = pm.Data(\"X\", X_train_sparse)\n\n    # Priors\n    # Unconstrained raw parameters\n    alpha_raw = pm.Normal(\"alpha_raw\", 0, 1, dims=\"classes\")\n    coefs_raw = pm.Normal(\"coefs_raw\", 0, 0.5, dims=(\"classes\", \"features\"))\n\n    # Enforce identifiability by subtracting mean across the class dimension\n    alpha = alpha_raw - pt.mean(alpha_raw)\n    coefs = coefs_raw - pt.mean(coefs_raw, axis=0)\n\n    # Compute logits using sparse dot\n    logits = alpha + spt.structured_dot(X, coefs.T)\n\n    # Likelihood\n    pm.Categorical(\"y_obs\", logit_p=logits, observed=y, dims=\"num_obs\")\n```\n:::\n\n\nOne thing to note is that at the moment sparse tensor operations are not supported in sampling accelerators like `nutpie` or `numpyro` via `PyMC`. So we will have to sample with the default `PyMC` sampler.\n\n::: {#a3a2a5d5 .cell execution_count=9}\n``` {.python .cell-code}\n# Sampling\nwith sparse_model:\n    idata = pm.sample(mp_ctx=\"spawn\", random_seed=rng)\n```\n:::\n\n\nWith our samples in hand, let's pull a summary of the learned weights and inspect what words based on the TF-IDF embeddings have the largest influence over each class. \n\n::: {#f305ccaa .cell execution_count=10}\n``` {.python .cell-code}\n# Get model summaries\nsummary_df = az.summary(idata, kind=\"stats\")\n```\n:::\n\n\nIn the tables below, you'll notice that in the brackets you have \"[class name, feature name]\" and these have been sorted by the posterior mean to represent the largest influential words per class. \n\nFor the `comp.graphics` class words like \"graphics\" and \"image\" are influential to push a document to being classified as `comp.graphics`. This seems sensible to me. \n\nSimilarly, the influential features for the other two classes also seem sensible. \n\n::: {#tbl-vanilla-compgraphics .cell tbl-cap='Most influencial words for class comp.graphics' execution_count=11}\n``` {.python .cell-code}\n# Look at weights learned for the comp.graphics class\nsummary_df.filter(regex=r\"(?i)coefs_raw\\[comp\", axis='index').sort_values(\"mean\", ascending=False).head(10)\n```\n\n::: {.cell-output .cell-output-display execution_count=10}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mean</th>\n      <th>sd</th>\n      <th>hdi_3%</th>\n      <th>hdi_97%</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>coefs_raw[comp.graphics, graphics]</th>\n      <td>1.392</td>\n      <td>0.482</td>\n      <td>0.553</td>\n      <td>2.351</td>\n    </tr>\n    <tr>\n      <th>coefs_raw[comp.graphics, image]</th>\n      <td>0.802</td>\n      <td>0.495</td>\n      <td>-0.098</td>\n      <td>1.746</td>\n    </tr>\n    <tr>\n      <th>coefs_raw[comp.graphics, file]</th>\n      <td>0.762</td>\n      <td>0.477</td>\n      <td>-0.130</td>\n      <td>1.671</td>\n    </tr>\n    <tr>\n      <th>coefs_raw[comp.graphics, 3d]</th>\n      <td>0.758</td>\n      <td>0.457</td>\n      <td>-0.055</td>\n      <td>1.653</td>\n    </tr>\n    <tr>\n      <th>coefs_raw[comp.graphics, hi]</th>\n      <td>0.625</td>\n      <td>0.480</td>\n      <td>-0.247</td>\n      <td>1.548</td>\n    </tr>\n    <tr>\n      <th>coefs_raw[comp.graphics, files]</th>\n      <td>0.619</td>\n      <td>0.486</td>\n      <td>-0.310</td>\n      <td>1.508</td>\n    </tr>\n    <tr>\n      <th>coefs_raw[comp.graphics, code]</th>\n      <td>0.612</td>\n      <td>0.487</td>\n      <td>-0.272</td>\n      <td>1.526</td>\n    </tr>\n    <tr>\n      <th>coefs_raw[comp.graphics, video]</th>\n      <td>0.601</td>\n      <td>0.497</td>\n      <td>-0.314</td>\n      <td>1.525</td>\n    </tr>\n    <tr>\n      <th>coefs_raw[comp.graphics, format]</th>\n      <td>0.595</td>\n      <td>0.488</td>\n      <td>-0.376</td>\n      <td>1.462</td>\n    </tr>\n    <tr>\n      <th>coefs_raw[comp.graphics, windows]</th>\n      <td>0.579</td>\n      <td>0.484</td>\n      <td>-0.332</td>\n      <td>1.464</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {#tbl-vanilla-scimed .cell tbl-cap='Most influencial words for class sci.med' execution_count=12}\n``` {.python .cell-code}\n# Look at weights learned for the sci.med class\nsummary_df.filter(regex=r\"(?i)coefs_raw\\[sci.med\", axis='index').sort_values(\"mean\", ascending=False).head(10)\n```\n\n::: {.cell-output .cell-output-display execution_count=11}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mean</th>\n      <th>sd</th>\n      <th>hdi_3%</th>\n      <th>hdi_97%</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>coefs_raw[sci.med, doctor]</th>\n      <td>0.776</td>\n      <td>0.480</td>\n      <td>-0.098</td>\n      <td>1.707</td>\n    </tr>\n    <tr>\n      <th>coefs_raw[sci.med, pitt]</th>\n      <td>0.738</td>\n      <td>0.473</td>\n      <td>-0.153</td>\n      <td>1.617</td>\n    </tr>\n    <tr>\n      <th>coefs_raw[sci.med, disease]</th>\n      <td>0.712</td>\n      <td>0.470</td>\n      <td>-0.210</td>\n      <td>1.537</td>\n    </tr>\n    <tr>\n      <th>coefs_raw[sci.med, health]</th>\n      <td>0.652</td>\n      <td>0.487</td>\n      <td>-0.238</td>\n      <td>1.596</td>\n    </tr>\n    <tr>\n      <th>coefs_raw[sci.med, medical]</th>\n      <td>0.612</td>\n      <td>0.482</td>\n      <td>-0.249</td>\n      <td>1.551</td>\n    </tr>\n    <tr>\n      <th>coefs_raw[sci.med, msg]</th>\n      <td>0.609</td>\n      <td>0.480</td>\n      <td>-0.276</td>\n      <td>1.527</td>\n    </tr>\n    <tr>\n      <th>coefs_raw[sci.med, treatment]</th>\n      <td>0.598</td>\n      <td>0.487</td>\n      <td>-0.316</td>\n      <td>1.473</td>\n    </tr>\n    <tr>\n      <th>coefs_raw[sci.med, geb]</th>\n      <td>0.584</td>\n      <td>0.488</td>\n      <td>-0.296</td>\n      <td>1.523</td>\n    </tr>\n    <tr>\n      <th>coefs_raw[sci.med, she]</th>\n      <td>0.579</td>\n      <td>0.480</td>\n      <td>-0.269</td>\n      <td>1.528</td>\n    </tr>\n    <tr>\n      <th>coefs_raw[sci.med, banks]</th>\n      <td>0.578</td>\n      <td>0.498</td>\n      <td>-0.363</td>\n      <td>1.477</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {#tbl-vanilla-scispace .cell tbl-cap='Most influencial words for class sci.space' execution_count=13}\n``` {.python .cell-code}\n# Look at weights learned for the sci.space class\nsummary_df.filter(regex=r\"(?i)coefs_raw\\[sci.space\", axis='index').sort_values(\"mean\", ascending=False).head(10)\n```\n\n::: {.cell-output .cell-output-display execution_count=12}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mean</th>\n      <th>sd</th>\n      <th>hdi_3%</th>\n      <th>hdi_97%</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>coefs_raw[sci.space, space]</th>\n      <td>1.725</td>\n      <td>0.478</td>\n      <td>0.841</td>\n      <td>2.631</td>\n    </tr>\n    <tr>\n      <th>coefs_raw[sci.space, orbit]</th>\n      <td>0.903</td>\n      <td>0.487</td>\n      <td>-0.047</td>\n      <td>1.764</td>\n    </tr>\n    <tr>\n      <th>coefs_raw[sci.space, nasa]</th>\n      <td>0.804</td>\n      <td>0.483</td>\n      <td>-0.116</td>\n      <td>1.672</td>\n    </tr>\n    <tr>\n      <th>coefs_raw[sci.space, moon]</th>\n      <td>0.731</td>\n      <td>0.485</td>\n      <td>-0.226</td>\n      <td>1.610</td>\n    </tr>\n    <tr>\n      <th>coefs_raw[sci.space, shuttle]</th>\n      <td>0.667</td>\n      <td>0.461</td>\n      <td>-0.212</td>\n      <td>1.526</td>\n    </tr>\n    <tr>\n      <th>coefs_raw[sci.space, pat]</th>\n      <td>0.651</td>\n      <td>0.495</td>\n      <td>-0.307</td>\n      <td>1.546</td>\n    </tr>\n    <tr>\n      <th>coefs_raw[sci.space, launch]</th>\n      <td>0.639</td>\n      <td>0.483</td>\n      <td>-0.282</td>\n      <td>1.494</td>\n    </tr>\n    <tr>\n      <th>coefs_raw[sci.space, henry]</th>\n      <td>0.573</td>\n      <td>0.485</td>\n      <td>-0.300</td>\n      <td>1.500</td>\n    </tr>\n    <tr>\n      <th>coefs_raw[sci.space, digex]</th>\n      <td>0.572</td>\n      <td>0.503</td>\n      <td>-0.363</td>\n      <td>1.521</td>\n    </tr>\n    <tr>\n      <th>coefs_raw[sci.space, planets]</th>\n      <td>0.542</td>\n      <td>0.475</td>\n      <td>-0.303</td>\n      <td>1.463</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nLet's inspect the IIA assumption next.\n\nIf we take the most influential feature weight of one of the classes then we should expect that the same feature weight for the other two classes to have a small weight. \n\nThis would imply that conditional on the high weight class the other two classes would be dependent. In other words, there would be some correlation (positive in this case) between that feature's weights in the other two classes. \n\nBelow, we plot the posterior weights for the feature \"space\", which is the most influential feature for class `sci.space`, for the classes `comp.graphics` and `sci.med`.\n\nHere you see the effects of IIA, where it is clear that there is no correlation between the two class weights for that feature.\n\n::: {#54f74bad .cell .light-mode execution_count=14}\n``` {.python .cell-code}\n# look at IIA correlations\nplt.style.use('seaborn-v0_8')\naz.plot_pair(idata, var_names=['coefs_raw'], coords={\"classes\": ['comp.graphics', 'sci.med'], \"features\": ['space']});\n```\n\n::: {.cell-output .cell-output-display}\n![Correlations between class parameters for the word space.](index_files/figure-html/cell-14-output-1.png){width=686 height=483}\n:::\n:::\n\n\n::: {#14d66e68 .cell .dark-mode execution_count=15}\n``` {.python .cell-code}\n# look at IIA correlations\nplt.style.use('dark_background')\naz.plot_pair(idata, var_names=['coefs_raw'], coords={\"classes\": ['comp.graphics', 'sci.med'], \"features\": ['space']});\n```\n\n::: {.cell-output .cell-output-display}\n![Correlations between class parameters for the word space.](index_files/figure-html/cell-15-output-1.png){width=686 height=483}\n:::\n:::\n\n\nAs we mentioned earlier, though, IIA in this particular case will have minor impacts to model performance.\n\nLet's produce out-of-sample predictions using our test dataset and compute some standard classification evaluation metrics.\n\nLooking at the `f1-scores`, the model performs well at differentiating between the three classes.\n\n::: {#f1198996 .cell execution_count=16}\n``` {.python .cell-code}\n# Out of sample predictions\nwith sparse_model:\n    pm.set_data(\n        new_data={\n            \"X\": X_test_sparse, \n            \"y\": np.ones_like(y_test)\n        },\n        coords={\n            \"num_obs\": np.arange(len(y_test))\n        }\n    )\n    idata_posterior = pm.sample_posterior_predictive(idata, predictions=True)\n```\n:::\n\n\n::: {#e1c9ebcb .cell execution_count=17}\n``` {.python .cell-code}\n# Evaluations\nidata_posterior['posterior_predictive'] = idata_posterior.predictions\nidata_posterior['observed_data'] = xr.Dataset({\"y_obs\": xr.DataArray(y_test)})\ny_pred = stats.mode(az.extract(idata_posterior, group=\"predictions\", var_names=\"y_obs\").values, axis=1).mode\nprint(classification_report(y_true=y_test, y_pred=y_pred))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n              precision    recall  f1-score   support\n\n           0       0.89      0.96      0.92       389\n           1       0.95      0.91      0.93       396\n           2       0.97      0.93      0.95       394\n\n    accuracy                           0.93      1179\n   macro avg       0.94      0.93      0.93      1179\nweighted avg       0.94      0.93      0.93      1179\n\n```\n:::\n:::\n\n\n## Structured Covariance MLR\n\nLet's extend the previous model to handle IIA. \n\nAs mentioned above, we are setting our prior to assume that the correlations between the classes are minute. However, with the large volume of data used for sampling, if this prior is not appropriate the likelihood will adjust our covariance weights to reflect that.\n\nNotice here, we are working with our coefficients in the transposed space which changes the axis used to induce our sum to zero constraint.\n\n:::{.callout-warning}\n\nThis model is more complex and as such requires more samples. Below we increased the number of chains to acquire more samples while leveraging parallel computing. If your machine has less than 8 cores, you will need to adjust the number of chains accordingly.\n\n:::\n\n::: {#d30424e1 .cell execution_count=18}\n``` {.python .cell-code}\n# Features and classes\nD = X_train_sparse.shape[1]\nK = 3\n\nwith pm.Model(coords=coords) as sparse_correlated_model:\n    # Data containers\n    y = pm.Data(\"y\", y_train, dims=\"num_obs\")\n    X = pm.Data(\"X\", X_train_sparse)\n\n    # Priors\n    # Unconstrained raw parameters\n    alpha_raw = pm.Normal(\"alpha_raw\", 0, 1, dims=\"classes\")\n\n    # Enforce identifiability by subtracting mean across the class dimension\n    alpha = alpha_raw - pt.mean(alpha_raw)\n\n    # covariance prior\n    sd_dist = pm.HalfNormal.dist(1.0)\n    chol, corr, stds = pm.LKJCholeskyCov(\n        \"class_cov\", n=K, eta=2.0, sd_dist=sd_dist\n    )\n\n    # non-centered parameterization for better sampling\n    z = pm.Normal(\"z\", mu=0, sigma=1, shape=(D, K), dims=(\"features\", \"classes\"))\n    coefs_T = pm.Deterministic(\"coefs_T\", pt.dot(z, chol.T), dims=(\"features\", \"classes\"))\n    \n    # Enforce identifiability by subtracting mean across the class dimension\n    coefs_T_constrained = coefs_T - pt.mean(coefs_T, axis=1, keepdims=True)\n\n    # Compute logits using sparse dot\n    logits = alpha + spt.structured_dot(X, coefs_T_constrained)\n\n    # Likelihood\n    pm.Categorical(\"y_obs\", logit_p=logits, observed=y, dims=\"num_obs\")\n\nwith sparse_correlated_model:\n    correlated_idata = pm.sample(mp_ctx=\"spawn\", target_accept=0.9, cores=8, random_seed=rng)\n```\n:::\n\n\nLet's take a look at the posterior means of the correlations between classes that the model estimated.\n\nOkay, looks like the correlations are indeed small.\n\n::: {#f318d23b .cell execution_count=19}\n``` {.python .cell-code}\ncorr_tbl = az.summary(correlated_idata, var_names=[\"class_cov_corr\"], kind=\"stats\")\n```\n:::\n\n\n::: {#tbl-cov-corr .cell tbl-cap='Estimated Correlations between Classes' execution_count=20}\n``` {.python .cell-code}\ncorr_tbl\n```\n\n::: {.cell-output .cell-output-display execution_count=19}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mean</th>\n      <th>sd</th>\n      <th>hdi_3%</th>\n      <th>hdi_97%</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>class_cov_corr[0, 0]</th>\n      <td>1.000</td>\n      <td>0.000</td>\n      <td>1.000</td>\n      <td>1.000</td>\n    </tr>\n    <tr>\n      <th>class_cov_corr[0, 1]</th>\n      <td>0.086</td>\n      <td>0.479</td>\n      <td>-0.616</td>\n      <td>1.000</td>\n    </tr>\n    <tr>\n      <th>class_cov_corr[0, 2]</th>\n      <td>-0.067</td>\n      <td>0.368</td>\n      <td>-0.767</td>\n      <td>0.530</td>\n    </tr>\n    <tr>\n      <th>class_cov_corr[1, 0]</th>\n      <td>0.086</td>\n      <td>0.479</td>\n      <td>-0.616</td>\n      <td>1.000</td>\n    </tr>\n    <tr>\n      <th>class_cov_corr[1, 1]</th>\n      <td>1.000</td>\n      <td>0.000</td>\n      <td>1.000</td>\n      <td>1.000</td>\n    </tr>\n    <tr>\n      <th>class_cov_corr[1, 2]</th>\n      <td>-0.025</td>\n      <td>0.366</td>\n      <td>-0.719</td>\n      <td>0.608</td>\n    </tr>\n    <tr>\n      <th>class_cov_corr[2, 0]</th>\n      <td>-0.067</td>\n      <td>0.368</td>\n      <td>-0.767</td>\n      <td>0.530</td>\n    </tr>\n    <tr>\n      <th>class_cov_corr[2, 1]</th>\n      <td>-0.025</td>\n      <td>0.366</td>\n      <td>-0.719</td>\n      <td>0.608</td>\n    </tr>\n    <tr>\n      <th>class_cov_corr[2, 2]</th>\n      <td>1.000</td>\n      <td>0.000</td>\n      <td>1.000</td>\n      <td>1.000</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nLet's also ensure that our influential features still make sense. \n\nNotice because of our transposed coefficients in the model definintion we now have the features first and the class second in the brackets. \n\nLooks like our weights, relatively speaking, are still sensible. You should notice that the weight values themselves are different.\n\n::: {#tbl-cov-compgraphics .cell tbl-cap='Most influencial words for class comp.graphics - Structured Covariance Model' execution_count=21}\n``` {.python .cell-code}\ncor_summary_df = az.summary(correlated_idata, var_names=[\"coefs_T\"], kind=\"stats\")\ncor_summary_df.filter(regex=r\"(?i)comp.graphics\\]$\", axis='index').sort_values(\"mean\", ascending=False).head(10)\n```\n\n::: {.cell-output .cell-output-display execution_count=20}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mean</th>\n      <th>sd</th>\n      <th>hdi_3%</th>\n      <th>hdi_97%</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>coefs_T[graphics, comp.graphics]</th>\n      <td>3.821</td>\n      <td>2.641</td>\n      <td>-0.180</td>\n      <td>8.096</td>\n    </tr>\n    <tr>\n      <th>coefs_T[image, comp.graphics]</th>\n      <td>2.216</td>\n      <td>1.860</td>\n      <td>-0.246</td>\n      <td>5.738</td>\n    </tr>\n    <tr>\n      <th>coefs_T[3d, comp.graphics]</th>\n      <td>2.124</td>\n      <td>1.836</td>\n      <td>-0.474</td>\n      <td>5.549</td>\n    </tr>\n    <tr>\n      <th>coefs_T[file, comp.graphics]</th>\n      <td>2.094</td>\n      <td>1.803</td>\n      <td>-0.473</td>\n      <td>5.526</td>\n    </tr>\n    <tr>\n      <th>coefs_T[files, comp.graphics]</th>\n      <td>1.728</td>\n      <td>1.726</td>\n      <td>-0.834</td>\n      <td>5.102</td>\n    </tr>\n    <tr>\n      <th>coefs_T[hi, comp.graphics]</th>\n      <td>1.721</td>\n      <td>1.667</td>\n      <td>-0.621</td>\n      <td>5.040</td>\n    </tr>\n    <tr>\n      <th>coefs_T[code, comp.graphics]</th>\n      <td>1.677</td>\n      <td>1.674</td>\n      <td>-0.727</td>\n      <td>5.086</td>\n    </tr>\n    <tr>\n      <th>coefs_T[format, comp.graphics]</th>\n      <td>1.658</td>\n      <td>1.625</td>\n      <td>-0.818</td>\n      <td>4.752</td>\n    </tr>\n    <tr>\n      <th>coefs_T[video, comp.graphics]</th>\n      <td>1.658</td>\n      <td>1.671</td>\n      <td>-0.859</td>\n      <td>4.992</td>\n    </tr>\n    <tr>\n      <th>coefs_T[card, comp.graphics]</th>\n      <td>1.596</td>\n      <td>1.644</td>\n      <td>-0.863</td>\n      <td>4.869</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {#tbl-cov-scimed .cell tbl-cap='Most influencial words for class sci.med - Structured Covariance Model' execution_count=22}\n``` {.python .cell-code}\ncor_summary_df.filter(regex=r\"(?i)sci.med\\]$\", axis='index').sort_values(\"mean\", ascending=False).head(10)\n```\n\n::: {.cell-output .cell-output-display execution_count=21}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mean</th>\n      <th>sd</th>\n      <th>hdi_3%</th>\n      <th>hdi_97%</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>coefs_T[doctor, sci.med]</th>\n      <td>1.684</td>\n      <td>1.482</td>\n      <td>-0.646</td>\n      <td>4.573</td>\n    </tr>\n    <tr>\n      <th>coefs_T[pitt, sci.med]</th>\n      <td>1.586</td>\n      <td>1.469</td>\n      <td>-0.584</td>\n      <td>4.579</td>\n    </tr>\n    <tr>\n      <th>coefs_T[disease, sci.med]</th>\n      <td>1.457</td>\n      <td>1.565</td>\n      <td>-0.601</td>\n      <td>4.513</td>\n    </tr>\n    <tr>\n      <th>coefs_T[msg, sci.med]</th>\n      <td>1.355</td>\n      <td>1.426</td>\n      <td>-0.820</td>\n      <td>4.245</td>\n    </tr>\n    <tr>\n      <th>coefs_T[health, sci.med]</th>\n      <td>1.343</td>\n      <td>1.517</td>\n      <td>-0.901</td>\n      <td>4.404</td>\n    </tr>\n    <tr>\n      <th>coefs_T[treatment, sci.med]</th>\n      <td>1.308</td>\n      <td>1.397</td>\n      <td>-0.808</td>\n      <td>4.234</td>\n    </tr>\n    <tr>\n      <th>coefs_T[medical, sci.med]</th>\n      <td>1.303</td>\n      <td>1.433</td>\n      <td>-0.784</td>\n      <td>4.233</td>\n    </tr>\n    <tr>\n      <th>coefs_T[she, sci.med]</th>\n      <td>1.251</td>\n      <td>1.379</td>\n      <td>-0.809</td>\n      <td>4.101</td>\n    </tr>\n    <tr>\n      <th>coefs_T[banks, sci.med]</th>\n      <td>1.196</td>\n      <td>1.445</td>\n      <td>-0.995</td>\n      <td>4.186</td>\n    </tr>\n    <tr>\n      <th>coefs_T[geb, sci.med]</th>\n      <td>1.170</td>\n      <td>1.406</td>\n      <td>-0.897</td>\n      <td>4.188</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {#tbl-cov-scispace .cell tbl-cap='Most influencial words for class sci.space - Structured Covariance Model' execution_count=23}\n``` {.python .cell-code}\ncor_summary_df.filter(regex=r\"(?i)sci.space\\]$\", axis='index').sort_values(\"mean\", ascending=False).head(10)\n```\n\n::: {.cell-output .cell-output-display execution_count=22}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mean</th>\n      <th>sd</th>\n      <th>hdi_3%</th>\n      <th>hdi_97%</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>coefs_T[space, sci.space]</th>\n      <td>3.200</td>\n      <td>2.602</td>\n      <td>-0.243</td>\n      <td>7.963</td>\n    </tr>\n    <tr>\n      <th>coefs_T[orbit, sci.space]</th>\n      <td>1.657</td>\n      <td>1.632</td>\n      <td>-0.514</td>\n      <td>4.835</td>\n    </tr>\n    <tr>\n      <th>coefs_T[nasa, sci.space]</th>\n      <td>1.480</td>\n      <td>1.497</td>\n      <td>-0.514</td>\n      <td>4.473</td>\n    </tr>\n    <tr>\n      <th>coefs_T[moon, sci.space]</th>\n      <td>1.367</td>\n      <td>1.486</td>\n      <td>-0.517</td>\n      <td>4.470</td>\n    </tr>\n    <tr>\n      <th>coefs_T[shuttle, sci.space]</th>\n      <td>1.227</td>\n      <td>1.428</td>\n      <td>-0.866</td>\n      <td>4.140</td>\n    </tr>\n    <tr>\n      <th>coefs_T[launch, sci.space]</th>\n      <td>1.199</td>\n      <td>1.426</td>\n      <td>-0.885</td>\n      <td>4.083</td>\n    </tr>\n    <tr>\n      <th>coefs_T[pat, sci.space]</th>\n      <td>1.196</td>\n      <td>1.386</td>\n      <td>-0.787</td>\n      <td>3.967</td>\n    </tr>\n    <tr>\n      <th>coefs_T[digex, sci.space]</th>\n      <td>1.056</td>\n      <td>1.335</td>\n      <td>-0.873</td>\n      <td>3.982</td>\n    </tr>\n    <tr>\n      <th>coefs_T[henry, sci.space]</th>\n      <td>1.041</td>\n      <td>1.312</td>\n      <td>-0.935</td>\n      <td>3.683</td>\n    </tr>\n    <tr>\n      <th>coefs_T[earth, sci.space]</th>\n      <td>1.011</td>\n      <td>1.352</td>\n      <td>-1.093</td>\n      <td>3.788</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nLet's inspect the same feature for the same two classes that we did earlier to check whether our structured covariance has appropriately delt with IIA. \n\nLooking at the plot below, we certainly see positive correlation between the two class weights for the \"space\" feature!\n\n::: {#82ec0837 .cell .light-mode execution_count=24}\n``` {.python .cell-code}\nplt.style.use('seaborn-v0_8')\naz.plot_pair(correlated_idata, var_names=['coefs_T'], coords={\"classes\": ['comp.graphics', 'sci.med'], \"features\": ['space']});\n```\n\n::: {.cell-output .cell-output-display}\n![Correlations between class parameters for the word space - Structured Covariance Model](index_files/figure-html/cell-24-output-1.png){width=682 height=483}\n:::\n:::\n\n\n::: {#31fd23a3 .cell .dark-mode execution_count=25}\n``` {.python .cell-code}\nplt.style.use('dark_background')\naz.plot_pair(correlated_idata, var_names=['coefs_T'], coords={\"classes\": ['comp.graphics', 'sci.med'], \"features\": ['space']});\n```\n\n::: {.cell-output .cell-output-display}\n![Correlations between class parameters for the word space - Structured Covariance Model](index_files/figure-html/cell-25-output-1.png){width=682 height=483}\n:::\n:::\n\n\nNow let's generate out-of-sample predictions and evaluate our `f1-scores` to see if we have any differences from the previous \"vanilla\" model.\n\nLooking at the `f1-scores`, seems like we have a slight improvement due to the additional information captured by the dependecies across the classes.\n\n::: {#e29410ee .cell execution_count=26}\n``` {.python .cell-code}\nwith sparse_correlated_model:\n    pm.set_data(\n        new_data={\n            \"X\": X_test_sparse, \n            \"y\": np.ones_like(y_test)\n        },\n        coords={\n            \"num_obs\": np.arange(len(y_test))\n        }\n    )\n    correlated_idata_posterior = pm.sample_posterior_predictive(correlated_idata, predictions=True)\n```\n:::\n\n\n::: {#bd946828 .cell execution_count=27}\n``` {.python .cell-code}\ncorrelated_idata_posterior['posterior_predictive'] = correlated_idata_posterior.predictions\ncorrelated_idata_posterior['observed_data'] = xr.Dataset({\"y_obs\": xr.DataArray(y_test)})\ncorrelated_y_pred = stats.mode(az.extract(correlated_idata_posterior, group=\"predictions\", var_names=\"y_obs\").values, axis=1).mode\nprint(classification_report(y_true=y_test, y_pred=correlated_y_pred))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n              precision    recall  f1-score   support\n\n           0       0.91      0.96      0.93       389\n           1       0.96      0.92      0.94       396\n           2       0.97      0.95      0.96       394\n\n    accuracy                           0.94      1179\n   macro avg       0.94      0.94      0.94      1179\nweighted avg       0.94      0.94      0.94      1179\n\n```\n:::\n:::\n\n\nFinally, let's plot a confusion matrix to inspect the predictions and identify where errors dominate.\n\nLooks like our model struggles in discriminating between `sci.med` and `comp.graphics`. This is apparent in the figure below where our model makes the most errors when it predicts `sci.med` when the actual class is `comp.graphics`\n\nMy speculation for why these classes are more challenging to discriminate may be due to polysemous words that can be found in both domains. \n\nFor example, words like \"screen\" in computer graphics can obviously mean the computer screen whereas in medicine it can mean to screen for a disease or condition. Another one I can think of may be the word \"virus\" where in computer graphics could be referring to a malicous attack versus in medicine which I guess also refers to a malicious attack but you understand what I am saying here. \n\n::: {#1d1f45a7 .cell .light-mode execution_count=28}\n``` {.python .cell-code}\nfig = go.Figure()\nfig.add_traces(\n    [\n        go.Heatmap(\n            x = newsgroups_test.target_names,\n            y = newsgroups_test.target_names,\n            z = confusion_matrix(y_test, correlated_y_pred),\n            text=[[col for col in row] for row in confusion_matrix(y_test, correlated_y_pred)],\n            texttemplate=\"%{text}\",\n            colorscale=\"rdbu\"\n        )\n    ]\n)\nfig.update_layout(\n    xaxis=dict(title=\"Actual Class\"),\n    yaxis=dict(autorange='reversed', title = \"Predicted Class\")\n)\n```\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<div>                            <div id=\"13c0f850-59ea-4e30-8bf1-5ac0d134ceb9\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"13c0f850-59ea-4e30-8bf1-5ac0d134ceb9\")) {                    Plotly.newPlot(                        \"13c0f850-59ea-4e30-8bf1-5ac0d134ceb9\",                        [{\"colorscale\":[[0.0,\"rgb(103,0,31)\"],[0.1,\"rgb(178,24,43)\"],[0.2,\"rgb(214,96,77)\"],[0.3,\"rgb(244,165,130)\"],[0.4,\"rgb(253,219,199)\"],[0.5,\"rgb(247,247,247)\"],[0.6,\"rgb(209,229,240)\"],[0.7,\"rgb(146,197,222)\"],[0.8,\"rgb(67,147,195)\"],[0.9,\"rgb(33,102,172)\"],[1.0,\"rgb(5,48,97)\"]],\"text\":[[375,8,6],[27,364,5],[12,9,373]],\"texttemplate\":\"%{text}\",\"x\":[\"comp.graphics\",\"sci.med\",\"sci.space\"],\"y\":[\"comp.graphics\",\"sci.med\",\"sci.space\"],\"z\":[[375,8,6],[27,364,5],[12,9,373]],\"type\":\"heatmap\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"},\"margin\":{\"b\":0,\"l\":0,\"r\":0,\"t\":30}}},\"yaxis\":{\"autorange\":\"reversed\",\"title\":{\"text\":\"Predicted Class\"}},\"xaxis\":{\"title\":{\"text\":\"Actual Class\"}}},                        {\"responsive\": true}                    ).then(function(){\n                            \nvar gd = document.getElementById('13c0f850-59ea-4e30-8bf1-5ac0d134ceb9');\nvar x = new MutationObserver(function (mutations, observer) {{\n        var display = window.getComputedStyle(gd).display;\n        if (!display || display === 'none') {{\n            console.log([gd, 'removed!']);\n            Plotly.purge(gd);\n            observer.disconnect();\n        }}\n}});\n\n// Listen for the removal of the full notebook cells\nvar notebookContainer = gd.closest('#notebook-container');\nif (notebookContainer) {{\n    x.observe(notebookContainer, {childList: true});\n}}\n\n// Listen for the clearing of the current output cell\nvar outputEl = gd.closest('.output');\nif (outputEl) {{\n    x.observe(outputEl, {childList: true});\n}}\n\n                        })                };                });            </script>        </div>\n```\n\nConfusion Matrix\n:::\n:::\n\n\n::: {#192a9888 .cell .dark-mode execution_count=29}\n``` {.python .cell-code}\nfig = go.Figure()\nfig.add_traces(\n    [\n        go.Heatmap(\n            x = newsgroups_test.target_names,\n            y = newsgroups_test.target_names,\n            z = confusion_matrix(y_test, correlated_y_pred),\n            text=[[col for col in row] for row in confusion_matrix(y_test, correlated_y_pred)],\n            texttemplate=\"%{text}\",\n            colorscale=\"rdbu\"\n        )\n    ]\n)\nfig.update_layout(\n    xaxis=dict(title=\"Actual Class\", gridcolor = 'rgba(68, 68, 68, 0.5)'),\n    yaxis=dict(autorange='reversed', title = \"Predicted Class\", gridcolor = 'rgba(68, 68, 68, 0.5)'),\n    plot_bgcolor='black',\n    paper_bgcolor='rgba(68, 68, 68, 0.5)',\n    font=dict(\n        family=\"Verdana\",\n        size=12,\n        color=\"white\"\n    ),\n)\n```\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<div>                            <div id=\"49263058-bce3-486a-ab2b-50e6cd4312cd\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"49263058-bce3-486a-ab2b-50e6cd4312cd\")) {                    Plotly.newPlot(                        \"49263058-bce3-486a-ab2b-50e6cd4312cd\",                        [{\"colorscale\":[[0.0,\"rgb(103,0,31)\"],[0.1,\"rgb(178,24,43)\"],[0.2,\"rgb(214,96,77)\"],[0.3,\"rgb(244,165,130)\"],[0.4,\"rgb(253,219,199)\"],[0.5,\"rgb(247,247,247)\"],[0.6,\"rgb(209,229,240)\"],[0.7,\"rgb(146,197,222)\"],[0.8,\"rgb(67,147,195)\"],[0.9,\"rgb(33,102,172)\"],[1.0,\"rgb(5,48,97)\"]],\"text\":[[375,8,6],[27,364,5],[12,9,373]],\"texttemplate\":\"%{text}\",\"x\":[\"comp.graphics\",\"sci.med\",\"sci.space\"],\"y\":[\"comp.graphics\",\"sci.med\",\"sci.space\"],\"z\":[[375,8,6],[27,364,5],[12,9,373]],\"type\":\"heatmap\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"},\"margin\":{\"b\":0,\"l\":0,\"r\":0,\"t\":30}}},\"xaxis\":{\"title\":{\"text\":\"Actual Class\"},\"gridcolor\":\"rgba(68, 68, 68, 0.5)\"},\"yaxis\":{\"autorange\":\"reversed\",\"title\":{\"text\":\"Predicted Class\"},\"gridcolor\":\"rgba(68, 68, 68, 0.5)\"},\"font\":{\"family\":\"Verdana\",\"size\":12,\"color\":\"white\"},\"plot_bgcolor\":\"black\",\"paper_bgcolor\":\"rgba(68, 68, 68, 0.5)\"},                        {\"responsive\": true}                    ).then(function(){\n                            \nvar gd = document.getElementById('49263058-bce3-486a-ab2b-50e6cd4312cd');\nvar x = new MutationObserver(function (mutations, observer) {{\n        var display = window.getComputedStyle(gd).display;\n        if (!display || display === 'none') {{\n            console.log([gd, 'removed!']);\n            Plotly.purge(gd);\n            observer.disconnect();\n        }}\n}});\n\n// Listen for the removal of the full notebook cells\nvar notebookContainer = gd.closest('#notebook-container');\nif (notebookContainer) {{\n    x.observe(notebookContainer, {childList: true});\n}}\n\n// Listen for the clearing of the current output cell\nvar outputEl = gd.closest('.output');\nif (outputEl) {{\n    x.observe(outputEl, {childList: true});\n}}\n\n                        })                };                });            </script>        </div>\n```\n\nConfusion Matrix\n:::\n:::\n\n\n# Conclusion\n\nIn this post we saw that TF-IDF embeddings can produce very informative features for tasks like text-classification. We also saw how we can handle features, like TF-IDF, that are sparse to improve computational effiency. \n\nFinally, for multi-class single classification using multinomial logistic regression we walked through how to extend the basic principles that impose the IIA assumption to a model that captures the covariances between the classes removing the IIA restrictions.\n\nI hope you found this post interesting and informative, and I hope to see you in the next one!\n\n",
    "supporting": [
      "index_files/figure-html"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n<script type=\"text/javascript\">\nwindow.PlotlyConfig = {MathJaxConfig: 'local'};\nif (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\nif (typeof require !== 'undefined') {\nrequire.undef(\"plotly\");\nrequirejs.config({\n    paths: {\n        'plotly': ['https://cdn.plot.ly/plotly-2.35.2.min']\n    }\n});\nrequire(['plotly'], function(Plotly) {\n    window._Plotly = Plotly;\n});\n}\n</script>\n\n<script src=\"https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js\" crossorigin=\"anonymous\"></script>\n"
      ],
      "include-after-body": [
        "<script type=application/vnd.jupyter.widget-state+json>\n{\"state\":{\"19b94062a08a4e5cbcce649114c0d518\":{\"model_module\":\"@jupyter-widgets/output\",\"model_module_version\":\"1.0.0\",\"model_name\":\"OutputModel\",\"state\":{\"_dom_classes\":[],\"_model_module\":\"@jupyter-widgets/output\",\"_model_module_version\":\"1.0.0\",\"_model_name\":\"OutputModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/output\",\"_view_module_version\":\"1.0.0\",\"_view_name\":\"OutputView\",\"layout\":\"IPY_MODEL_f4687ceff9cf45c9bd6f9772daa0c563\",\"msg_id\":\"\",\"outputs\":[{\"data\":{\"text/html\":\"<pre style=\\\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\\\">                                                                                                                   \\n <span style=\\\"font-weight: bold\\\"> Progress                  </span> <span style=\\\"font-weight: bold\\\"> Draws </span> <span style=\\\"font-weight: bold\\\"> Divergences </span> <span style=\\\"font-weight: bold\\\"> Step size </span> <span style=\\\"font-weight: bold\\\"> Grad evals </span> <span style=\\\"font-weight: bold\\\"> Sampling Speed </span> <span style=\\\"font-weight: bold\\\"> Elapsed </span> <span style=\\\"font-weight: bold\\\"> Remaining </span> \\n  \\n  <span style=\\\"color: #1f77b4; text-decoration-color: #1f77b4\\\"></span><span style=\\\"color: #3a3a3a; text-decoration-color: #3a3a3a\\\"></span>   215     0             0.00        1023         1.24 s/draws     0:04:28   2:02:57    \\n  <span style=\\\"color: #1f77b4; text-decoration-color: #1f77b4\\\"></span><span style=\\\"color: #3a3a3a; text-decoration-color: #3a3a3a\\\"></span>   233     0             0.02        255          1.15 s/draws     0:04:28   0:44:18    \\n  <span style=\\\"color: #1f77b4; text-decoration-color: #1f77b4\\\"></span><span style=\\\"color: #3a3a3a; text-decoration-color: #3a3a3a\\\"></span>   216     0             0.00        1023         1.23 s/draws     0:04:28   2:01:14    \\n  <span style=\\\"color: #1f77b4; text-decoration-color: #1f77b4\\\"></span><span style=\\\"color: #3a3a3a; text-decoration-color: #3a3a3a\\\"></span>   229     0             0.01        511          1.16 s/draws     0:04:28   1:01:05    \\n  <span style=\\\"color: #1f77b4; text-decoration-color: #1f77b4\\\"></span><span style=\\\"color: #3a3a3a; text-decoration-color: #3a3a3a\\\"></span>   236     0             0.01        511          1.14 s/draws     0:04:28   1:01:00    \\n  <span style=\\\"color: #1f77b4; text-decoration-color: #1f77b4\\\"></span><span style=\\\"color: #3a3a3a; text-decoration-color: #3a3a3a\\\"></span>   230     0             0.01        511          1.16 s/draws     0:04:28   1:05:47    \\n  <span style=\\\"color: #1f77b4; text-decoration-color: #1f77b4\\\"></span><span style=\\\"color: #3a3a3a; text-decoration-color: #3a3a3a\\\"></span>   216     0             0.00        1023         1.24 s/draws     0:04:28   2:01:27    \\n  <span style=\\\"color: #1f77b4; text-decoration-color: #1f77b4\\\"></span><span style=\\\"color: #3a3a3a; text-decoration-color: #3a3a3a\\\"></span>   228     0             0.01        511          1.17 s/draws     0:04:28   1:17:47    \\n                                                                                                                   \\n</pre>\\n\",\"text/plain\":\"                                                                                                                   \\n \\u001b[1m \\u001b[0m\\u001b[1mProgress                 \\u001b[0m\\u001b[1m \\u001b[0m \\u001b[1m \\u001b[0m\\u001b[1mDraws\\u001b[0m\\u001b[1m \\u001b[0m \\u001b[1m \\u001b[0m\\u001b[1mDivergences\\u001b[0m\\u001b[1m \\u001b[0m \\u001b[1m \\u001b[0m\\u001b[1mStep size\\u001b[0m\\u001b[1m \\u001b[0m \\u001b[1m \\u001b[0m\\u001b[1mGrad evals\\u001b[0m\\u001b[1m \\u001b[0m \\u001b[1m \\u001b[0m\\u001b[1mSampling Speed\\u001b[0m\\u001b[1m \\u001b[0m \\u001b[1m \\u001b[0m\\u001b[1mElapsed\\u001b[0m\\u001b[1m \\u001b[0m \\u001b[1m \\u001b[0m\\u001b[1mRemaining\\u001b[0m\\u001b[1m \\u001b[0m \\n  \\n  \\u001b[38;2;31;119;180m\\u001b[0m\\u001b[38;2;31;119;180m\\u001b[0m\\u001b[38;5;237m\\u001b[0m   215     0             0.00        1023         1.24 s/draws     0:04:28   2:02:57    \\n  \\u001b[38;2;31;119;180m\\u001b[0m\\u001b[38;2;31;119;180m\\u001b[0m\\u001b[38;5;237m\\u001b[0m   233     0             0.02        255          1.15 s/draws     0:04:28   0:44:18    \\n  \\u001b[38;2;31;119;180m\\u001b[0m\\u001b[38;2;31;119;180m\\u001b[0m\\u001b[38;5;237m\\u001b[0m   216     0             0.00        1023         1.23 s/draws     0:04:28   2:01:14    \\n  \\u001b[38;2;31;119;180m\\u001b[0m\\u001b[38;2;31;119;180m\\u001b[0m\\u001b[38;5;237m\\u001b[0m   229     0             0.01        511          1.16 s/draws     0:04:28   1:01:05    \\n  \\u001b[38;2;31;119;180m\\u001b[0m\\u001b[38;2;31;119;180m\\u001b[0m\\u001b[38;5;237m\\u001b[0m   236     0             0.01        511          1.14 s/draws     0:04:28   1:01:00    \\n  \\u001b[38;2;31;119;180m\\u001b[0m\\u001b[38;2;31;119;180m\\u001b[0m\\u001b[38;5;237m\\u001b[0m   230     0             0.01        511          1.16 s/draws     0:04:28   1:05:47    \\n  \\u001b[38;2;31;119;180m\\u001b[0m\\u001b[38;2;31;119;180m\\u001b[0m\\u001b[38;5;237m\\u001b[0m   216     0             0.00        1023         1.24 s/draws     0:04:28   2:01:27    \\n  \\u001b[38;2;31;119;180m\\u001b[0m\\u001b[38;2;31;119;180m\\u001b[0m\\u001b[38;5;237m\\u001b[0m   228     0             0.01        511          1.17 s/draws     0:04:28   1:17:47    \\n                                                                                                                   \\n\"},\"metadata\":{},\"output_type\":\"display_data\"}],\"tabbable\":null,\"tooltip\":null}},\"20854763f17640428a92e1c1ff557d49\":{\"model_module\":\"@jupyter-widgets/output\",\"model_module_version\":\"1.0.0\",\"model_name\":\"OutputModel\",\"state\":{\"_dom_classes\":[],\"_model_module\":\"@jupyter-widgets/output\",\"_model_module_version\":\"1.0.0\",\"_model_name\":\"OutputModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/output\",\"_view_module_version\":\"1.0.0\",\"_view_name\":\"OutputView\",\"layout\":\"IPY_MODEL_64fb8b230c1b4bfa9f17f36df59a6ab4\",\"msg_id\":\"ec9ac2e5-3b8dc03d7483d3217744c6a3_95098_11\",\"outputs\":[{\"data\":{\"text/html\":\"<pre style=\\\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\\\">                                                                                                                   \\n <span style=\\\"font-weight: bold\\\"> Progress                  </span> <span style=\\\"font-weight: bold\\\"> Draws </span> <span style=\\\"font-weight: bold\\\"> Divergences </span> <span style=\\\"font-weight: bold\\\"> Step size </span> <span style=\\\"font-weight: bold\\\"> Grad evals </span> <span style=\\\"font-weight: bold\\\"> Sampling Speed </span> <span style=\\\"font-weight: bold\\\"> Elapsed </span> <span style=\\\"font-weight: bold\\\"> Remaining </span> \\n  \\n  <span style=\\\"color: #1f77b4; text-decoration-color: #1f77b4\\\"></span><span style=\\\"color: #3a3a3a; text-decoration-color: #3a3a3a\\\"></span>   788     0             0.03        127          2.96 draws/s     0:04:26   0:06:29    \\n  <span style=\\\"color: #1f77b4; text-decoration-color: #1f77b4\\\"></span><span style=\\\"color: #3a3a3a; text-decoration-color: #3a3a3a\\\"></span>   787     0             0.03        127          2.96 draws/s     0:04:26   0:06:42    \\n  <span style=\\\"color: #1f77b4; text-decoration-color: #1f77b4\\\"></span><span style=\\\"color: #3a3a3a; text-decoration-color: #3a3a3a\\\"></span>   779     0             0.04        127          2.93 draws/s     0:04:26   0:06:45    \\n  <span style=\\\"color: #1f77b4; text-decoration-color: #1f77b4\\\"></span><span style=\\\"color: #3a3a3a; text-decoration-color: #3a3a3a\\\"></span>   783     0             0.04        127          2.94 draws/s     0:04:26   0:06:49    \\n                                                                                                                   \\n</pre>\\n\",\"text/plain\":\"                                                                                                                   \\n \\u001b[1m \\u001b[0m\\u001b[1mProgress                 \\u001b[0m\\u001b[1m \\u001b[0m \\u001b[1m \\u001b[0m\\u001b[1mDraws\\u001b[0m\\u001b[1m \\u001b[0m \\u001b[1m \\u001b[0m\\u001b[1mDivergences\\u001b[0m\\u001b[1m \\u001b[0m \\u001b[1m \\u001b[0m\\u001b[1mStep size\\u001b[0m\\u001b[1m \\u001b[0m \\u001b[1m \\u001b[0m\\u001b[1mGrad evals\\u001b[0m\\u001b[1m \\u001b[0m \\u001b[1m \\u001b[0m\\u001b[1mSampling Speed\\u001b[0m\\u001b[1m \\u001b[0m \\u001b[1m \\u001b[0m\\u001b[1mElapsed\\u001b[0m\\u001b[1m \\u001b[0m \\u001b[1m \\u001b[0m\\u001b[1mRemaining\\u001b[0m\\u001b[1m \\u001b[0m \\n  \\n  \\u001b[38;2;31;119;180m\\u001b[0m\\u001b[38;2;31;119;180m\\u001b[0m\\u001b[38;5;237m\\u001b[0m   788     0             0.03        127          2.96 draws/s     0:04:26   0:06:29    \\n  \\u001b[38;2;31;119;180m\\u001b[0m\\u001b[38;2;31;119;180m\\u001b[0m\\u001b[38;5;237m\\u001b[0m   787     0             0.03        127          2.96 draws/s     0:04:26   0:06:42    \\n  \\u001b[38;2;31;119;180m\\u001b[0m\\u001b[38;2;31;119;180m\\u001b[0m\\u001b[38;5;237m\\u001b[0m   779     0             0.04        127          2.93 draws/s     0:04:26   0:06:45    \\n  \\u001b[38;2;31;119;180m\\u001b[0m\\u001b[38;2;31;119;180m\\u001b[0m\\u001b[38;5;237m\\u001b[0m   783     0             0.04        127          2.94 draws/s     0:04:26   0:06:49    \\n                                                                                                                   \\n\"},\"metadata\":{},\"output_type\":\"display_data\"}],\"tabbable\":null,\"tooltip\":null}},\"24fc3c4e66b14dd894fb25d30e0bab11\":{\"model_module\":\"@jupyter-widgets/output\",\"model_module_version\":\"1.0.0\",\"model_name\":\"OutputModel\",\"state\":{\"_dom_classes\":[],\"_model_module\":\"@jupyter-widgets/output\",\"_model_module_version\":\"1.0.0\",\"_model_name\":\"OutputModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/output\",\"_view_module_version\":\"1.0.0\",\"_view_name\":\"OutputView\",\"layout\":\"IPY_MODEL_cb51a809c05241f39fa8c231726c8f41\",\"msg_id\":\"\",\"outputs\":[{\"data\":{\"text/html\":\"<pre style=\\\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\\\">Sampling ... <span style=\\\"color: #008000; text-decoration-color: #008000\\\"></span> <span style=\\\"color: #800080; text-decoration-color: #800080\\\">100%</span> 0:00:00 / 0:00:08\\n</pre>\\n\",\"text/plain\":\"Sampling ... \\u001b[32m\\u001b[0m \\u001b[35m100%\\u001b[0m 0:00:00 / 0:00:08\\n\"},\"metadata\":{},\"output_type\":\"display_data\"}],\"tabbable\":null,\"tooltip\":null}},\"64fb8b230c1b4bfa9f17f36df59a6ab4\":{\"model_module\":\"@jupyter-widgets/base\",\"model_module_version\":\"2.0.0\",\"model_name\":\"LayoutModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/base\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"LayoutModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"LayoutView\",\"align_content\":null,\"align_items\":null,\"align_self\":null,\"border_bottom\":null,\"border_left\":null,\"border_right\":null,\"border_top\":null,\"bottom\":null,\"display\":null,\"flex\":null,\"flex_flow\":null,\"grid_area\":null,\"grid_auto_columns\":null,\"grid_auto_flow\":null,\"grid_auto_rows\":null,\"grid_column\":null,\"grid_gap\":null,\"grid_row\":null,\"grid_template_areas\":null,\"grid_template_columns\":null,\"grid_template_rows\":null,\"height\":null,\"justify_content\":null,\"justify_items\":null,\"left\":null,\"margin\":null,\"max_height\":null,\"max_width\":null,\"min_height\":null,\"min_width\":null,\"object_fit\":null,\"object_position\":null,\"order\":null,\"overflow\":null,\"padding\":null,\"right\":null,\"top\":null,\"visibility\":null,\"width\":null}},\"6ab51400f0b34dc29d557d2daabf6a84\":{\"model_module\":\"@jupyter-widgets/output\",\"model_module_version\":\"1.0.0\",\"model_name\":\"OutputModel\",\"state\":{\"_dom_classes\":[],\"_model_module\":\"@jupyter-widgets/output\",\"_model_module_version\":\"1.0.0\",\"_model_name\":\"OutputModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/output\",\"_view_module_version\":\"1.0.0\",\"_view_name\":\"OutputView\",\"layout\":\"IPY_MODEL_95f3acd2697c476bb54dbad87a272bf2\",\"msg_id\":\"\",\"outputs\":[{\"data\":{\"text/html\":\"<pre style=\\\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\\\">Sampling ... <span style=\\\"color: #008000; text-decoration-color: #008000\\\"></span> <span style=\\\"color: #800080; text-decoration-color: #800080\\\">100%</span> 0:00:00 / 0:00:17\\n</pre>\\n\",\"text/plain\":\"Sampling ... \\u001b[32m\\u001b[0m \\u001b[35m100%\\u001b[0m 0:00:00 / 0:00:17\\n\"},\"metadata\":{},\"output_type\":\"display_data\"}],\"tabbable\":null,\"tooltip\":null}},\"95f3acd2697c476bb54dbad87a272bf2\":{\"model_module\":\"@jupyter-widgets/base\",\"model_module_version\":\"2.0.0\",\"model_name\":\"LayoutModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/base\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"LayoutModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"LayoutView\",\"align_content\":null,\"align_items\":null,\"align_self\":null,\"border_bottom\":null,\"border_left\":null,\"border_right\":null,\"border_top\":null,\"bottom\":null,\"display\":null,\"flex\":null,\"flex_flow\":null,\"grid_area\":null,\"grid_auto_columns\":null,\"grid_auto_flow\":null,\"grid_auto_rows\":null,\"grid_column\":null,\"grid_gap\":null,\"grid_row\":null,\"grid_template_areas\":null,\"grid_template_columns\":null,\"grid_template_rows\":null,\"height\":null,\"justify_content\":null,\"justify_items\":null,\"left\":null,\"margin\":null,\"max_height\":null,\"max_width\":null,\"min_height\":null,\"min_width\":null,\"object_fit\":null,\"object_position\":null,\"order\":null,\"overflow\":null,\"padding\":null,\"right\":null,\"top\":null,\"visibility\":null,\"width\":null}},\"cb51a809c05241f39fa8c231726c8f41\":{\"model_module\":\"@jupyter-widgets/base\",\"model_module_version\":\"2.0.0\",\"model_name\":\"LayoutModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/base\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"LayoutModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"LayoutView\",\"align_content\":null,\"align_items\":null,\"align_self\":null,\"border_bottom\":null,\"border_left\":null,\"border_right\":null,\"border_top\":null,\"bottom\":null,\"display\":null,\"flex\":null,\"flex_flow\":null,\"grid_area\":null,\"grid_auto_columns\":null,\"grid_auto_flow\":null,\"grid_auto_rows\":null,\"grid_column\":null,\"grid_gap\":null,\"grid_row\":null,\"grid_template_areas\":null,\"grid_template_columns\":null,\"grid_template_rows\":null,\"height\":null,\"justify_content\":null,\"justify_items\":null,\"left\":null,\"margin\":null,\"max_height\":null,\"max_width\":null,\"min_height\":null,\"min_width\":null,\"object_fit\":null,\"object_position\":null,\"order\":null,\"overflow\":null,\"padding\":null,\"right\":null,\"top\":null,\"visibility\":null,\"width\":null}},\"f4687ceff9cf45c9bd6f9772daa0c563\":{\"model_module\":\"@jupyter-widgets/base\",\"model_module_version\":\"2.0.0\",\"model_name\":\"LayoutModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/base\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"LayoutModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"LayoutView\",\"align_content\":null,\"align_items\":null,\"align_self\":null,\"border_bottom\":null,\"border_left\":null,\"border_right\":null,\"border_top\":null,\"bottom\":null,\"display\":null,\"flex\":null,\"flex_flow\":null,\"grid_area\":null,\"grid_auto_columns\":null,\"grid_auto_flow\":null,\"grid_auto_rows\":null,\"grid_column\":null,\"grid_gap\":null,\"grid_row\":null,\"grid_template_areas\":null,\"grid_template_columns\":null,\"grid_template_rows\":null,\"height\":null,\"justify_content\":null,\"justify_items\":null,\"left\":null,\"margin\":null,\"max_height\":null,\"max_width\":null,\"min_height\":null,\"min_width\":null,\"object_fit\":null,\"object_position\":null,\"order\":null,\"overflow\":null,\"padding\":null,\"right\":null,\"top\":null,\"visibility\":null,\"width\":null}}},\"version_major\":2,\"version_minor\":0}\n</script>\n"
      ]
    }
  }
}