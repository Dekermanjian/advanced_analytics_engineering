{
  "hash": "08f55d19287fbe4bb3b73d681f284abd",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Reproducibility & Scalability Part 3<br><sub>MLFlow Integration</sub>\"\nauthor: \"Jonathan Dekermanjian\"\ndate: \"2025-03-09\"\ncategories: [engineering]\ndraft: true\ncode-annotations: below\n---\n\n\n\n\n# Overview\nIn part three of this series, we configure MLFlow for production by defining our tracking server as a containerized PostgreSQL database and our artifact store as containerized MinIO storage. We then integrate MLFlow into our Kedro project in order to start tracking our model development.\n\n# Introduction\n\n# Getting Started\nFirst thing we need to do is to add `MLFlow` as a dependency into our `environment.yml` file. We are also going to need to install `psycopg2` to allow `MLFlow` to interact with `PostgreSQL`.\n\n:::{.callout-caution}\nDepending on what opperating system you are using you may need to install `psycopg2-binary` intead of `psycopg2`. In this post we are going to be running everything from inside containers that are running `Linux`. So we will be using `psycopg2-binary`.\n:::\n\n```yml\nname: my_environment_name\nchannels:\n  - conda-forge\n  - defaults\n\ndependencies:\n  - 'pymc=5.20.1'\n  - 'graphviz=12.2.1' # New dependency\n  - 'numpyro=0.17.0'\n  - pip:\n    - ipython>=8.10\n    - jupyterlab>=3.0\n    - kedro-datasets>=3.0\n    - kedro-viz>=6.7.0\n    - kedro[jupyter]~=0.19.11\n    - notebook\n    - pre-commit~=3.5\n    - polars~=1.23.0\n    - pyarrow~=18.1.0\n    - plotly~=5.24.0\n    - openpyxl~=3.1.0\n    - kedro-docker~=0.6.2\n    - pymc-extras==0.2.3\n    - mlflow==2.19.0 # New dependency\n    - psycopg2-binary==2.9.10 # New dependency\n```\n\n# Setting up MLFlow\nNow is a good time to transition from running our pipelines in virtual environments on our machine to running inside of containers. Let's modify the `Dockerfile` that we generated in part one of this series when we introduced the `kedro-docker` plugin. We are going to switch from using `UV` and `pip` to manage our dependencies when building our image to using `Conda`.\n\n```dockerfile\n#./Dockerfile\nARG BASE_IMAGE=python:3.9-slim\nFROM $BASE_IMAGE as runtime-environment\n\n# install base utils\nRUN apt-get update \\\n    && apt-get install -y build-essential \\\n    && apt-get install -y wget \\\n    && apt-get install -y git \\\n    && apt-get clean \\\n    && rm -rf /var/lib/apt/lists/*\n\n# install miniconda\nENV CONDA_DIR /opt/conda\nRUN wget --quiet https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-aarch64.sh -O ~/miniconda.sh && \\\n    /bin/bash ~/miniconda.sh -b -p /opt/conda\n\n# put conda in path\nENV PATH=$CONDA_DIR/bin:$PATH\n\n# install project requirements\nCOPY environment.yml /tmp/environment.yml\nRUN conda env update --name base --file /tmp/environment.yml --prune\n\n# add kedro user\nARG KEDRO_UID=999\nARG KEDRO_GID=0\nRUN groupadd -f -g ${KEDRO_GID} kedro_group && \\\n    useradd -m -d /home/kedro_docker -s /bin/bash -g ${KEDRO_GID} -u ${KEDRO_UID} kedro_docker\n\nWORKDIR /home/kedro_docker\nUSER kedro_docker\n\nFROM runtime-environment\n\n# copy the whole project except what is in .dockerignore\nARG KEDRO_UID=999\nARG KEDRO_GID=0\nCOPY --chown=${KEDRO_UID}:${KEDRO_GID} . .\n\nEXPOSE 8888\n\nCMD [\"kedro\", \"run\"]\n```\nNow that our `Kedro` project can run inside a container let's configure `MLFlow`. We need to configure the following:\n\n* Define a backend store where metrics, parameters, and metadata are stored\n* Define an artifact store where large files like models and figures are stored\n* Define a tracking server that is listening to incoming requests\n* Allow the server, storage, and our pipelines to communincate with each other\n\n## Backend Store\nAs we mentioned earlier, we are going to use PostgreSQL to store our metrics, parameters, and other metadata. In a docker compose file, let's define our PostgreSQL service. You can name the service however you like; we will name it `mlflow-logs`. We also need to define a username, password, and database name for our service. We will name our database `mlflowdb`. We are going to pass in the username and password as an environment variable using secrets. For that you need to make sure you set those variables in your environment. On Linux/MacOS you can set an environment variable like so:\n\n```zsh\nexport POSTGRES_USER=some_user_name\n```\n\nOn Windows using PowerShell you can set it like this:\n\n```powershell\n$env:POSTGRES_USER = \"some_user_name\"\n```\n\nIn addition, we need to make sure that what we log into our database persists after the container is brought down. We will define a docker volume and call it `pg_mlflow_db`. Our `docker-compose.yml` file looks like this so far:\n\n```yml\n#./docker-compose.yml\nservices:\n  mlflow-logs:\n    image: postgres:16.4\n    secrets:\n      - POSTGRES_USER\n      - POSTGRES_PASSWORD\n    environment:\n      - POSTGRES_USER_FILE=/run/secrets/POSTGRES_USER\n      - POSTGRES_PASSWORD_FILE=/run/secrets/POSTGRES_PASSWORD\n      - POSTGRES_DB=mlflowdb\n    ports:\n      - 5432:5432\n    volumes:\n      - pg_mlflow_db:/var/lib/postgresql/data\n\nvolumes:\n  pg_mlflow_db:\n\nsecrets:\n  POSTGRES_USER:\n    environment: POSTGRES_USER\n  POSTGRES_PASSWORD:\n    environment: POSTGRES_PASSWORD\n```\n\n:::{.callout-caution}\nNote how we are using `POSTGRES_USER_FILE` instead of `POSTGRES_USER`. This is a convention used by many docker images for passing in secrets.\n:::\n\n## Artifact Store\nWe need a place to store larger file objects like our models, figures, or artifacts like preprocessors. For that, we are going to define another service in our `docker-compose.yml` that will host a `MinIO` S3-compatible object store. Again, we define a username and password like we did before. Let's also define a default bucket that will be created on creation of the service to store our artifacts. Again, like we did for our backend store our artifact store needs a persistant volume. So far we have:\n\n```yml\n#./docker-compose.yml\nservices:\n  mlflow-logs:\n    image: postgres:16.4\n    secrets:\n      - POSTGRES_USER\n      - POSTGRES_PASSWORD\n    environment:\n      - POSTGRES_USER_FILE=/run/secrets/POSTGRES_USER\n      - POSTGRES_PASSWORD_FILE=/run/secrets/POSTGRES_PASSWORD\n      - POSTGRES_DB=mlflowdb\n    ports:\n      - 5432:5432\n    volumes:\n      - pg_mlflow_db:/var/lib/postgresql/data\n  # New MinIO service\n  minio:\n    container_name: minio\n    image: 'bitnami/minio:latest'\n    ports:\n      - '9000:9000'\n      - '9001:9001'\n    secrets:\n      - AWS_ACCESS_KEY_ID\n      - AWS_SECRET_ACCESS_KEY\n    environment:\n      - MINIO_ROOT_USER_FILE=/run/secrets/AWS_ACCESS_KEY_ID\n      - MINIO_ROOT_PASSWORD_FILE=/run/secrets/AWS_SECRET_ACCESS_KEY\n      - MINIO_DEFAULT_BUCKETS=mlflow\n    volumes:\n      - minio_data:/bitnami/minio/data\n\nvolumes:\n  pg_mlflow_db:\n  minio_data:\n\nsecrets:\n  AWS_ACCESS_KEY_ID:\n    environment: AWS_ACCESS_KEY_ID\n  AWS_SECRET_ACCESS_KEY:\n    environment: AWS_SECRET_ACCESS_KEY\n  POSTGRES_USER:\n    environment: POSTGRES_USER\n  POSTGRES_PASSWORD:\n    environment: POSTGRES_PASSWORD\n```\n\n## Tracking Server\nWe are now ready to define the MLFlow tracking-server and configure it so that it can communicate with our backend store and artifact store. The key here is to define the proper environment variables:\n\n- <b>MLFLOW_BACKEND_STORE_URI</b>: This will point to our PostgreSQL service has the form `postgresql://username:password@service_name:port/database_name`\n- <b>MLFLOW_ARTIFACTS_DESTINATION</b>: This will point to our storage bucket and has the form `s3://bucket_name`\n- <b>MLFLOW_S3_ENDPOINT_URL</b>: This will point to our MinIO service and has the form `http://service_name:port`\n- <b>AWS_ACCESS_KEY_ID</b>: This is the access key ID to our MinIO service. It can be the root user (not recommended) or bucket specific access key\n- <b>AWS_SECRET_ACCESS_KEY</b>: This is the secret access key ID to our MinIO service. It can be the root password (not recommended) or bucket specific secret access key\n\nLet's go ahead and add our tracking-server service to our compose file:\n\n```yml\n#./docker-compose.yml\nservices:\n  mlflow-logs:\n    image: postgres:16.4\n    secrets:\n      - POSTGRES_USER\n      - POSTGRES_PASSWORD\n    environment:\n      - POSTGRES_USER_FILE=/run/secrets/POSTGRES_USER\n      - POSTGRES_PASSWORD_FILE=/run/secrets/POSTGRES_PASSWORD\n      - POSTGRES_DB=mlflowdb\n    ports:\n      - 5432:5432\n    volumes:\n      - pg_mlflow_db:/var/lib/postgresql/data\n\n  minio:\n    container_name: minio\n    image: 'bitnami/minio:latest'\n    ports:\n      - '9000:9000'\n      - '9001:9001'\n    secrets:\n      - AWS_ACCESS_KEY_ID\n      - AWS_SECRET_ACCESS_KEY\n    environment:\n      - MINIO_ROOT_USER_FILE=/run/secrets/AWS_ACCESS_KEY_ID\n      - MINIO_ROOT_PASSWORD_FILE=/run/secrets/AWS_SECRET_ACCESS_KEY\n      - MINIO_DEFAULT_BUCKETS=mlflow\n    volumes:\n      - minio_data:/bitnami/minio/data\n  # new tracking server service\n  mlflow-tracking-server:\n    image: ghcr.io/mlflow/mlflow:v2.19.0\n    command: >\n      bash -c \"\n          pip install -U pip\n          pip install psycopg2-binary boto3\n          mlflow server --host 0.0.0.0 --port 5000 --workers 1\n      \"\n    ports:\n      - 5050:5000\n    environment:\n      - MLFLOW_BACKEND_STORE_URI=postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@mlflow-logs:5432/mlflowdb\n      - MLFLOW_ARTIFACTS_DESTINATION=s3://mlflow\n      - MLFLOW_S3_ENDPOINT_URL=http://minio:9000\n      - MLFLOW_S3_IGNORE_TLS=true\n      - AWS_ACCESS_KEY_ID\n      - AWS_SECRET_ACCESS_KEY\n    depends_on:\n      - minio\n      - mlflow-logs\n    stop_grace_period: 1s\n\nvolumes:\n  pg_mlflow_db:\n  minio_data:\n\nsecrets:\n  AWS_ACCESS_KEY_ID:\n    environment: AWS_ACCESS_KEY_ID\n  AWS_SECRET_ACCESS_KEY:\n    environment: AWS_SECRET_ACCESS_KEY\n  POSTGRES_USER:\n    environment: POSTGRES_USER\n  POSTGRES_PASSWORD:\n    environment: POSTGRES_PASSWORD\n```\n\n:::{.callout-important}\nEnvironment variables that aren't passed in using the /run/secrets method can be viewed by echoing that variable in the container's shell!\n:::\n\n## Kedro Project Image\nThe final service we will be adding in this section is the image of our `Kedro` project. We set a profile of manual on this service so that the container isn't brought up automatically. We do this because it is likely that you want this service to run on a schedule so we just let the scheduler handle bringing the service up. Notice that we set the environment variable `MLFLOW_TRACKING_URI` to the service name and port that is running our MLFlow tracking server. This will allow us to communicate from our Kedro project service.\n\n\n\n\n```{yml}\n#| echo: TRUE\n#| eval: FALSE\n#| code-line-numbers: \"1\"\n#./docker-compose.yml\nservices:\n  mlflow-logs:\n    image: postgres:16.4\n    secrets:\n      - POSTGRES_USER\n      - POSTGRES_PASSWORD\n    environment:\n      - POSTGRES_USER_FILE=/run/secrets/POSTGRES_USER\n      - POSTGRES_PASSWORD_FILE=/run/secrets/POSTGRES_PASSWORD\n      - POSTGRES_DB=mlflowdb\n    ports:\n      - 5432:5432\n    volumes:\n      - pg_mlflow_db:/var/lib/postgresql/data\n\n  minio:\n    container_name: minio\n    image: 'bitnami/minio:latest'\n    ports:\n      - '9000:9000'\n      - '9001:9001'\n    secrets:\n      - AWS_ACCESS_KEY_ID\n      - AWS_SECRET_ACCESS_KEY\n    environment:\n      - MINIO_ROOT_USER_FILE=/run/secrets/AWS_ACCESS_KEY_ID\n      - MINIO_ROOT_PASSWORD_FILE=/run/secrets/AWS_SECRET_ACCESS_KEY\n      - MINIO_DEFAULT_BUCKETS=mlflow\n    volumes:\n      - minio_data:/bitnami/minio/data\n\n  mlflow-tracking-server:\n    image: ghcr.io/mlflow/mlflow:v2.19.0\n    command: >\n        bash -c \"\n            pip install -U pip\n            pip install psycopg2-binary boto3\n            mlflow server --host 0.0.0.0 --port 5000 --workers 1\n        \"\n    ports:\n      - 5050:5000\n    environment:\n      - MLFLOW_BACKEND_STORE_URI=postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@mlflow-logs:5432/mlflowdb\n      - MLFLOW_ARTIFACTS_DESTINATION=s3://mlflow\n      - MLFLOW_S3_ENDPOINT_URL=http://minio:9000\n      - MLFLOW_S3_IGNORE_TLS=true\n      - AWS_ACCESS_KEY_ID\n      - AWS_SECRET_ACCESS_KEY\n    depends_on:\n      - minio\n      - mlflow-logs\n    stop_grace_period: 1s\n  # New service added\n  climate-brazil:\n    profiles:\n      - manual\n    image: climate-brazil:latest\n    build: .\n    environment:\n      - MLFLOW_TRACKING_URI=http://mlflow-tracking-server:5000\n    depends_on:\n      - mlflow-tracking-server\n      - minio\n      - mlflow-logs\n\nvolumes:\n  pg_mlflow_db:\n  minio_data:\n\nsecrets:\n  AWS_ACCESS_KEY_ID:\n    environment: AWS_ACCESS_KEY_ID\n  AWS_SECRET_ACCESS_KEY:\n    environment: AWS_SECRET_ACCESS_KEY\n  POSTGRES_USER:\n    environment: POSTGRES_USER\n  POSTGRES_PASSWORD:\n    environment: POSTGRES_PASSWORD\n```\n\n\n\n\n# Integrating MLFlow\nNow that we have our services defined we need to modify our `Kedro` project to interact with the MLFlow tracking server. For that we are going to introduce hooks; another useful `Kedro` feature.\n\n## Kedro Hooks\nHooks are a great way to add additional functionality to your pipelines at specific points during or before execution. Common use cases of hooks are:\n\n* Injecting additional logging behavior before/after pipeline/node execution\n* Validating input data is in the correct format before executing a node\n* Debugging a node/pipeline\n* Tracking/Profiling resource utilization\n* Customizing load and save methods\n\nWe are going to use hooks to inject additional logging and customize saving methods by logging metrics and parameters, and saving models/artifacts using `MLFlow`.\n\n## Tracking Priors\n\n\n## Tracking Sampler Configurations\n\n## Tracking Model Specifications\n\n## Logging Divergences\n\n## Logging Metrics\n\n## Saving a Custom Model\n\n# Summary\n\n# Coming Next\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}