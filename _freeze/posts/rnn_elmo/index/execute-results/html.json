{
  "hash": "c9a9cf238d0170b26ca93334b07d14e0",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"How Machines Comprehend Language<br><sub>Recurrent Neural Networks - Embeddings from Language Models (ELMo)</sub>\"\nauthor: \"Jonathan Dekermanjian\"\ndate: \"2026-01-31\"\ncategories: [Natural Language Processing]\ncode-annotations: below\ncss: styles.css\njupyter: jax_env\nimage: \"thumbnail.png\"\nexecute: \n  cache: true\nbibliography: references.bib\nnocite: |\n  @*\n---\n\n# Overview\n\nWe go over contextual embeddings and recurrent neural networks. Describing their inner workings and walking through an implementation of Embeddings from Language Models (ELMo) from scratch, using the JAX ecosystem.\n\n# Introduction\n\nIn previous posts we learned what embeddings are, why they are important and how they can be levaraged in practice. We got our hands dirty by implementing two of the earliest foundational methods GloVe and Skip-gram with negative sampling (SGNS). \n\nIf you need a refresher or just hadn't seen them you can check out my posts on [GloVe](/posts/glove_embeddings/index.qmd) and [SGNS](/posts/skipgram_embeddings/index.qmd). \n\nIn this post we will briefly review the differences between static and contextual embeddings and discuss some of the methodologies used to create contextual embeddings. Leading us to the architecture of recurrent neural networks and how they are leveraged in contextual embeddings. \n\nFinally, we will delve into the architecture of Embeddings from Language Models (ELMo), and implement it from scratch.\n\n# From Static to Contextual Embeddings\n\nAs a reminder, a strength of contextual embeddings is the ability to discriminate between polysemous words. For example, the word \"ship\" can refer to the action verb of shipping an item or to the noun of a water vessel. A static embedding would represent both meanings of the word \"ship\" with the same vector. In contrast, contextual embeddings are able to disambiguate the two meanings of the same word. \n\nSince our vectors now need to be aware of context, the sequence in which words occur must influence the resultant vectors. Importantly, not only during training must our vectors be aware of context but also during inference. Therefore, we must build vector embeddings dynamically (on the fly) during inference to ensure that this is the case. This contrasts with earlier methods like SGNS or GloVe:\n\n* SGNS trains a vector for each word by predicting its local context, but at inference it simply looks up the pre‑computed static vector—there’s no per‑sentence adaptation.\n\n* GloVe builds a word‑co‑occurrence matrix and factorizes it, again yielding one static vector per word that is reused for every sentence.\n\nOne way we include information from sequences is by using recurrent computation. This brings us to the topic of recurrent neural networks.\n\n\n# Recurrent Neural Networks\n\nA recurrent neural network (RNN) is a neural architecture designed for sequential data, in which information from previous time steps is carried forward through a recurrent hidden state. At each time step, the hidden state is updated based on the current input and the previous hidden state, and the output is computed from this hidden representation.\n\nWe have two equations that define a simple RNN, also known as an Elman RNN.\n\n$$\nh_{t} = \\phi(W_{h}h_{t-1} + W_{x}x_{t} + b)\n$$\n\n$$\ny_{t} = \\psi(W_{y}h_{t} + b_{y})\n$$\n\nWhere $x_{t}$, $h_{t}$, and $y_{t}$ are the input, hidden state, and output at time t, respectively. The weight matrices $W$ are linear operators that project variables from one space into another. For example, $W_{x}$ projects $x_{t}$ into the hidden state space. Notice that these weight matrices are not indexed by t, signifying that they are shared across time and allows RNNs to generalize to varying sequence lengths.  \n\n::: {style=\"background-color:#dcdce0; padding:1em; border-radius:6px;\"}\n```{mermaid}\n%%| label: fig-elman-rnn\n%%| fig-cap: \"Elman (Simple) Recurrent Neural Network\"\n%%| eval: true\n\nflowchart BT\n  X[$$x_t$$] -->|input| H[$$h_t$$]\n  H -->|output| Y[$$y_t$$]\n  H -->|recurrent state| H\n\n  style H fill:#eef,stroke:#333,stroke-width:1.5px\n\n```\n\n:::\n\n\nThe Elman RNN maintains only one hidden state, at each time step the same computation must compress all past information into a single $h_{t}$. This is challenging to do for longer time dependencies. Additionally, Elman RNNs are prone to vanishing/exploding gradients due to the gradients needing to repeatedly pass through $W_{h}$ and the nonlinearity $\\phi$, via backpropagation through time. In practice, these limitations are enough to opt for a modified RNN architecture. \n\n\n## Long Short-Term Memory\n\nA Long Short-Term Memory (LSTM) RNN is an architectural extension of the standard Elman RNN designed to mitigate the vanishing gradient problem when modeling long-range dependencies. The LSTM introduces gating mechanisms (input, forget, and output gates) that explicitly regulate how much past information is retained and how much new information is incorporated, rather than relying on implicit storage in the recurrent weights as in an Elman RNN. Crucially, the LSTM maintains a cell state whose update follows an additive structure, forming a linear path through time. These additive dynamics enables more stable gradient propagation and substantially reduces vanishing gradients during training.\n\nLet's dive a little bit deeper here. Given an input at time $t$, $x_{t}$, and a previous hidden state $h_{t-1}$ and a previous cell state $c_{t-1}$, we have the following equations that control the evolution of the hidden and cell states over time.\n\nFirst, the forget gate $f_{t}$ is a parametric control for the signal that should be propagated forward from the prior cell state $c_{t-1}$ to the current cell state $c_{t}$ that we are computing.\n\n$$\nf_{t} = \\sigma(W_{f} \\cdot [h_{t-1}, x_{t}] + b_{f})\n$$\n\nThe input gate $i_{t}$, you'll notice, is almost the same computation as the forget gate and its role is to regulate how much signal should pass through from the candidate cell state $\\tilde{c}_{t}$ to the current cell state $c_{t}$.\n\n$$\ni_{t} = \\sigma(W_{i} \\cdot [h_{t-1}, x_{t}] + b_{i})\n$$\n\nOur last gate, the output gate $o_{t}$ is again the same computation as the other gates and its role is to regulate how much of the current cell state $c_{t}$ is exposed to the current hidden state $h_{t}$.\n\n$$\no_{t} = \\sigma(W_{o} \\cdot [h_{t-1}, x_{t}] + b_{o})\n$$\n\nYou may be thinking if all the computations are the same how do the different gates perform different functions? Well, that is a good question and one that is not obvious. However, their distinct roles emerge from how each gate modulates different computational paths in the forward pass, and how gradients propagate through those paths during backpropagation. \n\nIn the remaining equations you can see how these gates act as controls on the input, cell state, and hidden state. \n\n$$\n\\tilde{c}_{t} = \\tanh(W_{c} \\cdot [h_{t-1}, x_{t}] + b_{c})\n$$\n\n$$\nc_{t} = f_{t} \\odot c_{t-1} + i_{t} \\odot \\tilde{c}_{t}\n$$\n\n$$\nh_{t} = o_{t} \\odot \\tanh(c_{t})\n$$\n\nBelow is a graph that depicts the flow of computations that take place to update the hidden state.\n\n::: {style=\"background-color:#dcdce0; padding:1em; border-radius:6px;\"}\n```{mermaid}\n%%| label: fig-lstm-rnn\n%%| fig-cap: \"Long Short-Term Memory Recurrent Neural Network\"\n%%| eval: true\nflowchart BT\n    x_t[\"$$x_{t}$$ (input)\"] --> concat\n    h_prev[\"$$h_{t-1}$$ (prev hidden)\"] --> concat\n\n    concat[\"Concatenate\"] --> f_gate[\"Forget Gate<br> $$\\sigma(W_f \\cdot [h_{t-1}, x_{t}] + b_f)$$\"]\n    concat --> i_gate[\"Input Gate<br/> $$\\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i)$$\"]\n    concat --> c_tilde[\"Candidate State<br/> $$\\tanh(W_c \\cdot [h_{t-1}, x_t] + b_c)$$\"]\n    concat --> o_gate[\"Output Gate<br/> $$σ(W_o \\cdot [h_{t-1}, x_t] + b_o)$$\"]\n\n    c_prev[\"$$c_{t-1}$$ (prev cell)\"] --> mult_f[\"×\"]\n    f_gate --> mult_f\n\n    i_gate --> mult_i[\"×\"]\n    c_tilde --> mult_i\n\n    mult_f --> c_t[\"cₜ (cell state)\"]\n    mult_i --> c_t\n\n    c_t --> tanh_c[\"$$\\tanh$$\"]\n    tanh_c --> mult_o[\"×\"]\n    o_gate --> mult_o\n\n    mult_o --> h_t[\"$$h_t$$ (hidden state)\"]\n\n```\n\n:::\n\n\n# Embeddings from Language Models\n\nNow that we better understanding of RNNs, specifically LSTMs, we are ready to talk about a deep learning architecture used to generate contextual embeddings. The influential Embeddings from Language Models (ELMo) architecture. \n\nBefore transformers became popular ELMo was one of the dominant models for generating contextual embeddings. ELMo blends character‑level convolutional neural networks (CNN) token encodings for handling out-of-vocabulary (OOV) words, multi‑layer bidirectional LSTMs to capture both left-to-right and right-to-left long range dependencies, and a trainable weighted sum of their hidden states to produce word‑level embeddings that are both context‑sensitive and adaptable through fine‑tuning.\n\n\n## Architecture\n\nLet's look at a high-level map of ELMo's architecture. The important sub-components are:\n\n1. <b>Character Level CNN</b>: Converts each token into character-level feature vectors and is well suited to handling OOV words.\n2. <b>Multi-layer bi-directional LSTM</b>: processes the character vectors in both directions, producing hidden states that capture left and right‑context.\n3. <b>Contextual Hidden State</b>: combines forward and backward outputs to form a deep, context‑sensitive representation of each token.\n4. <b>Scalar Mix parameters</b>: Learn a task-specific weighted combination of the representations from different layers, enabling effective transfer of the pretrained language model to downstream tasks without retraining the full model.\n\nBelow is a high-level graph of an ELMo model.\n\n::: {style=\"background-color:#dcdce0; padding:1.5em; border-radius:6px;\"}\n```{mermaid}\n%%| label: fig-elmo-arch\n%%| fig-cap: \"ELMo Architecture\"\n%%| eval: true\nflowchart BT\n\n%% 1 Input\nTokens[\"$$\\text{Token IDs } T_{1} \\text{ to } T_{n}$$\"] --> CharEmb[Character Embedding]\n\n%% 2 Character CNN (h0)\nCharEmb --> CharCNN[Character CNN Output]\nCharCNN --> H0[\"$$h_{0} \\text{ Character-based token representation}$$\"]\n\n%% 3 BiLSTM layers\nsubgraph BiLSTM[BiLSTM]\n    direction RL\n\n    Title[Bidirectional LSTM Stack]\n    style Title fill:none,stroke:none\n\n    subgraph Layer2[Layer 2]\n        F2[Forward LSTM 2] --> HF2[h_fwd_2]\n        B2[Backward LSTM 2] --> HB2[h_bwd_2]\n    end\n\n    subgraph Layer1[Layer 1]\n        F1[Forward LSTM 1] --> HF1[h_fwd_1]\n        B1[Backward LSTM 1] --> HB1[h_bwd_1]\n    end\nend\n\nH0 --> BiLSTM\n\n%% 4 Contextual states\nsubgraph HiddenStates[Contextual Representations]\n    H1[\"$$h_{1} = \\text{concat fwd1 bwd1}$$\"]\n    H2[\"$$h_{2} = \\text{concat fwd2 bwd2}$$\"]\nend\n\nHF1 --> H1\nHB1 --> H1\nHF2 --> H2\nHB2 --> H2\n\n%% 5 Forward & Backward LM heads\nsubgraph LMHeads[LM Heads]\n    direction LR\n\n    LMTitle[Language Modeling Heads]\n    style LMTitle fill:none,stroke:none\n\n    FLM[Forward LM Head] --> FSoftmax[\"Softmax(vocab)\"]\n    BLM[Backward LM Head] --> BSoftmax[\"Softmax(vocab)\"]\nend\n\nHF1 --> FLM\nHF2 --> FLM\nHB1 --> BLM\nHB2 --> BLM\n\n%% 6 ELMo scalar mixer\nsubgraph Mixer[ELMo Scalar Mixer]\n    A0[\"$$a_{0}$$\"] --> Softmax[\"$$\\text{Softmax over } a_{i}$$\"]\n    A1[\"$$a_{1}$$\"] --> Softmax\n    A2[\"$$a_{2}$$\"] --> Softmax\n\n    Softmax --> S0[\"$$s_{0}$$\"]\n    Softmax --> S1[\"$$s_{1}$$\"]\n    Softmax --> S2[\"$$s_{2}$$\"]\n\n    Sum[\"$$\\text{Sum }s_{i}h_{i}$$\"]\n    Gamma[Gamma]\nend\n\nH0 --> Sum\nH1 --> Sum\nH2 --> Sum\n\nS0 --> Sum\nS1 --> Sum\nS2 --> Sum\n\nSum --> Gamma\n\n%% 7 ELMo output\nsubgraph ELMO[ELMo Embedding]\n    ELMOvec[\"ELMo(t)\"]\nend\n\nGamma --> ELMOvec\n\nstyle LMHeads fill:#5eaabf,stroke:#5eaabf,stroke-width:2px\n\n```\n\n:::\n\n## Pretraining Objective\n\nELMo uses a bidirectional language modeling objective to maximize the likelihood of observed tokens given all surrounding context. For ease of understanding, we can breakdown the objective into two directional components.\n\nFirst, we have the forward language model objective which predicts each token $x_{t}$ given all prior tokens.\n\n$$\nP_{fwd}(x) = \\prod_{t=1}^{T} P(x_{t}|x_{1}...x_{t-1})\n$$\n\nSecond, we have the backwards language model objective which as the name implies predicts each token $x_{t}$ given all proceeding tokens.\n\n$$\nP_{bwd}(x) = \\prod_{t=1}^{T} P(x_{t}|x_{T}...x_{t+1})\n$$\n\nThe total pretraining objective is the sum of the forward and backward negative log-likelihoods:\n\n$$\n\\mathcal{L} = - \\sum_{t=1}^{T} \\log P_{fwd}(x_t | x_{<t}) \\;-\\; \\sum_{t=1}^{T} \\log P_{bwd}(x_t | x_{>t})\n$$\n\n# Hands-on Implementation\n\nOkay then, time to get our hands dirty! We are going to build an ELMo model from scratch using the JAX deep learning ecosystem. \n\nSpecifically, we will pre-train the model using the aforementioned language modeling objective. Subsequently, we will fine-tune our scalar mix on a downstream task to evaluate our embeddings. I hope you enjoy!\n\nIn this part of the post, I’ll collapse all but the most important code cells, leaving only the ones I’ll discuss in depth open.\n\n## Processing Utilities\n\nTo get started we are going to need some toy data. I chose the c4 dataset provided by AllenAI because it had nice long textual examples that were relatively clean. \n\nIn order to keep things running on a local machine I took one million examples for training and 50,000 examples for validation. I personally streamed the data to my local disk for faster and easier of reinitialization. However, with the streaming dataset utility function we define you can also just stream the data directly from HuggingFace.\n\nWe also need to initialize our vocabulary, at the character level for our character level CNN and at the word level to feed into our language modeling head. \n\nFor our character level vocabulary we simply use the characters that you'd typically see in english text, in addition to an index for a padding character and an unknown character. \n\nOur word vocabulary is specified very similarly to how we have done it in the previous posts. We build a word vocabulary of up to 100,000 words with a minimum word frequency of 20 words. \n\n:::{.callout-note}\n\nWe use a simple tokenizer, splitting text on spaces, which is not canonically what ELMo utilizes in the original paper but works almost just as well.\n\n:::\n\nFinally, as I eluded to earlier we need a way to stream batches of our data to the model as we are training. The utility defined below handles shuffling, tokenizing, and encoding both characters and words before yielding them to the model. \n\n:::{.callout-important}\n\nI have elected to use non-overlapping sequences because it is a lot faster and I am running on a local machine. For best results it would be best to use overlapping sequences. You can adjust this in the `__iter__` method.\n\n:::\n\n::: {#fa04b6e3 .cell execution_count=2}\n``` {.python .cell-code code-fold=\"true\"}\nimport os\n\nfrom functools import partial\nfrom collections import Counter\nimport itertools\nimport jax\nimport jax.numpy as jnp\nfrom flax import nnx\nimport optax\nimport numpy as np\n\nfrom datasets import load_dataset, DownloadConfig, Dataset, load_from_disk\nimport string\nimport random\nimport orbax.checkpoint as ocp\n\nfrom datasets.utils.logging import disable_progress_bar\ndisable_progress_bar()\n```\n:::\n\n\n::: {#1236637d .cell execution_count=3}\n``` {.python .cell-code code-fold=\"true\"}\n# Stream data to local disk: one time only\n# You can also just stream from HF, however on reload (i.e. next epoch) you need to stream again (re-download).\n\ndef train_gen():\n    \"\"\"\n    Streams english c4 training data from HF Allen AI. We subset the data to 1M examples.\n    \"\"\"\n    stream = load_dataset(\n        \"allenai/c4\",\n        \"en\",\n        split=\"train\",\n        streaming=True,\n    )\n    for i, row in enumerate(stream):\n        if i >= 1_000_000:\n            break\n        yield row\n\ndef val_gen():\n    \"\"\"\n    Streams english c4 validatiom data from HF Allen AI. We subset the data to 50k examples.\n    \"\"\"\n    stream = load_dataset(\n        \"allenai/c4\",\n        \"en\",\n        split=\"train\",\n        streaming=True,\n    )\n    for i, row in enumerate(stream):\n        if i < 2_000_000:\n            continue\n        if i >= 2_050_000:\n            break\n        yield row\n\n\n# In batches of 1k save the data to disk\ntrain_ds = Dataset.from_generator(\n    train_gen,\n    writer_batch_size=1_000,\n)\n\ntrain_ds.save_to_disk(\"c4_train\")\n\nval_ds = Dataset.from_generator(\n    val_gen,\n    writer_batch_size=1_000,\n)\n\nval_ds.save_to_disk(\"c4_val\")\n```\n:::\n\n\n::: {#7ddab5af .cell execution_count=4}\n``````````` {.python .cell-code code-fold=\"true\"}\n# Tokens used when padding or encountering unknown characters in a string.\nPAD_CHAR, UNK_CHAR = \"<pad>\", \"<unk>\"\n\n# Tokens used when padding or encountering unknown words in a sentence.\nPAD_WORD, UNK_WORD = \"<pad>\", \"<unk>\"\n\n# Characters that we expect to see in typical English text\nchar_vocab = [PAD_CHAR, UNK_CHAR] + list(string.ascii_lowercase + string.ascii_uppercase + string.digits + string.punctuation)\nchar_to_id = {ch: i for i, ch in enumerate(char_vocab)}\n\ndef build_word_vocab(dataset, batch_size=32, min_freq=20, max_vocab=100_000) -> tuple[dict[str, int], dict[int, str]]:\n    \"\"\"\n    Build a word–to–index mapping from a text dataset.\n\n    The function scans the provided dataset for tokenised words\n    (splitting on whitespace) and retains only words that appear at least\n    :param min_freq: times in the corpus.  The returned dictionary is\n    capped to at most :param max_vocab: entries.\n\n    Parameters\n    ----------\n    dataset\n        Iterable of records where each record is a mapping\n        (``dict``) that contains a ``\"text\"`` key holding the raw string.\n    batch_size\n        Number of records processed in a single counting pass.\n    min_freq\n        Minimum frequency required for a word to be included in the\n        resulting vocabularies.\n    max_vocab\n        Maximum number of words (excluding ``<pad>`` and ``<unk>``) to keep in\n        the vocabularies.  The words chosen are the most frequent ones that\n        pass ``min_freq`` filtering.\n\n    Returns\n    -------\n    word_to_index : dict\n        Mapping from a word string to a unique integer ID, with\n        ``<pad>`` mapping to 0 and ``<unk>`` mapping to 1.\n    index_to_word : dict\n        Inverse mapping from integer ID back to the corresponding word.\n\n    Notes\n    -----\n    * The tokenisation strategy is simplistic: it uses the standard\n      ``str.split()`` which splits on any whitespace.  For more advanced\n      tokenisation pipelines (e.g., handling sub‑words, hyphenated\n      compounds, etc.) replace the ``s.split()`` call accordingly.\n    * The frequency counter is updated in batches to keep the memory\n      footprint low; ``Counter.update`` is called repeatedly on\n      concatenated lists of tokens.\n\n    \"\"\"\n    counter = Counter()\n    buf = []\n\n    for row in dataset:\n        buf.append(row[\"text\"])\n        if len(buf) >= batch_size:\n            for s in buf:\n                counter.update(s.split())\n            buf = []\n\n    if buf:\n        for s in buf:\n            counter.update(s.split())\n\n    # top words with min frequency\n    most_common = [(w, c) for w, c in counter.most_common(max_vocab) if c >= min_freq]\n    word_to_index = {PAD_WORD: 0, UNK_WORD: 1}\n    index_to_word = {0: PAD_WORD, 1: UNK_WORD}\n    for i, (w, _) in enumerate(most_common, start=2):\n        word_to_index[w] = i\n        index_to_word[i] = w\n    return word_to_index, index_to_word\n```````````\n:::\n\n\n::: {#800c9d21 .cell execution_count=5}\n``````````` {.python .cell-code code-fold=\"true\"}\nclass StreamingTextDataLoader:\n    \"\"\"\n    A streaming data loader for text corpora that yields batches of tokenized\n    sequences.\n\n    This class supports streaming from an arbitrary dataset, automatically shuffling,\n    tokenizing into fixed‑length windows (with stride = seq_len), and encoding both\n    words and characters. It provides batched dictionaries containing ``word_ids``,\n    ``char_ids`` and ``target_ids`` as JAX arrays.\n\n    Parameters\n    ----------\n    ds: Iterable[dict]\n        A dataset yielding rows that contain a text field (e.g., \"text\", \"sentence\",\n        \"review\", or \"content\").  Each row should be a mapping from column name to value.\n    vocab: dict[str, int] | Vocab object\n        Mapping from token strings to unique integer IDs.\n    char_to_id: dict[str, int]\n        Mapping from characters to their integer IDs.\n    seq_len: int\n        Length of the input sequence window (number of tokens). Each yielded batch will contain sequences of this size.\n    word_len: int\n        Maximum length of each token when encoded at the character level. Tokens longer than this are truncated; shorter ones are padded with ``PAD_CHAR``.\n    batch_size: int\n        Number of examples to accumulate before yielding a batched dictionary.\n    shuffle_buffer: int, optional (default=2048)\n        Size of the buffer used for shuffling tokens from the stream. Larger buffers\n        provide better randomness at the cost of memory usage.\n    seed: int or None, optional (default=0)\n        Random seed for reproducibility when shuffling.\n\n    Attributes\n    ----------\n    self.ds: original iterable dataset.\n    self.vocab: token vocabulary.\n    self.char_to_id: character‑to‑ID mapping.\n    self.seq_len: sequence length used for chunking.\n    self.word_len: maximum character width per token.\n    self.batch_size: number of samples per batch to yield.\n    self.shuffle_buffer: size of the shuffle buffer.\n    self.seed: RNG seed.\n    self.token_buffer: internal queue holding pre‑assembled tokens awaiting windowing.\n\n    Yields\n    ------\n    dict\n        A dictionary with three JAX arrays:\n        ``word_ids`` – shape ``(batch_size, seq_len)`` integer IDs for each token,\n        ``char_ids`` – shape ``(batch_size, seq_len, word_len)`` character‑level IDs,\n        ``target_ids`` – shifted version of ``word_ids`` used as language‑model targets.\n\n    Notes\n    -----\n    * The class performs a non‑overlapping stride split (`self.token_buffer =\n      self.token_buffer[self.seq_len:]`) which can be changed to an overlapping stride\n      if desired for better coverage.\n    * All returned arrays are JAX ``jnp`` objects.\n    \"\"\"\n    def __init__(self, ds, vocab, char_to_id, seq_len, word_len, batch_size, shuffle_buffer=2048, seed=0):\n        self.ds = ds\n        self.vocab = vocab\n        self.char_to_id = char_to_id\n        self.seq_len = seq_len\n        self.word_len = word_len\n        self.batch_size = batch_size\n        self.shuffle_buffer = shuffle_buffer\n        self.seed = seed\n\n        self.token_buffer = []\n\n    def _get_text_field(self, row):\n        for key in [\"text\", \"sentence\", \"review\", \"content\"]:\n            if key in row:\n                return row[key]\n        raise KeyError(f\"No text field found in row keys: {list(row.keys())}\")\n\n\n    def _encode_window(self, toks):\n        word_ids = [self.vocab.get(w, self.vocab.get(UNK_WORD)) for w in toks]\n\n        char_ids = np.full(\n            (self.seq_len, self.word_len),\n            self.char_to_id[PAD_CHAR],\n            dtype=np.int32,\n        )\n\n        for i, w in enumerate(toks):\n            cids = [self.char_to_id.get(c, self.char_to_id[UNK_CHAR]) for c in w[:self.word_len]]\n            char_ids[i, :len(cids)] = cids\n\n        return {\n            \"word_ids\": np.array(word_ids, dtype=np.int32),\n            \"char_ids\": char_ids,\n        }\n\n    def _shuffle_buffer_iter(self, ds):\n        buf = []\n        for row in ds:\n            buf.append(row)\n            if len(buf) >= self.shuffle_buffer:\n                random.shuffle(buf)\n                while buf:\n                    yield buf.pop()\n        random.shuffle(buf)\n        while buf:\n            yield buf.pop()\n\n    def __iter__(self):\n        self.token_buffer = []\n        batch_words, batch_chars, batch_targets = [], [], []\n\n        for row in self._shuffle_buffer_iter(self.ds):\n            text = self._get_text_field(row)\n\n            new_toks = text.split()\n            self.token_buffer.extend(new_toks)\n\n            while len(self.token_buffer) >= self.seq_len + 1:\n                x_toks = self.token_buffer[:self.seq_len]\n                y_toks = self.token_buffer[1:self.seq_len + 1]\n\n                # Non-overlapping stride (for speed) ideally you want overlapping\n                self.token_buffer = self.token_buffer[self.seq_len:]\n\n                x_enc = self._encode_window(x_toks)\n                y_ids = np.array(\n                    [self.vocab.get(w, self.vocab.get(UNK_WORD)) for w in y_toks],\n                    dtype=np.int32,\n                )\n\n                batch_words.append(x_enc[\"word_ids\"])\n                batch_chars.append(x_enc[\"char_ids\"])\n                batch_targets.append(y_ids)\n\n                if len(batch_words) == self.batch_size:\n                    yield {\n                        \"word_ids\": jnp.stack(batch_words),\n                        \"char_ids\": jnp.stack(batch_chars),\n                        \"target_ids\": jnp.stack(batch_targets),\n                    }\n                    batch_words, batch_chars, batch_targets = [], [], []\n\n```````````\n:::\n\n\n## Build Vocabulary\n\nIf you have also opted to save the toy dataset to disk, then the next step is to load it and build the word vocabulary. Note that we have already built the character vocabulary above.\n\n::: {#61cccd41 .cell execution_count=6}\n``` {.python .cell-code code-fold=\"true\"}\nhf_ds = load_from_disk(\"c4_train\")\nvocab, _ = build_word_vocab(hf_ds)\n```\n:::\n\n\n## Define Architecture Components\n\nWe went over the bulk of the architecture up above with the diagrams and the LSTM overview, however, I wanted to focus on a component within the character-level CNN called the `highway`. \n\nThe CNN applies a set of parallel one‑dimensional convolutions with varying kernel widths over the character sequences. Each filter’s output is max‑pooled across the temporal dimension, and the pooled features from all filters are concatenated to form a fixed‑size vector. This vector is then fed into a `highway` network, which learns a gated mixture of the raw CNN output and a transformed version of it, yielding the final character‑level embedding.\n\nMore specifically, we have our transformed piece of the input vector $x$ defined as\n\n$$\ntrans = ReLU(W_{t}x + b_{t})\n$$\n\nand learnable gates defined as\n\n$$\ngate = \\sigma(W_{g}x + b_{g})\n$$\n\nand we blend the two together with the following\n\n$$\nhighway(x) = gate(x) \\odot trans + (1 - gate(x)) \\odot x\n$$\n\nYou'll notice that it is possible for gates to be zero implying that the raw CNN outputs flow through unchanged. The `highway` layers add robustness to vanisihing/exploding gradients and to noisy CNN outputs. They also allow learning more diverse representations because the network can keep low-level, high-frequency patterns or mix them into higher-level, smoothed features. \n\n::: {#a9cc82b5 .cell execution_count=7}\n````````` {.python .cell-code}\nclass Highway(nnx.Module):\n    \"\"\"\n    A simple ``Highway`` network module\n\n    The layer implements the classic gating mechanism that controls how much of the\n    transformed input should be let through versus left for a direct residual shortcut.\n\n    Parameters\n    ----------\n    dim : int\n        Dimensionality of the input and output vectors.\n    rngs : nnx.Rngs\n        Random number generators.\n\n    Returns\n    -------\n    nnx.Module\n        A callable module whose ``__call__(self, x)`` method returns\n\n        .. code-block:: python\n\n            H * T + x * (1 - T)\n\n        where\n\n        * ``proj(x)`` --> ``H = relu(proj(x))`` is the transformed (candidate) signal,\n        * ``trans(x)`` --> ``T = sigmoid(trans(x))`` is a gate in ``[0, 1]``,\n        * ``x * (1 - T)`` passes through the original input weighted by the complement of\n          the gate.\n\n    \"\"\"\n    def __init__(self, dim, *, rngs: nnx.Rngs):\n        self.proj = nnx.Linear(dim, dim, rngs=rngs)\n        self.trans = nnx.Linear(dim, dim, rngs=rngs)\n\n    def __call__(self, x):\n        H = jax.nn.relu(self.proj(x))\n        T = jax.nn.sigmoid(self.trans(x))\n        return H * T + x * (1 - T)\n\n\nclass CharCNN(nnx.Module):\n    \"\"\"\n    Character‑level convolutional encoder that produces a dense\n    representation for each token in a sequence.\n\n    The module first embeds each input character, applies a set of 1‑D\n    convolutions across the character dimension, performs a global\n    max‑pool, concatenates the filter responses, and optionally\n    processes them through Highway layers and a final projection\n    layer.\n\n    Parameters\n    ----------\n    vocab_size : int\n        Size of the character vocabulary.  Must be at least the number\n        of distinct characters (plus any padding/unknown tokens) used\n        in the input data.\n\n    char_dim : int\n        Dimensionality of the learned character embeddings.\n\n    filters : Sequence[tuple[int, int]]\n        A list of `(width, out_channels)` pairs that specify the\n        kernel width and number of output channels for each 1‑D\n        convolution.\n\n    highway_layers : int\n        Number of Highway layers applied after the convolution\n        stack.\n\n    proj_dim : int | None, default ``None``\n        If given, a final linear projection is applied to the\n        concatenated convolution+highway features to reduce (or\n        expand) the dimensionality to ``proj_dim``.  If ``None``,\n        the output dimensionality equals the total number of\n        convolution channels.\n\n    rngs : nnx.Rngs\n        Random number generator.\n\n    Notes\n    -----\n    * **Input shape**: ``char_ids`` must have shape\n      ``[B, T, W]`` where ``B`` is the batch size, ``T`` the number of\n      tokens per sequence, and ``W`` the maximum number of characters\n      per token (words are padded to this length).\n    * Each convolution operates on the character dimension.\n      After convolution it is followed by a ReLU activation and a\n      channel‑wise global max‑pool over the remaining character\n      positions, resulting in a single scalar per filter channel.\n    * If ``proj_dim`` is ``None`` the output shape will be\n      ``[B, T, total_filters]`` where\n      ``total_filters`` is the sum of all ``out_channels`` across\n      the filters.  If a projection is used the output shape is\n      ``[B, T, proj_dim]``.\n\n    Returns\n    -------\n    jnp.ndarray\n        Character‑derived embedding of shape ``[B, T, proj_dim]`` if a\n        projection layer is supplied, otherwise ``[B, T, total_filters]``.\n    \"\"\"\n    def __init__(self, vocab_size, char_dim, filters, highway_layers, proj_dim=None, *, rngs: nnx.Rngs):\n        self.emb = nnx.Embed(vocab_size, char_dim, rngs=rngs)\n        self.convs = nnx.List([nnx.Conv(\n            in_features=char_dim,\n            out_features=out_channels,\n            kernel_size=(width,),\n            feature_group_count=1,\n            use_bias=True,\n            rngs=rngs\n        )\n        for (width, out_channels) in filters])\n        # highway layers\n        total_filters = sum(out for _, out in filters)\n        self.highways = nnx.List([Highway(total_filters, rngs=rngs) for _ in range(highway_layers)])\n        self.proj_dim = proj_dim\n        if proj_dim is not None:\n            self.proj = nnx.Linear(total_filters, proj_dim, rngs=rngs)\n\n    def __call__(self, char_ids):\n        B, T, W = char_ids.shape\n        # embed -> [B, T, W, D]\n        x = self.emb(char_ids)\n        # apply convs along the word-length axis: we first reshape to merge batch/time\n        x_flat = x.reshape((B*T, W, x.shape[-1]))  # [B*T, W, D]\n        conv_outs = []\n        for conv in self.convs:\n            y = conv(x_flat)  # [B*T, new_len, out_ch]\n            y = jax.nn.relu(y)\n            y = jnp.max(y, axis=1)  # max-pool over positions\n            conv_outs.append(y)\n        x_cat = jnp.concatenate(conv_outs, axis=-1)  # [B*T, total_filters]\n        # highways\n        for h in self.highways:\n            x_cat = h(x_cat)\n        if self.proj_dim is not None:\n            x_cat = self.proj(x_cat)  # [B*T, proj_dim]\n        return x_cat.reshape((B, T, -1))  # [B, T, embed_dim]\n`````````\n:::\n\n\n::: {#2df2d58e .cell execution_count=8}\n````````````` {.python .cell-code code-fold=\"true\"}\nclass LSTMCell(nnx.Module):\n    \"\"\"\n    A single LSTM cell implemented in ``flax.nnx`` that supports\n    dropout on both the input and the recurrent connection.\n\n    Parameters\n    ----------\n    input_dim : int\n        Dimensionality of the input vector ``x_t``\n    hidden_dim : int\n        Number of LSTM hidden units; defines the size of the hidden\n        state ``h`` and cell state ``c``.\n    dropout : float, default 0.0\n        Dropout probability applied to both the input vector and the\n        recurrent hidden state when ``deterministic=False``.\n    rngs : nnx.Rngs | None, default ``None``\n        Random number generators.\n\n    Notes\n    -----\n    * Internal weight maps:\n\n      ``Wx : input_dim: 4 x hidden_dim``  \n      ``Wh : hidden_dim: 4 x hidden_dim``\n\n      The four output channels correspond respectively to the\n      *input*, *forget*, *output*, and *candidate* gates.\n\n    * Dropout is applied according to the standard\n      “inverted” scheme (`mask / (1‑p)`), where the mask is drawn\n      from a Bernoulli distribution with probability `1‑dropout`.\n      Two independent masks are used: one for the current input\n      ``x_t`` and one for the recurrent hidden state ``h``.  A\n      distinct RNG key must be passed via ``jax_rng`` when\n      ``deterministic=False``.\n\n    * The cell state is a tuple ``(h, c)`` where each component has\n      shape ``[batch, hidden_dim]``.  The method returns a tuple\n      ``((h_new, c_new), h_new)``; the outer tuple contains the\n      updated cell state, and the inner ``h_new`` is the output\n      vector that can be consumed by ``nnx.scan`` or another\n      sequence wrapper.\n\n    Returns\n    -------\n    tuple\n        * ``((h_new, c_new), h_new)``  \n          where ``h_new`` and ``c_new`` are arrays of shape\n          ``[batch, hidden_dim]`` representing the updated\n          hidden and cell states, respectively.  The second\n          ``h_new`` in the outer tuple is the output of this\n          cell and matches the batch dimension of x_t``.\n    \"\"\"\n    def __init__(self, input_dim, hidden_dim, dropout=0.0, *, rngs=None):\n        self.Wx = nnx.Linear(input_dim, 4 * hidden_dim, rngs=rngs)\n        self.Wh = nnx.Linear(hidden_dim, 4 * hidden_dim, rngs=rngs)\n        self.hidden_dim = hidden_dim\n        self.dropout = dropout\n\n    def __call__(self, carry, x_t, deterministic=True, jax_rng=None):\n        h, c = carry\n\n        if not deterministic:\n            assert jax_rng is not None, \"RNG key must be passed for dropout\"\n            rng_inp, rng_rec = jax.random.split(jax_rng)\n            x_mask = jax.random.bernoulli(rng_inp, 1.0 - self.dropout, x_t.shape)\n            x_t = x_t * x_mask / (1.0 - self.dropout)\n\n            h_mask = jax.random.bernoulli(rng_rec, 1.0 - self.dropout, h.shape)\n            h = h * h_mask / (1.0 - self.dropout)\n\n        gates = self.Wx(x_t) + self.Wh(h)\n        i, f, o, g = jnp.split(gates, 4, axis=-1)\n        i = jax.nn.sigmoid(i)\n        f = jax.nn.sigmoid(f)\n        o = jax.nn.sigmoid(o)\n        g = jnp.tanh(g)\n        c_new = f * c + i * g\n        h_new = o * jnp.tanh(c_new)\n        return (h_new, c_new), h_new\n\n\nclass BiLSTMLayer(nnx.Module):\n    \"\"\"\n    A bidirectional LSTM layer built on top of :class:`LSTMCell`.\n\n    Each input sequence is processed in two directions:\n    * a forward LSTM that runs from the first to the last token,\n    * a backward LSTM that runs from the last to the first token.\n    The outputs of the two directions are returned separately and also\n    concatenated along the feature dimension.\n\n    Parameters\n    ----------\n    in_dim : int\n        Dimensionality of input tokens.\n    hidden_dim : int\n        Size of the hidden state in each direction (`h` and `c`).\n    dropout : float, default 0.0\n        Dropout probability applied inside each :class:`LSTMCell`.  The\n        same dropout probability is used for both forward and backward\n        streams.\n    rngs : nnx.Rngs | None, default ``None``\n        Random number generators\n\n    Notes\n    -----\n    * **RNG handling** -  \n      When ``deterministic=False`` a single ``jax_rng`` is split once\n      into two keys (for forward and backward).  These keys are then\n      split further into a per‑time‑step key that is passed to\n      :class:`LSTMCell`.  For deterministic execution ``rngs_fwd`` and\n      ``rngs_bwd`` are lists of ``None``.\n    * **State management** -  \n      Each direction starts from a zero initial hidden and cell state of\n      shape ``(B, hidden_dim)``.\n    * **Output dimensions** -  \n      For an input of shape ``(B, T, D)`` the three returned tensors have\n      shapes:\n        * ``hs_fwd``: ``(B, T, hidden_dim)``\n        * ``hs_bwd``: ``(B, T, hidden_dim)``\n        * ``hs_concat``: ``(B, T, 2 * hidden_dim)``\n\n    Returns\n    -------\n    tuple\n        ``(hs_fwd, hs_bwd, hs_concat)``\n        * ``hs_fwd`` - hidden states produced by the forward LSTM,\n        * ``hs_bwd`` - hidden states produced by the backward LSTM,\n        * ``hs_concat`` - concatenation of ``hs_fwd`` and ``hs_bwd``.\n\n    \"\"\"\n    def __init__(self, in_dim, hidden_dim, dropout=0.0, *, rngs=None):\n        self.fwd = LSTMCell(in_dim, hidden_dim, dropout=dropout, rngs=rngs)\n        self.bwd = LSTMCell(in_dim, hidden_dim, dropout=dropout, rngs=rngs)\n\n    def __call__(self, inputs, deterministic=True, jax_rng=None):\n        B, T, D = inputs.shape\n        h0 = jnp.zeros((B, self.fwd.hidden_dim))\n        c0 = jnp.zeros((B, self.fwd.hidden_dim))\n\n        if not deterministic and jax_rng is not None:\n            # Pre-split RNGs for each time step\n            rng_fwd, rng_bwd = jax.random.split(jax_rng)\n            rngs_fwd = jax.random.split(rng_fwd, T)\n            rngs_bwd = jax.random.split(rng_bwd, T)\n\n        else:\n            rngs_fwd = [None] * T\n            rngs_bwd = [None] * T\n\n        def fwd_scan(carry, x_and_rng):\n            x_t, rng_t = x_and_rng\n            return self.fwd(carry, x_t, deterministic=deterministic, jax_rng=rng_t)\n\n        def bwd_scan(carry, x_and_rng):\n            x_t, rng_t = x_and_rng\n            return self.bwd(carry, x_t, deterministic=deterministic, jax_rng=rng_t)\n\n        xs_fwd = (inputs.swapaxes(0, 1), rngs_fwd)\n        xs_bwd = (jnp.flip(inputs, axis=1).swapaxes(0, 1), rngs_bwd)\n\n        _, hs_fwd = jax.lax.scan(fwd_scan, (h0, c0), xs_fwd)\n        hs_fwd = hs_fwd.swapaxes(0, 1)\n\n        _, hs_bwd_rev = jax.lax.scan(bwd_scan, (h0, c0), xs_bwd)\n        hs_bwd = jnp.flip(hs_bwd_rev.swapaxes(0, 1), axis=1)\n\n        return hs_fwd, hs_bwd, jnp.concatenate([hs_fwd, hs_bwd], axis=-1)\n\n\nclass StackedBiLSTM(nnx.Module):\n    \"\"\"\n    A stack of bidirectional LSTM layers implemented with :class:`BiLSTMLayer`.\n\n    Each layer receives the concatenated hidden states of its predecessors\n    (``h_fwd`` || ``h_bwd``) as input.\n\n    Parameters\n    ----------\n    input_dim : int\n        Dimensionality of the raw token embeddings fed to the first\n        :class:`BiLSTMLayer`.  The dimensionality of all following layers\n        will be `2 * hidden_dim`.\n    hidden_dim : int\n        Hidden size of each unidirectional LSTM within every\n        :class:`BiLSTMLayer`.  The actual output of a layer\n        has shape ``(B, T, 2 * hidden_dim)``.\n    num_layers : int\n        Number of stacked bidirectional layers.\n    dropout : float, default 0.0\n        Dropout probability applied inside each :class:`LSTMCell`.\n    rngs : nnx.Rngs | None, default ``None``\n        Random number generators\n\n    Returns\n    ------- \n      1. ``outs`` - A list containing the *raw input* followed by the\n         concatenated output of each layer.  Hence ``len(outs) ==\n         num_layers + 1`` and the last element has shape\n         ``(B, T, 2 * hidden_dim)``.\n      2. ``fwd_states`` - A list of the forward hidden states from each\n         layer (shape ``(B, T, hidden_dim)``).\n      3. ``bwd_states`` - A list of the backward hidden states from each\n         layer (shape ``(B, T, hidden_dim)``).\n\n    \"\"\"\n    def __init__(self, input_dim, hidden_dim, num_layers, dropout=0.0, *, rngs=None):\n        self.layers = nnx.List([\n            BiLSTMLayer(input_dim if i == 0 else 2*hidden_dim, hidden_dim, dropout=dropout, rngs=rngs)\n            for i in range(num_layers)\n        ])\n\n    def __call__(self, x, deterministic=False, jax_rng=None):\n        outs = [x]\n        fwd_states, bwd_states = [], []\n\n        if not deterministic and jax_rng is not None:\n            rngs = jax.random.split(jax_rng, len(self.layers))\n        else:\n            rngs = [None] * len(self.layers)\n\n        for layer, r in zip(self.layers, rngs):\n            fwd, bwd, x = layer(x, deterministic=deterministic, jax_rng=r)\n            fwd_states.append(fwd)\n            bwd_states.append(bwd)\n            outs.append(x)\n        return outs, fwd_states, bwd_states\n\n\nclass LMHead(nnx.Module):\n    \"\"\"\n    Language‑model output head that projects hidden states to logits over a\n    target vocabulary.\n\n    The module consists of a single linear transformation that maps the\n    last hidden dimension of the model to a vector of size ``vocab_size``.\n\n    Parameters\n    ----------\n    hidden_dim : int\n        Dimensionality of the input hidden states ``h``.\n    vocab_size : int\n        Size of the target vocabulary (number of output logits).\n    rngs : nnx.Rngs\n        Random number generators\n\n    Returns\n    -------\n    jax.numpy.ndarray\n        Logits of shape ``(B, T, vocab_size)``\n\n    \"\"\"\n    def __init__(self, hidden_dim, vocab_size, *, rngs: nnx.Rngs):\n        self.linear = nnx.Linear(hidden_dim, vocab_size, rngs=rngs)\n    def __call__(self, h):\n        return self.linear(h)  # [B,T,V]\n`````````````\n:::\n\n\nWe define our ELMo model by initializing and chaining together the subcomponents:\n\n1. Character Level CNN\n2. Bidirectional LSTM\n3. Language Modeling Heads\n\nIn addition, we also initialize ELMo's scalar mix parameters that are used to adapt the embeddings during fine-tuning on downstream specific tasks, and we define model regularization in the form of dropout. Our ELMo model has three layers of dropout, at the input layer, the LSTM layer, and the output layer. \n\nIn our implementation you'll notice we project outputs into a common dimension to ensure the dimensional correctness for matrix multiplication, and you'll also notice that the method `forward_embeddings` is only used during fine-tuning to tune the scalar mix parameters on downstream specific tasks. \n\n\n:::{.callout-important}\n\nOutput dropout regularizes language-model training, not downstream embeddings. Scalar-mixed embeddings themselves are not dropout-regularized in this implementation.\n\n:::\n\n::: {#c838c331 .cell execution_count=9}\n``````````````` {.python .cell-code}\nclass ElmoModel(nnx.Module):\n    \"\"\"\n    The model implements the core components of the original ELMo\n    architecture: a character‑level CNN that produces sub‑word\n    representations, a stack of bidirectional LSTMs that encode the\n    sentence, and forward/backward language‑model heads. A\n    *scalar‑mix* (parameterised by a softmax over learnable weights)\n    combines the character‑CNN output and every LSTM layer into a\n    fixed‑dimensional semantic vector (`common_dim`).  This vector can\n    be used as contextualised word embeddings downstream.\n\n    Parameters\n    ----------\n    char_vocab_size : int\n        Vocabulary size for character indices.\n    char_dim : int\n        Size of the character embedding vectors.\n    filters : Sequence[Tuple[int, int]]\n        List of ``(num_filters, filter_width)`` tuples that define the\n        convolutional channels in the character CNN.\n    highway_layers : int\n        Number of highway network layers in the character CNN.\n    proj_dim : int\n        Dimensionality of the output of the projection layer that comes\n        after the character CNN.\n    common_dim : int\n        Dimensionality of the final ELMo embedding.\n    hidden_dim : int\n        Hidden state size of each BiLSTM cell.\n    num_layers : int\n        Number of stacked BiLSTM layers.\n    word_vocab_size : int\n        Size of the vocabulary for the forward and backward language‑model\n        heads.\n    input_dropout : float, default 0.1\n        Dropout probability applied to the output of the character CNN\n        during training.\n    lstm_dropout : float, default 0.1\n        Dropout probability applied within each BiLSTM layer during\n        training.\n    output_dropout : float, default 0.1\n        Dropout probability applied to the top‑layer LSTM states before\n        they are fed to the language‑model heads.\n    rngs : nnx.Rngs\n        JAX random number generator state used to initialise parameters.\n\n    Notes\n    -----\n    * The character‑CNN (`self.char_cnn`) maps the raw character IDs to\n      a vector of dimensionality ``proj_dim``.  This vector is then\n      projected to the common embedding space (`common_dim`) by a\n      linear layer.\n    * Each BiLSTM layer produces a forward state of shape\n      `(batch, seq_len, hidden_dim)` and a backward state of the\n      same shape.  The states of a layer are concatenated along the\n      feature dimension and projected to ``common_dim``.\n    * The scalar mix treats the character‑CNN output as layer 0 and\n      each BiLSTM layer as a subsequent layer.  The weight vector\n      (`self.scalar_weights`) is soft‑maxed so that the weights sum\n      to 1. The result is scaled by the learnable `gamma` parameter.\n    * During evaluation (``deterministic=True``) drop‑outs are disabled\n      and the same provided RNG is used to keep the computation\n      deterministic.\n\n    Methods\n    -------\n    forward_backbone(char_ids, jax_rng, deterministic=True)\n        Compute the character embeddings, forward and backward LSTM states.\n\n        Returns\n        -------\n        char_embs : jnp.ndarray\n            The raw output of the character CNN, shape\n            `(batch, seq_len, proj_dim)`.\n        fwd_states : List[jnp.ndarray]\n            Forward LSTM states for each of the ``num_layers`` layers,\n            each of shape `(batch, seq_len, hidden_dim)`.\n        bwd_states : List[jnp.ndarray]\n            Backward LSTM states for each layer, each of shape\n            `(batch, seq_len, hidden_dim)`.\n\n    forward_logits(char_ids, jax_rng, deterministic=True)\n        Return the forward and backward language‑model logits together\n        with the intermediate representations.\n\n        Returns\n        -------\n        fwd_logits : jnp.ndarray\n            Forward language‑model logits, shape\n            `(batch, seq_len, word_vocab_size)`.\n        bwd_logits : jnp.ndarray\n            Backward language‑model logits (time‑reversed), same shape as\n            ``fwd_logits``.\n        char_embs : jnp.ndarray\n            Raw character‑CNN embeddings (as in ``forward_backbone``).\n        fwd_states : List[jnp.ndarray]\n            Forward LSTM states.\n        bwd_states : List[jnp.ndarray]\n            Backward LSTM states.\n\n    forward_embeddings(char_embs, fwd_states, bwd_states)\n        Produce the final contextualised embedding vector.\n\n        Returns\n        -------\n        x : jnp.ndarray\n            Contextualised ELMo embedding of shape\n            `(batch, seq_len, common_dim)`.  It is a weighted sum of the\n            projected character‑CNN output and each concatenated\n            forward/backward LSTM layer, scaled by `gamma`.\n\n    \"\"\"\n    def __init__(self, char_vocab_size, char_dim, filters, highway_layers,\n                 proj_dim, common_dim, hidden_dim, num_layers, word_vocab_size,\n                 input_dropout=0.1, lstm_dropout=0.1, output_dropout=0.1, *,\n                 rngs: nnx.Rngs):\n        # Submodules\n        self.char_cnn = CharCNN(char_vocab_size, char_dim, filters, highway_layers, proj_dim=proj_dim, rngs=rngs)\n        self.bilstm = StackedBiLSTM(proj_dim, hidden_dim, num_layers, dropout=lstm_dropout, rngs=rngs)\n        self.fwd_head = LMHead(hidden_dim, word_vocab_size, rngs=rngs)\n        self.bwd_head = LMHead(hidden_dim, word_vocab_size, rngs=rngs)\n\n        # Scalar mix for ELMo embeddings\n        self.common_dim = common_dim\n        self.scalar_weights = nnx.Param(jnp.zeros(num_layers + 1))\n        self.gamma = nnx.Param(jnp.array(1.0))\n\n        # Projection layers to common_dim\n        self.layer_projections = nnx.List()\n\n        # CharCNN output to common dim\n        self.layer_projections.append(\n            nnx.Linear(proj_dim, common_dim, rngs=rngs)\n        )\n\n        # BiLSTM layers (2 * hidden_dim) to common_dim\n        for _ in range(num_layers):\n            self.layer_projections.append(\n                nnx.Linear(2 * hidden_dim, common_dim, rngs=rngs)\n            )\n\n        assert len(self.layer_projections) == len(self.scalar_weights.value)\n\n        # Dropout layers\n        self.input_dropout = nnx.Dropout(rate=input_dropout, rngs=rngs)\n        self.output_dropout = nnx.Dropout(rate=output_dropout, rngs=rngs)\n\n    def forward_backbone(self, char_ids, jax_rng, deterministic: bool = True):\n        char_embs = self.char_cnn(char_ids)\n\n        if not deterministic:\n            assert jax_rng is not None\n            rng_in, rng_lstm = jax.random.split(jax_rng)\n            x = self.input_dropout(char_embs, rngs=rng_in)\n        else:\n            x = char_embs\n            rng_lstm = jax_rng\n\n        _, fwd_states, bwd_states = self.bilstm(x, deterministic=deterministic, jax_rng=rng_lstm)\n\n        return char_embs, fwd_states, bwd_states\n\n    def forward_logits(self, char_ids, jax_rng, deterministic: bool = True):\n        char_embs, fwd_states, bwd_states = self.forward_backbone(\n            char_ids, deterministic=deterministic, jax_rng=jax_rng\n        )\n\n        top_fwd = fwd_states[-1]\n        top_bwd = bwd_states[-1]\n\n        top_fwd = self.output_dropout(top_fwd)\n        top_bwd = self.output_dropout(top_bwd)\n\n        fwd_logits = self.fwd_head(top_fwd)\n        bwd_logits = jnp.flip(\n            self.bwd_head(jnp.flip(top_bwd, axis=1)), axis=1\n        )\n\n        return fwd_logits, bwd_logits, char_embs, fwd_states, bwd_states\n\n    def forward_embeddings(self, char_embs, fwd_states, bwd_states):\n        layers = [char_embs] + [\n            jnp.concatenate([fwd, bwd], axis=-1)\n            for fwd, bwd in zip(fwd_states, bwd_states)\n        ]\n\n        w = jax.nn.softmax(self.scalar_weights.value)\n\n        projected = [\n            proj(layer) for proj, layer in zip(self.layer_projections, layers)\n        ]\n\n        x = sum(w_i * p for w_i, p in zip(w, projected))\n        x = self.gamma.value * x\n\n        return x\n\n```````````````\n:::\n\n\n## Loss Function\n\nWe have already discussed the learning objective, below is an implementation of a masked cross entropy loss function that masks padding from the loss computation. \n\n::: {#402b82a1 .cell execution_count=10}\n``````````` {.python .cell-code code-fold=\"true\"}\ndef masked_cross_entropy(logits, targets, pad_id=0):\n    \"\"\"\n    Computes the average cross‑entropy loss for a batch while ignoring\n    padding tokens.\n\n    The function applies a *mask* to the per‑token loss so that any token\n    whose target index equals ``pad_id`` is dropped from the loss\n    calculation.\n\n    Parameters\n    ----------\n    logits : jnp.ndarray\n        Logits produced by the model.  Expected shape\n        ``(batch, seq_len, vocab_size)``.\n    targets : jnp.ndarray\n        Ground‑truth token indices.  Expected shape ``(batch, seq_len)``\n        with integer values in ``[0, vocab_size)``.  Positions that\n        contain ``pad_id`` are treated as padding.\n    pad_id : int, default=0\n        The integer value used to mark padding positions in ``targets``.\n\n    Returns\n    -------\n    loss : float\n        The mean cross‑entropy loss over all non‑padding tokens in the\n        batch.  The denominator is the total number of non‑padding\n        tokens plus a small constant ``1e-12`` to avoid division by\n        zero.\n\n    \"\"\"\n    vocab_size = logits.shape[-1]\n    log_probs = jax.nn.log_softmax(logits, axis=-1)\n    targets_onehot = jax.nn.one_hot(targets, vocab_size)\n    per_token_loss = -jnp.sum(targets_onehot * log_probs, axis=-1)\n    mask = (targets != pad_id).astype(jnp.float32)\n    return jnp.sum(per_token_loss * mask) / (jnp.sum(mask) + 1e-12)\n```````````\n:::\n\n\n## Define Training Loop\n\nNext, we define the training loop, which encompasses the standard steps performed for each batch: executing the forward pass, computing gradients, and updating model parameters. This requires initializing the model and optimizer, configuring checkpointing to persist training state, and iterating over the training step for the desired number of epochs. These components follow conventional neural network training practice and do not introduce any ELMo-specific complexity.\n\nThere are, however, two practical considerations worth noting:\n\n1. We intentionally select hyperparameters corresponding to a reduced ELMo configuration, as the full-scale model is computationally infeasible on the available hardware.\n2. We limit pre-training to five epochs for the same reason.\n\nDespite these constraints, even this abbreviated pre-training regimen yields clearly observable improvements on downstream tasks, as demonstrated in subsequent sections.\n\n::: {#7dd54216 .cell execution_count=11}\n``````````` {.python .cell-code code-fold=\"true\"}\n@jax.jit\ndef train_step(optimizer, batch, jax_rng):\n    \"\"\"\n    Performs a single optimisation update on the ELMo‐style model.\n\n    The function runs a forward pass of the model to obtain forward and\n    backward language‑model logits, computes the masked cross‑entropy\n    loss for each direction, sums the two losses, back‑propagates the\n    gradients, and finally applies the optimiser's update rule.\n\n    Parameters\n    ----------\n    optimizer : nnx.optim.Optimizer\n        An *nnx.optim.Optimizer* that owns the model to be trained.\n        The optimiser must expose a ``model`` attribute and provide an\n        ``update`` method that accepts the gradient dictionary.\n    batch : Mapping[str, jnp.ndarray]\n        A batch of training data mapping the following keys to\n        integer arrays:\n        * ``\"char_ids\"``: shape ``(batch, seq_len, word_len)``\n          containing character indices for each token.\n        * ``\"target_ids\"``: shape ``(batch, seq_len)``\n          containing the target token indices for the language‑model head.\n    jax_rng : jax.random.PRNGKey\n        Random number generator used for dropout and other stochastic\n        components of the model.\n\n    Returns\n    -------\n    optimizer : nnx.optim.Optimizer\n        The optimiser after the update.  It holds the freshly\n        updated model parameters.\n    loss : float\n        The scalar loss value that was optimised.  It is the sum of the\n        forward and backward masked cross‑entropy losses.\n\n    Notes\n    -----\n    * The optimiser should store the model in the ``optimizer.model``\n      attribute.  After the update, modifications to ``optimizer.model``\n      reflect the new parameters.\n\n    \"\"\"\n    char_ids = batch[\"char_ids\"]\n    targets  = batch[\"target_ids\"]\n\n    def loss_fn(model):\n        fwd_logits, bwd_logits, _, _, _ = model.forward_logits(\n            char_ids, deterministic=False, jax_rng=jax_rng\n        )\n\n        fwd_loss = masked_cross_entropy(\n            fwd_logits[:, :-1, :], targets[:, 1:]\n        )\n\n        bwd_loss = masked_cross_entropy(\n            bwd_logits[:, 1:, :], targets[:, :-1]\n        )\n\n        return fwd_loss + bwd_loss\n\n    loss = loss_fn(optimizer.model)\n    grads = nnx.grad(loss_fn)(optimizer.model)\n    optimizer.update(grads)\n\n    return optimizer, loss\n```````````\n:::\n\n\n::: {#c1beb0f8 .cell execution_count=12}\n``` {.python .cell-code code-fold=\"true\"}\n# RNG setup\nrngs = nnx.Rngs(0)\n\n# Hyperparameters\nchar_vocab_size = len(char_to_id)\nchar_dim = 16\nfilters = [\n    (1, 64),\n    (2, 128),\n    (3, 256),\n    (4, 256),\n    (5, 256),\n    (6, 256),\n]\nhighway_layers = 2\nproj_dim = 512\nhidden_dim = 512\nnum_layers = 2\n\nword_vocab_size = len(vocab)\ncommon_dim = 512\n\n# Model instantiation\nmodel = ElmoModel(\n    char_vocab_size=char_vocab_size,\n    char_dim=char_dim,\n    filters=filters,\n    highway_layers=highway_layers,\n    proj_dim=proj_dim,\n    common_dim=common_dim,\n    hidden_dim=hidden_dim,\n    num_layers=num_layers,\n    word_vocab_size=word_vocab_size,\n    input_dropout=0.1, \n    lstm_dropout=0.3, \n    output_dropout=0.1,\n    rngs=rngs\n)\n\n# Optimizer instantiation\ntx = optax.adamw(1e-3, weight_decay=1e-2)\noptimizer = nnx.ModelAndOptimizer(model, tx)\n```\n:::\n\n\n::: {#970aef1d .cell execution_count=13}\n``````````` {.python .cell-code code-fold=\"true\"}\nclass ELMoTrainer:\n    \"\"\"\n    A training wrapper for an ELMo bidirectional language model.  \n    It handles epoch‑wise training, validation\n    loss computation, and an early‑stopping strategy.\n\n    Parameters\n    ----------\n    optimizer : nnx.optim.Optimizer\n        The optimiser that owns the model to be trained.  It must expose a\n        ``model`` attribute (the trainable model) and an ``update`` method.\n    patience : int, optional (default=3)\n        Number of consecutive validation epochs without improvement on the\n        loss after which training is stopped early and the best model\n        parameters are restored.\n    rng_seed : int, optional (default=0)\n        Seed for the JAX random number generator.\n\n    Attributes\n    ----------\n    optimizer : nnx.optim.Optimizer\n        The optimiser being used.\n    patience : int\n        See ``patience`` above.\n    best_loss : float\n        Best validation loss observed so far.  Initialized to ``∞``.\n    wait : int\n        Number of epochs since the last improvement.\n    best_params : nnx.State | None\n        The model parameters that produced the best validation loss.\n        Stored as a JAX ``Mutable`` state so that they can be copied back\n        into ``optimizer.model`` on early‑stopping.\n    jax_rng : jax.random.PRNGKey\n        Current random key.\n\n    Methods\n    -------\n    train_epoch(train_loader)\n        Runs one epoch of training on ``train_loader``.\n    validate(model, val_loader)\n        Computes the average loss over ``val_loader``.\n    validate_and_stop(val_loader)\n        Performs validation, logs results and checks the early‑stopping\n        criterion.  Returns ``True`` if training should stop.\n    \"\"\"\n    def __init__(self, optimizer, patience=3, rng_seed=0):\n        self.optimizer = optimizer\n        self.patience = patience\n        self.best_loss = float(\"inf\")\n        self.wait = 0\n        self.best_params = None\n        self.jax_rng = jax.random.PRNGKey(rng_seed)\n\n    def train_epoch(self, train_loader):\n        total_loss = 0.0\n        n = 0\n        for batch in train_loader:\n            self.jax_rng, subkey = jax.random.split(self.jax_rng)\n            self.optimizer, loss = train_step(self.optimizer, batch, jax_rng=subkey)\n            total_loss += float(loss)\n            n += 1\n        return total_loss / max(1, n)\n\n    def validate(self, model, val_loader):\n        total_loss = 0.0\n        n = 0\n\n        for batch in val_loader:\n            targets  = batch[\"target_ids\"]\n\n            fwd_logits, bwd_logits, _, _, _ = model.forward_logits(\n                batch[\"char_ids\"], jax_rng=self.jax_rng\n            )\n\n            # Forward predicts t+1\n            fwd_loss = masked_cross_entropy(fwd_logits[:, :-1, :], targets[:, 1:])\n\n            # Backward predicts t-1\n            bwd_loss = masked_cross_entropy(bwd_logits[:, 1:, :], targets[:, :-1])\n\n            loss = fwd_loss + bwd_loss\n\n            total_loss += float(loss)\n            n += 1\n\n        mean_loss = total_loss / max(1, n)\n        return mean_loss\n\n\n    def validate_and_stop(self, val_loader):\n        val_loss = self.validate(self.optimizer.model, val_loader)\n        print(f\"  val_loss={val_loss:.4f}\")\n\n        if val_loss < self.best_loss:\n            self.best_loss = val_loss\n            self.best_params = nnx.state(self.optimizer.model)\n            self.wait = 0\n            print(\"  New best model saved.\")\n            return False  # continue training\n\n        self.wait += 1\n        print(f\"  No improvement ({self.wait}/{self.patience})\")\n\n        if self.wait >= self.patience:\n            print(\"Early stopping triggered!\")\n            nnx.update(self.optimizer.model, self.best_params)  # restore best params\n            return True  # stop training\n\n        return False\n```````````\n:::\n\n\n::: {#a5878f77 .cell execution_count=14}\n``` {.python .cell-code code-fold=\"true\"}\n# Where to save model checkpoints\nckpt_dir = \"./checkpoints/elmo/state/\"\ncheckpointer = ocp.StandardCheckpointer()\n```\n:::\n\n\n:::{.callout-caution}\n\nWhen creating your checkpoints, ensure you are checkpointing the state from the model that has the updated weights. In our case the one initialized in our trainer class tied to our optimizer and not the initialized `model` from above as those weights will not be updated. \n\n:::\n\n::: {#ef8a92be .cell execution_count=15}\n``` {.python .cell-code code-fold=\"true\"}\ntrainer = ELMoTrainer(optimizer, patience=100) # No need to early stop with pre-training\n\ntrain_ds = load_from_disk(\"c4_train\")\nval_ds = load_from_disk(\"c4_val\")\n\nepochs = 5\n\nfor epoch in range(epochs):\n    print(f\"Epoch {epoch + 1}\")\n\n    # reset streaming ds\n    train_loader = StreamingTextDataLoader(train_ds, vocab, char_to_id,\n                                    seq_len=128, word_len=50,\n                                    batch_size=20, shuffle_buffer=2048)\n\n    train_loss = trainer.train_epoch(train_loader)\n\n    if (epoch + 1) % 2 == 0:\n        # reset streaming ds\n        val_loader = StreamingTextDataLoader(val_ds, vocab, char_to_id,\n                                        seq_len=128, word_len=50,\n                                        batch_size=20, shuffle_buffer=2048)\n\n        stop = trainer.validate_and_stop(val_loader)\n\n    # Save checkpoint each epoch\n    _, state = nnx.split(trainer.optimizer.model) # Make sure you use the model with the updated weights not the initialized model from above\n    checkpointer.save(\n        os.path.abspath(\n            os.path.join(ckpt_dir, f\"epoch{ epoch + 1 }\")\n        ), \n        state\n    )\n\n    if stop:\n        break\n```\n:::\n\n\n## Evaluate Learned Embeddings on Downstream Task\n\nOkay, now that we have a pre-trained ELMo model on hand we are going to fine-tune it for a text classification task. For this task, we are going to use the Stanford Sentiment Treebank v2 (SST‑2) dataset. \n\n### Compare Random Weights to Pretrained Model\n\nWe are going to compare how a random weights initialized ELMo model performs in comparison to our breifly pre-trained ELMo model. Below we initialize a random model, very much the same way we initialized a model for pre-training. We also load our pre-trained weights from our saved checkpoint.\n\n::: {#b60c5394 .cell execution_count=16}\n``` {.python .cell-code code-fold=\"true\"}\n# Initialize a random weights model\n\n# RNG setup\nrng = jax.random.PRNGKey(0)\nrngs = nnx.Rngs(rng)\n\n# Hyperparameters\nchar_vocab_size = len(char_to_id)\nchar_dim = 16\nfilters = [\n    (1, 64),\n    (2, 128),\n    (3, 256),\n    (4, 256),\n    (5, 256),\n    (6, 256),\n]\nhighway_layers = 2\nproj_dim = 512\nhidden_dim = 512\nnum_layers = 2\n\nword_vocab_size = len(vocab)\ncommon_dim = 512\n\n# Model instantiation\nmodel_random = ElmoModel(\n    char_vocab_size=char_vocab_size,\n    char_dim=char_dim,\n    filters=filters,\n    highway_layers=highway_layers,\n    proj_dim=proj_dim,\n    common_dim=common_dim,\n    hidden_dim=hidden_dim,\n    num_layers=num_layers,\n    word_vocab_size=word_vocab_size,\n    input_dropout=0.1, \n    lstm_dropout=0.3, \n    output_dropout=0.1,\n    rngs=rngs\n)\n```\n:::\n\n\n::: {#47889fcb .cell execution_count=17}\n``` {.python .cell-code code-fold=\"true\"}\n# Load checkpointed model\n\n# Construct an abstract version of the model (this is an empty scaffold so memory utilization is minimal)\nabstract_model = nnx.eval_shape(\n    lambda: ElmoModel(\n        char_vocab_size=char_vocab_size,\n        char_dim=char_dim,\n        filters=filters,\n        highway_layers=highway_layers,\n        proj_dim=proj_dim,\n        common_dim=common_dim,\n        hidden_dim=hidden_dim,\n        num_layers=num_layers,\n        word_vocab_size=word_vocab_size,\n        input_dropout=0.1,\n        lstm_dropout=0.3,\n        output_dropout=0.1,\n        rngs=nnx.Rngs(0)\n    )\n)\n\n# Split to get graphdef and an abstract state\ngraphdef, abstract_state = nnx.split(abstract_model)\n\n# Restore into that abstract state\nckpt_dir = \"./checkpoints/elmo/state/\"\nepoch = 5\ncheckpointer = ocp.StandardCheckpointer()\nrestored_state = checkpointer.restore(\n    os.path.abspath(os.path.join(ckpt_dir, f\"epoch{epoch}\"))\n)\n\n# Merge to produce a real model with pretrained weights\nmodel_trained = nnx.merge(graphdef, restored_state)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWARNING:absl:`StandardCheckpointHandler` expects a target tree to be provided for restore. Not doing so is generally UNSAFE unless you know the present topology to be the same one as the checkpoint was saved under.\n```\n:::\n:::\n\n\n:::{.callout-tip}\n\nIf you used a GPU for pre-training and you decide that you want to load your pre-trained weights onto a different device. You will need to map the state onto the new device. Below is a code cell that shows you how to do that.\n\n:::\n\n::: {#7aea38e8 .cell execution_count=18}\n``` {.python .cell-code code-fold=\"true\"}\n# To load the model that was trained using GPU onto a CPU only device use this:\n\n# Ensure abstract_state is placed on the current local devices\ncpu_device = jax.devices('cpu')[0]\nsharding = jax.sharding.SingleDeviceSharding(cpu_device)\n\n# Construct an abstract version of the model\nabstract_model = nnx.eval_shape(\n    lambda: ElmoModel(\n        char_vocab_size=char_vocab_size,\n        char_dim=char_dim,\n        filters=filters,\n        highway_layers=highway_layers,\n        proj_dim=proj_dim,\n        common_dim=common_dim,\n        hidden_dim=hidden_dim,\n        num_layers=num_layers,\n        word_vocab_size=word_vocab_size,\n        input_dropout=0.1,\n        lstm_dropout=0.3,\n        output_dropout=0.1,\n        rngs=nnx.Rngs(0)\n    )\n)\n\n# Split to get graphdef and an abstract state\ngraphdef, abstract_state = nnx.split(abstract_model)\n\n# Map the sharding onto your abstract state leaves\nabstract_state = jax.tree.map(\n    lambda x: jax.ShapeDtypeStruct(x.shape, x.dtype, sharding=sharding),\n    abstract_state\n)\n\n# Initialize and restore\nckpt_dir = \"./checkpoints/elmo/state/\"\nepoch = 5\ncheckpoint_path = os.path.abspath(os.path.join(ckpt_dir, f\"epoch{epoch}\"))\n\ncheckpointer = ocp.StandardCheckpointer()\n\n# Pass abstract_state\nrestored_state = checkpointer.restore(\n    checkpoint_path,\n    abstract_state \n)\n\n# Load into the model\nmodel_trained = nnx.merge(graphdef, restored_state)\n```\n:::\n\n\n### Classifier Architecture\n\nOkey-dokes, let's put together a classifier head to leverage our ELMo embeddings on our downstream text classification task. The sub-components for our network will be:\n\n1. The ELMo model as the backbone\n2. An attention pooling layer with explicit padding masking to collapse our seqeunces into a single representation\n3. A two-layer multilayer perceptron with dropout and ReLU activation yeilding our logits\n\nIt is worth noting that the attention pooling layer here is not the same as attention in transformer models. This layer simply learns a scalar scoring function over token embeddings.\n\n::: {#c65aade4 .cell execution_count=19}\n``````` {.python .cell-code}\nclass AttnPool(nnx.Module):\n    \"\"\"\n    Attention‑based pooling layer that collapses a sequence of vectors into a\n    single representation using a learnable attention weight.\n\n    Parameters\n    ----------\n    dim : int\n        Dimensionality of each input vector (``x.shape[-1]``).\n    rngs : jax.random.PRNGKey\n        Random number generator\n\n    Returns\n    -------\n    jnp.ndarray\n        A tensor of shape ``(batch_size, dim)`` containing the weighted\n        sum of the input sequence.\n\n    \"\"\"\n    def __init__(self, dim, *, rngs):\n        self.proj = nnx.Linear(dim, 1, rngs=rngs)\n\n    def __call__(self, x, mask=None):\n        scores = self.proj(x).squeeze(-1)\n        if mask is not None:\n            scores = scores + (mask - 1) * 1e9\n        weights = jax.nn.softmax(scores, axis=1)\n        return jnp.sum(x * weights[..., None], axis=1)\n\n\nclass ElmoClassifier(nnx.Module):\n    \"\"\"\n    Sequence classifier that builds on a pre‑trained ELMo backbone.\n\n    The network follows the classic ELMo‑to‑text‑classification pipeline:\n\n    1. **ELMo backbone** – A shared ELMo `ElmoModel` is used to generate\n       contextual embeddings for each token (`char_ids`).\n    2. **Mask‑aware attention pooling** – The token‑wise embeddings are\n       weighted with an attention mechanism that respects the padding mask.\n    3. **MLP classifier** – A two‑layer MLP with dropout and ReLU non‑linearity\n       produces the final logits for *n_classes*.\n\n    Parameters\n    ----------\n    elmo_model : ElmoModel\n        Pre‑trained ELMo backbone that exposes two forward\n        stages:\n          * ``forward_backbone(char_ids, deterministic, jax_rng)``\n            returns word‑level embeddings and forward/backward LSTM states.\n          * ``forward_embeddings(char_embs, fwd_states, bwd_states)``\n        The backbone must expose ``common_dim`` - the dimensionality of the ELMo\n        embeddings that the classifier consumes.\n    num_classes : int\n        Number of target classes for the downstream classification task.\n    dropout_rate : float, default 0.1\n        Dropout probability applied before and after the hidden MLP layer.\n    rngs : nnx.Rngs\n        Random number generators\n\n    Attributes\n    ----------\n    backbone : ElmoModel\n        Reference to the ELMo backbone used for feature extraction.\n    dropout : nnx.Dropout\n        Dropout layer applied to the pooled representation and to the hidden\n        MLP output.\n    attn_pool : AttnPool\n        Attention pooling head that weights tokens based on the ELMo output.\n    classifier_hidden : nnx.Linear\n        First linear layer of the classification MLP.\n    classifier : nnx.Linear\n        Final linear layer producing logits of shape ``(B, num_classes)``.\n\n    Forward Pass\n    ------------\n    The module expects a 3‑D integer array of character IDs\n    ``char_ids`` with shape ``(B, T, C)`` where:\n\n    * **B** - batch size\n    * **T** - sequence length (token count for each example)\n    * **C** - number of character embeddings per token\n\n    **Deterministic flag** – When ``deterministic=True`` the dropout\n    layers are disabled\n\n    **Masking** – A binary mask is inferred by checking for rows of all\n    zeros in ``char_ids`` (treated as padding).  The mask is added to the\n    raw attention scores before softmax, ensuring that padded positions\n    receive negligible weight.\n\n    Returns\n    -------\n    logits : jnp.ndarray\n        Unnormalised class scores with shape ``(B, num_classes)``.\n\n\n    The returned ``logits`` can be fed to a standard cross‑entropy loss\n    during training.\n\n    Notes\n    -----\n    * The attention pooling performs **softmax over the sequence dimension**\n      and uses a large negative constant to mask out padding before softmax\n      (effectively treating those positions as having negligible weight),\n      which is a stable and differentiable alternative to masking in the\n      exponent step.\n    * The model relies on the ELMo backbone providing *common_dim*‑dimensional\n      embeddings.  If the backbone uses a different dimensionality, the\n      attributes and the MLP width need adjustment accordingly.\n    \"\"\"\n    def __init__(self, elmo_model: ElmoModel, num_classes: int, dropout_rate: float = 0.1, *, rngs: nnx.Rngs):\n        self.backbone = elmo_model\n        self.dropout = nnx.Dropout(rate=dropout_rate, rngs=rngs)\n        self.attn_pool = AttnPool(self.backbone.common_dim, rngs=rngs)\n        self.classifier_hidden = nnx.Linear(self.backbone.common_dim, self.backbone.common_dim, rngs=rngs)\n        self.classifier = nnx.Linear(self.backbone.common_dim, num_classes, rngs=rngs)\n\n    def __call__(self, char_ids, deterministic: bool = False, jax_rng=None):\n        char_embs, fwd_states, bwd_states = self.backbone.forward_backbone(char_ids, deterministic=deterministic, jax_rng=jax_rng)\n        elmo_embs = self.backbone.forward_embeddings(char_embs, fwd_states, bwd_states)\n\n        mask = jnp.logical_not(jnp.all(char_ids == 0, axis=-1)).astype(jnp.float32)  # [B, T]\n        pooled = self.attn_pool(elmo_embs, mask)\n\n        x = self.dropout(pooled, deterministic=deterministic)\n        x = jax.nn.relu(self.classifier_hidden(x))\n        x = self.dropout(x, deterministic=deterministic)\n        logits = self.classifier(x)\n        return logits\n```````\n:::\n\n\n### Prepare Dataset\n\nWe need to build a new data loader. The current one was designed for language‑modeling pre‑training, but our goal is now text classification. Therefore, we must produce character and word indices that operate on whole sentences, not on a sliding window.\n\n::: {#6e27d35f .cell execution_count=20}\n``````````` {.python .cell-code code-fold=\"true\"}\nhf_ds = load_dataset(\"glue\", \"sst2\")\ntrain_stream = hf_ds[\"train\"]\nval_stream = hf_ds[\"validation\"]\n\n# DataLoader setup\nbatch_size = 256\nseq_len = 64\nword_len = 50\n\ndef encode_batch(texts, vocab, char_to_id, seq_len, word_len):\n    \"\"\"\n    Encode a batch of raw text strings into fixed‑size integer tensors.\n\n    Words and characters that are unseen in the supplied dictionaries are\n    replaced with the special unknown token.  Sequences longer than the\n    requested limits are truncated, while shorter ones are padded with the\n    special padding token.\n\n    Parameters\n    ----------\n    texts : Iterable[str]\n        A batch of raw text strings.  Each\n        element is split on whitespace to produce a list of words.\n    vocab : Mapping[str, int]\n        Word‑to‑ID dictionary.  Must contain the special tokens ``\"<pad>\"``\n        and ``\"<unk>\"``;\n    char_to_id : Mapping[str, int]\n        Character‑to‑ID dictionary.  It must contain ``\"<pad>\"`` and\n        ``\"<unk>\"`` for padding and unknown characters respectively.\n    seq_len : int\n        Maximum number of words per sentence that will be encoded.  All\n        sentences are truncated to this length or padded with the word\n        padding token.\n    word_len : int\n        Maximum number of characters per word that will be encoded.  Words\n        longer than this length are truncated; shorter words are padded\n        with the character padding token.\n\n    Returns\n    -------\n    dict\n        A dictionary with two keys:\n        ``\"word_ids\"`` : numpy.ndarray of shape ``(batch_size, seq_len)``\n        ``\"char_ids\"`` : numpy.ndarray of shape ``(batch_size, seq_len,\n        word_len)``\n\n        * ``word_ids[i, j]`` holds the ID of the *j*-th word in the\n          *i*-th input string;  ``vocab[PAD_WORD]`` if the position is\n          padded.\n        * ``char_ids[i, j, k]`` holds the ID of the *k*-th character of\n          the *j*-th word in the *i*-th input string;  ``char_to_id[PAD_CHAR]``\n          if the position is padded.\n\n    \"\"\"\n    PAD_WORD = \"<pad>\"\n    UNK_WORD = \"<unk>\"\n    PAD_CHAR = \"<pad>\"\n    UNK_CHAR = \"<unk>\"\n\n    batch_size = len(texts)\n    word_ids = np.full((batch_size, seq_len), vocab.get(PAD_WORD), dtype=np.int32)\n    char_ids = np.full((batch_size, seq_len, word_len), char_to_id[PAD_CHAR], dtype=np.int32)\n\n    for i, text in enumerate(texts):\n        toks = text.split()[:seq_len]\n        # Encode word IDs\n        wid = [vocab.get(w, vocab.get(UNK_WORD)) for w in toks]\n        word_ids[i, :len(wid)] = wid\n        # Encode char IDs\n        for j, w in enumerate(toks):\n            cids = [char_to_id.get(c, char_to_id[UNK_CHAR]) for c in w[:word_len]]\n            char_ids[i, j, :len(cids)] = cids\n\n    return {\"word_ids\": word_ids, \"char_ids\": char_ids}\n\n\ndef sst2_loader(train_ds, vocab, char_to_id, seq_len, word_len, batch_size):\n    \"\"\"\n    Yield batched, encoded SST‑2 dataset.\n\n    The function takes a HuggingFace ``datasets.Dataset`` containing the\n    Stanford Sentiment Treebank v2 (SST‑2) data, encodes the textual\n    component into word‑ and character‑ids, and yields a Python generator\n    that returns a dictionary of JAX arrays for each mini‑batch.\n\n    Parameters\n    ----------\n    train_ds : :class:`datasets.Dataset`\n        A HuggingFace ``Dataset`` object that must contain at least two\n        columns:\n        ``\"sentence\"`` – raw text data (a list of strings)\n        ``\"label\"``   – integer labels (0: negative, 1: positive)\n    vocab : Mapping[str, int]\n        Word‑to‑ID vocabulary.  Must contain the special tokens\n        ``\"<pad>\"`` and ``\"<unk>\"`` used by :func:`encode_batch`.\n    char_to_id : Mapping[str, int]\n        Character‑to‑ID mapping.  Must contain the special tokens\n        ``\"<pad>\"`` and ``\"<unk>\"``.\n    seq_len : int\n        Maximum number of words per sentence.  Sentences longer than this\n        limit will be truncated; shorter ones padded to ``seq_len``.\n    word_len : int\n        Maximum number of characters per word.  Characters longer than\n        this limit are truncated; shorter ones padded to ``word_len``.\n    batch_size : int\n        Number of examples per yielded batch.\n\n    Yields\n    ------\n    dict\n        A dictionary with the following JAX array entries (dtype\n        ``jnp.int32``):\n        ``\"char_ids\"`` : shape ``(batch_size, seq_len, word_len)``\n        ``\"word_ids\"`` : shape ``(batch_size, seq_len)``\n        ``\"labels\"``  : shape ``(batch_size,)``\n\n    \"\"\"\n    ds = train_ds.shuffle()\n    for i in range(0, len(ds), batch_size):\n        batch = ds[i:i + batch_size]\n\n        # Each field is a list\n        texts = batch[\"sentence\"]\n        labels = batch[\"label\"]\n\n        # Encode text to char IDs\n        enc = encode_batch(texts, vocab, char_to_id, seq_len, word_len) # These are already batch sized\n\n        yield {\n            \"char_ids\": jnp.array(enc[\"char_ids\"]),\n            \"word_ids\": jnp.array(enc[\"word_ids\"]),\n            \"labels\": jnp.array(labels, dtype=jnp.int32),\n        }\n\ntrain_loader = sst2_loader(\n    train_stream,\n    vocab=vocab,\n    char_to_id=char_to_id,\n    seq_len=seq_len,\n    word_len=word_len,\n    batch_size=batch_size,\n)\n```````````\n:::\n\n\n### Phased Fine-Tuning\n\nWe fine‑tune the model in two distinct stages in order to avoid large gradients destroying the pre‑trained representation.\n\nPhase 1 as a “High‑level” adaptation:\n\n* All parameters of the ELMo encoder, the bi‑LSTM and its learned representations, are frozen.\n\n* The only trainable parameters are the scalar‑mix weights, which blend the encoder layers, and the classifier head.\n\nPhase 2 as a “Deep” adaptation:\n\n* The bi‑LSTM parameters are now unfrozen so that the encoder can adjust its internal representations to the target task. \n\n* The scalar‑mix weights and the classifier head remain trainable. \n\n* A very small learning rate is employed to limit catastrophic forgetting.\n\nThe following code block explicitly lists the parameter groups updated in each phase.\n\n::: {#d455e5d1 .cell execution_count=21}\n``` {.python .cell-code}\n# Fine-tuning phase 1 updates the parameters of the classifier and the scalar mix parameters in the backbone\ntrainable_phase1 = nnx.All(\n    nnx.Param,\n    nnx.Any(\n        nnx.PathContains(\"layer_projections\"),\n        nnx.PathContains(\"scalar_weights\"),\n        nnx.PathContains(\"gamma\"),\n        nnx.PathContains(\"attn_pool\"),\n        nnx.PathContains(\"classifier\"),\n    )\n)\n\n# Fine-tuning phase 2 unfreezes the biltsm layers in the backbone\ntrainable_phase2 = nnx.All(\n    nnx.Param,\n    nnx.Any(\n        nnx.PathContains(\"bilstm\"),\n        nnx.PathContains(\"layer_projections\"),\n        nnx.PathContains(\"scalar_weights\"),\n        nnx.PathContains(\"gamma\"),\n        nnx.PathContains(\"attn_pool\"),\n        nnx.PathContains(\"classifier\"),\n    )\n)\n```\n:::\n\n\n### Define Training Loop\n\nNext, we set up the training step, initialize the classifier and optimizer, and run the training loop.\nTo keep the gradient updates confined to the intended parameters, we pass our `trainable_phase` object to create a `DiffState`.\n\nIn the first phase we instantiate two copies of the model:\n\n1. one with random‑initialized ELMo backbone weights, and\n2. one with pre‑trained ELMo weights.\n\nThis dual run lets us directly compare performance and confirm that our pre‑training regime is effective. We run phase-1 for 10 epochs. \n\nBelow you will notice the large difference in performance between the random weights backbone and the pre-trained backbone; Suggesting that our pre-training regime was indeed effective.\n\n::: {#167c25d9 .cell execution_count=22}\n``````````` {.python .cell-code code-fold=\"true\"}\n@nnx.jit(static_argnames=(\"trainable_phase\",))\ndef train_step(model_opt, batch, rng, *, trainable_phase):\n    \"\"\"\n    Perform one training step for a JAX/NNX model using a custom optimizer.\n\n    Parameters\n    ----------\n    model_opt: OptimizerWrapper\n        A lightweight optimizer object that holds the current model\n        parameters \n\n    batch: dict[str, jnp.ndarray]\n        A batch dictionary produced by :func:`sst2_loader`.\n\n    rng: jax.random.PRNGKey\n        Randon number generator\n\n    trainable_phase\n        tells :class:`nnx.DiffState` which part of the model should\n        get gradients.\n\n    Returns\n    -------\n    tuple\n        ``(model_opt, loss, acc, grads)``\n\n        * ``model_opt`` - the :class:`OptimizerWrapper` after applying\n          the gradient update.\n        * ``loss`` (float)  - the mean soft‑max cross‑entropy\n          computed on the current batch.\n        * ``acc`` (float)   - accuracy of the model on this batch.\n        * ``grads``         - a PyTree of gradients with the same\n          structure as ``model_opt.model``.\n\n    \"\"\"\n\n    # DiffState must match optimizer wrt argument\n    diff_state = nnx.DiffState(0, trainable_phase)\n\n    def loss_fn(model):\n        logits = model(batch[\"char_ids\"], deterministic=False, jax_rng=rng)\n        loss = optax.softmax_cross_entropy_with_integer_labels(\n            logits, batch[\"labels\"]\n        ).mean()\n        return loss, logits\n\n    (loss, logits), grads = nnx.value_and_grad(\n        loss_fn,\n        has_aux=True,\n        argnums=diff_state,\n    )(model_opt.model)\n\n    model_opt.update(grads)\n\n    preds = jnp.argmax(logits, axis=-1)\n    acc = jnp.mean(preds == batch[\"labels\"])\n\n    return model_opt, loss, acc, grads\n\n\nclassifier_random = ElmoClassifier(\n    model_random,\n    num_classes=2,\n    dropout_rate=0.3,\n    rngs=nnx.Rngs(jax.random.PRNGKey(1))\n)\n\nclassifier_trained = ElmoClassifier(\n    model_trained,\n    num_classes=2,\n    dropout_rate=0.3,\n    rngs=nnx.Rngs(jax.random.PRNGKey(1))\n)\n```````````\n:::\n\n\n::: {#a08a1c2f .cell execution_count=23}\n``` {.python .cell-code code-fold=\"true\"}\ntx = optax.adamw(1e-3, weight_decay=1e-2)\n\nmodel_opt_random = nnx.ModelAndOptimizer(\n    classifier_random,\n    tx,\n    wrt=trainable_phase1\n)\n\nmodel_opt_trained = nnx.ModelAndOptimizer(\n    classifier_trained,\n    tx,\n    wrt=trainable_phase1\n)\n```\n:::\n\n\n::: {#ddf17cea .cell execution_count=24}\n``` {.python .cell-code}\nnum_epochs = 10\ntrain_rng = jax.random.PRNGKey(0)\n\nfor epoch in range(num_epochs):\n    print(f\"\\n===== Epoch {epoch + 1}/{num_epochs} =====\")\n\n    # reinitialize or reshuffle dataset each epoch\n    train_loader = sst2_loader(\n        train_stream, vocab, char_to_id, seq_len, word_len, batch_size\n    )\n\n    epoch_loss_random = []\n    epoch_loss_trained = []\n    epoch_acc_random = []\n    epoch_acc_trained = []\n\n    for batch in train_loader:\n        \n        train_rng, subkey = jax.random.split(train_rng)\n        model_opt_random, loss_random, acc_random, grads_random = train_step(model_opt_random, batch, rng=subkey, trainable_phase=trainable_phase1)\n        model_opt_trained, loss_trained, acc_trained, grads_trained = train_step(model_opt_trained, batch, rng=subkey, trainable_phase=trainable_phase1)\n        \n        epoch_loss_random.append(float(loss_random))\n        epoch_acc_random.append(float(acc_random))\n        epoch_loss_trained.append(float(loss_trained))\n        epoch_acc_trained.append(float(acc_trained))\n        \n\n    print(f\"Epoch {epoch + 1} | loss={np.mean(epoch_loss_random):.4f} | acc={np.mean(epoch_acc_random):.4f}\")\n    print(f\"Epoch {epoch + 1} | loss={np.mean(epoch_loss_trained):.4f} | acc={np.mean(epoch_acc_trained):.4f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n===== Epoch 1/10 =====\nEpoch 1 | loss=0.6870 | acc=0.5525\nEpoch 1 | loss=0.6398 | acc=0.6273\n\n===== Epoch 2/10 =====\nEpoch 2 | loss=0.6799 | acc=0.5675\nEpoch 2 | loss=0.5723 | acc=0.6964\n\n===== Epoch 3/10 =====\nEpoch 3 | loss=0.6665 | acc=0.5960\nEpoch 3 | loss=0.5337 | acc=0.7277\n\n===== Epoch 4/10 =====\nEpoch 4 | loss=0.6487 | acc=0.6239\nEpoch 4 | loss=0.5135 | acc=0.7440\n\n===== Epoch 5/10 =====\nEpoch 5 | loss=0.6328 | acc=0.6446\nEpoch 5 | loss=0.4989 | acc=0.7517\n\n===== Epoch 6/10 =====\nEpoch 6 | loss=0.6181 | acc=0.6612\nEpoch 6 | loss=0.4839 | acc=0.7643\n\n===== Epoch 7/10 =====\nEpoch 7 | loss=0.6073 | acc=0.6708\nEpoch 7 | loss=0.4750 | acc=0.7704\n\n===== Epoch 8/10 =====\nEpoch 8 | loss=0.5989 | acc=0.6778\nEpoch 8 | loss=0.4663 | acc=0.7777\n\n===== Epoch 9/10 =====\nEpoch 9 | loss=0.5911 | acc=0.6844\nEpoch 9 | loss=0.4594 | acc=0.7807\n\n===== Epoch 10/10 =====\nEpoch 10 | loss=0.5864 | acc=0.6887\nEpoch 10 | loss=0.4510 | acc=0.7863\n```\n:::\n:::\n\n\nNext, we extract the fine-tuned model from the optimizer and instantiate a new `model_opt` via `nnx.ModelAndOptimizer()`. Note that for phase two we reduce the learning rate to $5 \\times 10^{-5}$. This lower rate helps preserve the pretrained bidirectional LSTM weights while allowing for small, corrective updates. We then train for five additional epochs in this phase.\n\n::: {#b7d78f35 .cell execution_count=25}\n``` {.python .cell-code code-fold=\"true\"}\ntx = optax.adamw(5e-5, weight_decay=1e-2)\n\nmodel_opt = nnx.ModelAndOptimizer(\n    model_opt_trained.model,\n    tx,\n    wrt=trainable_phase2\n)\n```\n:::\n\n\n::: {#0f1561a9 .cell execution_count=26}\n``` {.python .cell-code code-fold=\"true\"}\nnum_epochs = 5\ntrain_rng = jax.random.PRNGKey(0)\n\nfor epoch in range(num_epochs):\n    print(f\"\\n===== Epoch {epoch + 1}/{num_epochs} =====\")\n\n    # reinitialize dataset each epoch\n    train_loader = sst2_loader(\n        train_stream, vocab, char_to_id, seq_len, word_len, batch_size\n    )\n\n    epoch_loss = []\n    epoch_acc = []\n\n    for batch in train_loader:\n        train_rng, subkey = jax.random.split(train_rng)\n        model_opt, loss, acc, grads = train_step(model_opt, batch, rng=subkey, trainable_phase=trainable_phase2)\n        epoch_loss.append(float(loss))\n        epoch_acc.append(float(acc))\n\n    print(f\"Epoch {epoch + 1} | loss={np.mean(epoch_loss):.4f} | acc={np.mean(epoch_acc):.4f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n===== Epoch 1/5 =====\nEpoch 1 | loss=0.4292 | acc=0.8011\n\n===== Epoch 2/5 =====\nEpoch 2 | loss=0.4156 | acc=0.8091\n\n===== Epoch 3/5 =====\nEpoch 3 | loss=0.4039 | acc=0.8141\n\n===== Epoch 4/5 =====\nEpoch 4 | loss=0.3953 | acc=0.8215\n\n===== Epoch 5/5 =====\nEpoch 5 | loss=0.3860 | acc=0.8254\n```\n:::\n:::\n\n\nFinally, lets check the performance on the validation data.\n\n::: {#f6010289 .cell execution_count=27}\n``` {.python .cell-code}\nval_loader = sst2_loader(\n    val_stream,\n    vocab=vocab,\n    char_to_id=char_to_id,\n    seq_len=seq_len,\n    word_len=word_len,\n    batch_size=batch_size,\n)\n\nacc = []\nfor batch in val_loader:\n    raw_preds = model_opt.model(batch[\"char_ids\"], deterministic=True)\n    preds = np.argmax(raw_preds, axis=1)\n    batch_acc = np.sum(batch[\"labels\"] == preds) / preds.shape[0]\n    acc.append(batch_acc)\n\nprint(f\"Accuracy on validation: {np.mean(acc): .3f}\")\n\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAccuracy on validation:  0.742\n```\n:::\n:::\n\n\n# Conclusion\n\nThis was a long one. Good job making it all the way to the end!\n\n In this post we reviewed the differences beween static and contextual embeddings, we delved into the math and architecture of recurrent neural networks, and we implemented ELMo practically from scratch in JAX. In addition to all of that, we also then fine-tined ELMo to perform text classification and we showed the difference in having pre-trained embeddings versus random weights. \n\n# Coming Next\n\nIn the next post of this series, we will go over the transformer architecture, take a look at how attention works and implement a transformer model from scratch. \n\nI hope to see you there!\n\n# References\n\n::: {#refs}\n:::\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}