{
  "hash": "c04a86913323c62519eb17be52536589",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"How Machines Comprehend Language<br><sub>Recurrent Neural Networks - Embeddings from Language Models (ELMo)</sub>\"\nauthor: \"Jonathan Dekermanjian\"\ndate: \"2026-01-25\"\ncategories: [Natural Language Processing]\ncode-annotations: below\ncss: styles.css\njupyter: jax_env\ndraft: True\nexecute: \n  cache: true\n  eval: false\n---\n\n# Overview\n\nIn this post we go over contextual embeddings and recurrent neural networks. Briefly describing their inner workings and walking through an implementation of Embeddings from Language Models (ELMo), basically from scratch using the Jax ecosystem.\n\n# Introduction\n\nIn previous posts we learned what embeddings are, why they are important, and we got our hands dirty by implementing two of the earliest foundational methods. If you need a refresher or just hadn't seen them you can check out my posts on [GloVe](/posts/glove_embeddings/index.qmd) and [Skip-gram with negative sampling](/posts/skipgram_embeddings/index.qmd). In this post we will briefly review the differences between static and contextual embeddings, we will discuss some of the methodologies used to create contextual embeddings leading us to learn about recurrent neural networks, and finally we will look at the architecture and implement from scratch Embeddings from Language Models (ELMo).\n\n# From Static to Contextual Embeddings\n\nAs a reminder, an advantage of contextual embeddings is the ability to discriminate between polysemous words. For example, the word \"ship\" can refer to the action verb of shipping an item or to the noun of a water vessel. A static embedding would represent both meanings of the word \"ship\" with the same vector. In contrast, contextual embeddings are able to disambiguate the two meanings of the same word. \n\nOur vectors need to be aware of context which means that now the sequence in which words occur must influence the resultant vectors. For example, if we look at the sentences \"I need to ship it.\" and \"I own a ship.\" to disambiguate the word \"ship\" you need to know about the words before and after \"ship\". This is different than when we were learning embeddings using Skip-gram with Negative Sampling (SGNS) where we used local context in training to predict a target word's context words then during inference we used a collapsed averaged representation of target words across our entire corpus. \n\nSimilarly, when we implemented GloVe we again used local contexts to create a word-word co-occurrence matrix and then learn our embeddings by factorizing that matrix resulting in again an averaged vector embedding during inference for a given word across our corpus.\n\nOne way we include information from sequences is by using recurrent computation. This brings us to the topic of recurrent neural networks.\n\n\n# Recurrent Neural Networks\n\nA recurrent neural network (RNN) is a neural architecture designed for sequential data, in which information from previous time steps is carried forward through a recurrent hidden state. At each time step, the hidden state is updated based on the current input and the previous hidden state, and the output is computed from this hidden representation.\n\nWe have two equations that define a simple RNN, also known as an Elman RNN.\n\n$$\nh_{t} = \\phi(W_{h}h_{t-1} + W_{x}x_{t} + b)\n$$\n\n$$\ny_{t} = \\psi(W_{y}h_{t} + b_{y})\n$$\n\nWhere $x_{t}$, $h_{t}$, and $y_{t}$ are the input, hidden state, and output at time t, respectively. The weight matrices $W$ are linear operators that projects variables from one space into another. For example, $W_{x}$ projects $x_{t}$ into the hidden state space. Notice that these weight matrices are not indexed by t, signifying that they are shared across time and allows RNNs to generalize to varying sequence lengths.  \n\n::: {style=\"background-color:#dcdce0; padding:1em; border-radius:6px;\"}\n```{mermaid}\n%%| label: fig-elman-rnn\n%%| fig-cap: \"Elman (Simple) Recurrent Neural Network\"\n%%| eval: true\n\nflowchart BT\n  X[$$x_t$$] -->|input| H[$$h_t$$]\n  H -->|output| Y[$$y_t$$]\n  H -->|recurrent state| H\n\n  style H fill:#eef,stroke:#333,stroke-width:1.5px\n\n```\n\n:::\n\n\nThe Elman RNN maintains only one hidden state where at each time step the same computation must compress all past information in the single $h_{t}$. This is challenging to do for longer time dependencies. Additionally, Elman RNNs are prone to vanishing/exploding gradients due to the gradients needing to repeatedly pass through $W_{h}$ and the nonlinearity $\\phi$, via backpropagation through time. In practice, these limitations are enough to opt for a modified RNN architecture. \n\n\n## Long Short-Term Memory\n\nA Long Short-Term Memory (LSTM) RNN is an architectural extension of the standard Elman RNN designed to mitigate the vanishing gradient problem when modeling long-range dependencies. The LSTM introduces gating mechanisms (input, forget, and output gates) that explicitly regulate how much past information is retained and how much new information is incorporated, rather than relying on implicit storage in the recurrent weights as in an Elman RNN. Crucially, the LSTM maintains a cell state whose update follows an additive structure, forming a linear path through time. This additive dynamics enables more stable gradient propagation and substantially reduces vanishing gradients during training.\n\nLet's dive a little bit deeper here. Given an input at time $t$ $x_{t}$ and a previous hidden state $h_{t-1}$ and a previous cell state $c_{t-1}$, we have the following equations that control the evolution of the hidden and cell states over time.\n\nFirst, the forget gate $f_{t}$ is a parametric control for the signal that should be propagated forward from the prior cell state $c_{t-1}$ to the current cell state $c_{t}$ that we are computing.\n\n$$\nf_{t} = \\sigma(W_{f} \\cdot [h_{t-1}, x_{t}] + b_{f})\n$$\n\nThe input gate $i_{t}$, you'll notice, is almost the same computation as the forget gate and its role is to regulate how much signal should pass through from the candidate cell state $\\tilde{c}_{t}$ to the current cell state $c_{t}$.\n\n$$\ni_{t} = \\sigma(W_{i} \\cdot [h_{t-1}, x_{t}] + b_{i})\n$$\n\nOur last gate, the output gate $o_{t}$ is again the same computation as the other gates and its role is to regulate how much of the current cell state $c_{t}$ is exposed to the current hidden state $h_{t}$.\n\n$$\no_{t} = \\sigma(W_{o} \\cdot [h_{t-1}, x_{t}] + b_{o})\n$$\n\nYou may be thinking if all the computations are the same how do the different gates perform different functions? Well, that is a good question and one that is not obvious. However, their distinct roles emerge from how each gate modulates different computational paths in the forward pass, and how gradients propagate through those paths during backpropagation. \n\nIn the remaining equations you can see how these gates act as controls on the input, cell state, and hidden state. \n\n$$\n\\tilde{c}_{t} = \\tanh(W_{c} \\cdot [h_{t-1}, x_{t}] + b_{c})\n$$\n\n$$\nc_{t} = f_{t} \\odot c_{t-1} + i_{t} \\odot \\tilde{c}_{t}\n$$\n\n$$\nh_{t} = o_{t} \\odot \\tanh(c_{t})\n$$\n\nBelow is a graph that depicts the flow of computations that take place to update the hidden state.\n\n::: {style=\"background-color:#dcdce0; padding:1em; border-radius:6px;\"}\n```{mermaid}\n%%| label: fig-lstm-rnn\n%%| fig-cap: \"Long Short-Term Memory Recurrent Neural Network\"\n%%| eval: true\nflowchart BT\n    x_t[\"$$x_{t}$$ (input)\"] --> concat\n    h_prev[\"$$h_{t-1}$$ (prev hidden)\"] --> concat\n\n    concat[\"Concatenate\"] --> f_gate[\"Forget Gate<br> $$\\sigma(W_f \\cdot [h_{t-1}, x_{t}] + b_f)$$\"]\n    concat --> i_gate[\"Input Gate<br/> $$\\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i)$$\"]\n    concat --> c_tilde[\"Candidate State<br/> $$\\tanh(W_c \\cdot [h_{t-1}, x_t] + b_c)$$\"]\n    concat --> o_gate[\"Output Gate<br/> $$σ(W_o \\cdot [h_{t-1}, x_t] + b_o)$$\"]\n\n    c_prev[\"$$c_{t-1}$$ (prev cell)\"] --> mult_f[\"×\"]\n    f_gate --> mult_f\n\n    i_gate --> mult_i[\"×\"]\n    c_tilde --> mult_i\n\n    mult_f --> c_t[\"cₜ (cell state)\"]\n    mult_i --> c_t\n\n    c_t --> tanh_c[\"$$\\tanh$$\"]\n    tanh_c --> mult_o[\"×\"]\n    o_gate --> mult_o\n\n    mult_o --> h_t[\"$$h_t$$ (hidden state)\"]\n\n```\n\n:::\n\n\n# Embeddings from Language Models\n\nNow that we briefly reviewed RNNs, specifically LSTMs, we are ready to talk about a deep learning architecture used to generate contextual embeddings. The influential Embeddings from Language Models (ELMo) architecture. Before transformers became popular ELMo was one of the dominant models for generating contextual embeddings. ELMo blends character‑level convolutional neural networks (CNN) token encodings for handling out-of-vocabulary (OOV) words, multi‑layer bidirectional LSTMs to capture both left-to-right and right-to-left long range dependencies, and a trainable weighted sum of their hidden states to produce word‑level embeddings that are both context‑sensitive and adaptable through fine‑tuning.\n\n\n## Architecture\n\nLet's look at a high-level map of ELMo's architecture. The important sub-components are:\n\n1. <b>Character Level CNN</b>: Converts each token into character-level feature vectors and is well suited to handling OOV words.\n2. <b>Multi-layer bi-directional LSTM</b>: processes the character vectors in both directions, producing hidden states that capture left and right‑context.\n3. <b>Contextual Hidden State</b>: combines forward and backward outputs to form a deep, context‑sensitive representation of each token.\n4. <b>Scalar Mix parameters</b>: Learn a task-specific weighted combination of the representations from different layers, enabling effective transfer of the pretrained language model to downstream tasks without retraining the full model.\n\nBelow is a high-level graph of an ELMo model.\n\n::: {style=\"background-color:#dcdce0; padding:1.5em; border-radius:6px;\"}\n```{mermaid}\n%%| label: fig-elmo-arch\n%%| fig-cap: \"ELMo Architecture\"\n%%| eval: true\nflowchart BT\n\n%% 1 Input\nTokens[\"$$\\text{Token IDs } T_{1} \\text{ to } T_{n}$$\"] --> CharEmb[Character Embedding]\n\n%% 2 Character CNN (h0)\nCharEmb --> CharCNN[Character CNN Output]\nCharCNN --> H0[\"$$h_{0} \\text{ Character-based token representation}$$\"]\n\n%% 3 BiLSTM layers\nsubgraph BiLSTM[BiLSTM]\n    direction RL\n\n    Title[Bidirectional LSTM Stack]\n    style Title fill:none,stroke:none\n\n    subgraph Layer2[Layer 2]\n        F2[Forward LSTM 2] --> HF2[h_fwd_2]\n        B2[Backward LSTM 2] --> HB2[h_bwd_2]\n    end\n\n    subgraph Layer1[Layer 1]\n        F1[Forward LSTM 1] --> HF1[h_fwd_1]\n        B1[Backward LSTM 1] --> HB1[h_bwd_1]\n    end\nend\n\nH0 --> BiLSTM\n\n%% 4 Contextual states\nsubgraph HiddenStates[Contextual Representations]\n    H1[\"$$h_{1} = \\text{concat fwd1 bwd1}$$\"]\n    H2[\"$$h_{2} = \\text{concat fwd2 bwd2}$$\"]\nend\n\nHF1 --> H1\nHB1 --> H1\nHF2 --> H2\nHB2 --> H2\n\n%% 5 Forward & Backward LM heads\nsubgraph LMHeads[LM Heads]\n    direction LR\n\n    LMTitle[Language Modeling Heads]\n    style LMTitle fill:none,stroke:none\n\n    FLM[Forward LM Head] --> FSoftmax[\"Softmax(vocab)\"]\n    BLM[Backward LM Head] --> BSoftmax[\"Softmax(vocab)\"]\nend\n\nHF1 --> FLM\nHF2 --> FLM\nHB1 --> BLM\nHB2 --> BLM\n\n%% 6 ELMo scalar mixer\nsubgraph Mixer[ELMo Scalar Mixer]\n    A0[\"$$a_{0}$$\"] --> Softmax[\"$$\\text{Softmax over } a_{i}$$\"]\n    A1[\"$$a_{1}$$\"] --> Softmax\n    A2[\"$$a_{2}$$\"] --> Softmax\n\n    Softmax --> S0[\"$$s_{0}$$\"]\n    Softmax --> S1[\"$$s_{1}$$\"]\n    Softmax --> S2[\"$$s_{2}$$\"]\n\n    Sum[\"$$\\text{Sum }s_{i}h_{i}$$\"]\n    Gamma[Gamma]\nend\n\nH0 --> Sum\nH1 --> Sum\nH2 --> Sum\n\nS0 --> Sum\nS1 --> Sum\nS2 --> Sum\n\nSum --> Gamma\n\n%% 7 ELMo output\nsubgraph ELMO[ELMo Embedding]\n    ELMOvec[\"ELMo(t)\"]\nend\n\nGamma --> ELMOvec\n\nstyle LMHeads fill:#5eaabf,stroke:#5eaabf,stroke-width:2px\n\n```\n\n:::\n\n## Pretraining Objective\n\nELMo uses a bidirectional language modeling objective to maximize the likelihood of observed tokens given all surrounding context. For ease of understanding, we can breakdown the objective into two directional components.\n\nFirst, we have the forward language model objective which predicts each token $x_{t}$ given all prior tokens.\n\n$$\nP_{fwd}(x) = \\prod_{t=1}^{T} P(x_{t}|x_{1}...x_{t-1})\n$$\n\nSecond, we have the backwards language model objective which as the name implies predicts each token $x_{t}$ given all proceeding tokens.\n\n$$\nP_{bwd}(x) = \\prod_{t=1}^{T} P(x_{t}|x_{T}...x_{t+1})\n$$\n\nThe total pretraining objective is the sum of the forward and backward negative log-likelihoods:\n\n$$\n\\mathcal{L} = - \\sum_{t=1}^{T} \\log P_{fwd}(x_t | x_{<t}) \\;-\\; \\sum_{t=1}^{T} \\log P_{bwd}(x_t | x_{>t})\n$$\n\n# Hands-on Implementation\n\nOkay then, time to get our hands dirty! We are going to build an ELMo model from scratch using the JAX deep learning ecosystem. Specifically, we will pre-train the model using the aforementioned language modeling objective. Later, we will fine-tune our scalar mix on a downstream task to evaluate our embeddings. I hope you enjoy!\n\n## Processing Utilities\n\n::: {#d6b991c9 .cell execution_count=2}\n``` {.python .cell-code code-fold=\"true\"}\nimport os\n\nfrom functools import partial\nfrom collections import Counter\nimport itertools\nimport jax\nimport jax.numpy as jnp\nfrom flax import nnx\nimport optax\nimport numpy as np\n\nfrom datasets import load_dataset, DownloadConfig, Dataset, load_from_disk\nimport string\nimport random\nimport orbax.checkpoint as ocp\n\nfrom datasets.utils.logging import disable_progress_bar\ndisable_progress_bar()\n```\n:::\n\n\n::: {#3b37b8d9 .cell execution_count=3}\n``` {.python .cell-code code-fold=\"true\"}\n# Stream data to local disk: one time only\n# You can also just stream from HF, however on reload (i.e. next epoch) you need to stream again (re-download).\n\ndef train_gen():\n    \"\"\"\n    Streams english c4 training data from HF Allen AI. We subset the data to 1M examples.\n    \"\"\"\n    stream = load_dataset(\n        \"allenai/c4\",\n        \"en\",\n        split=\"train\",\n        streaming=True,\n    )\n    for i, row in enumerate(stream):\n        if i >= 1_000_000:\n            break\n        yield row\n\ndef val_gen():\n    \"\"\"\n    Streams english c4 validatiom data from HF Allen AI. We subset the data to 50k examples.\n    \"\"\"\n    stream = load_dataset(\n        \"allenai/c4\",\n        \"en\",\n        split=\"train\",\n        streaming=True,\n    )\n    for i, row in enumerate(stream):\n        if i < 2_000_000:\n            continue\n        if i >= 2_050_000:\n            break\n        yield row\n\n\n# In batches of 1k save the data to disk\ntrain_ds = Dataset.from_generator(\n    train_gen,\n    writer_batch_size=1_000,\n)\n\ntrain_ds.save_to_disk(\"c4_train\")\n\nval_ds = Dataset.from_generator(\n    val_gen,\n    writer_batch_size=1_000,\n)\n\nval_ds.save_to_disk(\"c4_val\")\n```\n:::\n\n\n::: {#ddd44d3d .cell execution_count=4}\n``````````` {.python .cell-code code-fold=\"true\"}\n# Tokens used when padding or encountering unknown characters in a string.\nPAD_CHAR, UNK_CHAR = \"<pad>\", \"<unk>\"\n\n# Tokens used when padding or encountering unknown words in a sentence.\nPAD_WORD, UNK_WORD = \"<pad>\", \"<unk>\"\n\n# Characters that we expect to see in typical English text\nchar_vocab = [PAD_CHAR, UNK_CHAR] + list(string.ascii_lowercase + string.ascii_uppercase + string.digits + string.punctuation)\nchar_to_id = {ch: i for i, ch in enumerate(char_vocab)}\n\ndef build_word_vocab(dataset, batch_size=32, min_freq=20, max_vocab=100_000) -> Tuple[Dict[str, int], Dict[int, str]]:\n    \"\"\"\n    Build a word–to–index mapping from a text dataset.\n\n    The function scans the provided dataset for tokenised words\n    (splitting on whitespace) and retains only words that appear at least\n    :param min_freq: times in the corpus.  The returned dictionary is\n    capped to at most :param max_vocab: entries.\n\n    Parameters\n    ----------\n    dataset\n        Iterable of records where each record is a mapping\n        (``dict``) that contains a ``\"text\"`` key holding the raw string.\n    batch_size\n        Number of records processed in a single counting pass.  Small\n        batches reduce peak memory usage when the underlying dataset is\n        large, but the final counts are the union of all batches.\n    min_freq\n        Minimum frequency required for a word to be included in the\n        resulting vocabularies.\n    max_vocab\n        Maximum number of words (excluding ``<pad>`` and ``<unk>``) to keep in\n        the vocabularies.  The words chosen are the most frequent ones that\n        pass ``min_freq`` filtering.\n\n    Returns\n    -------\n    word_to_index : dict\n        Mapping from a word string to a unique integer ID, with\n        ``<pad>`` mapping to 0 and ``<unk>`` mapping to 1.\n    index_to_word : dict\n        Inverse mapping from integer ID back to the corresponding word.\n\n    Notes\n    -----\n    * The function expects the dataset rows to contain a ``text`` field.\n      If your dataset uses a different key, adapt the ``row[\"text\"]`` line\n      accordingly.\n    * The tokenisation strategy is simplistic: it uses the standard\n      ``str.split()`` which splits on any whitespace.  For more advanced\n      tokenisation pipelines (e.g., handling sub‑words, hyphenated\n      compounds, etc.) replace the ``s.split()`` call accordingly.\n    * The frequency counter is updated in batches to keep the memory\n      footprint low; ``Counter.update`` is called repeatedly on\n      concatenated lists of tokens.\n\n    \"\"\"\n    counter = Counter()\n    buf = []\n\n    for row in dataset:\n        buf.append(row[\"text\"])\n        if len(buf) >= batch_size:\n            for s in buf:\n                counter.update(s.split())\n            buf = []\n\n    if buf:\n        for s in buf:\n            counter.update(s.split())\n\n    # top words with min frequency\n    most_common = [(w, c) for w, c in counter.most_common(max_vocab) if c >= min_freq]\n    word_to_index = {PAD_WORD: 0, UNK_WORD: 1}\n    index_to_word = {0: PAD_WORD, 1: UNK_WORD}\n    for i, (w, _) in enumerate(most_common, start=2):\n        word_to_index[w] = i\n        index_to_word[i] = w\n    return word_to_index, index_to_word\n```````````\n:::\n\n\n::: {#182baea8 .cell execution_count=5}\n``````````` {.python .cell-code code-fold=\"true\"}\nclass StreamingTextDataLoader:\n    \"\"\"\n    A streaming data loader for text corpora that yields batches of tokenized\n    sequences suitable for training language models or other sequence‑based tasks.\n\n    This class supports streaming from an arbitrary dataset, automatically shuffling,\n    tokenizing into fixed‑length windows (with stride = seq_len), and encoding both\n    words and characters. It provides batched dictionaries containing ``word_ids``,\n    ``char_ids`` and ``target_ids`` as JAX arrays.\n\n    Parameters\n    ----------\n    ds: Iterable[dict]\n        A dataset yielding rows that contain a text field (e.g., \"text\", \"sentence\",\n        \"review\", or \"content\").  Each row should be a mapping from column name to value.\n    vocab: dict[str, int] | Vocab object\n        Mapping from token strings to unique integer IDs.  Used to convert words into\n        ``word_ids`` and optionally for character vocabulary lookup.\n    char_to_id: dict[str, int]\n        Mapping from characters to their integer IDs; needed for ``char_encode``.\n    seq_len: int\n        Length of the input sequence window (number of tokens). Each yielded batch will\n        contain sequences of this size.\n    word_len: int\n        Maximum length of each token when encoded at the character level. Tokens longer\n        than this are truncated; shorter ones are padded with ``PAD_CHAR``.\n    batch_size: int\n        Number of examples to accumulate before yielding a batched dictionary.\n    shuffle_buffer: int, optional (default=2048)\n        Size of the buffer used for shuffling tokens from the stream. Larger buffers\n        provide better randomness at the cost of memory usage.\n    seed: int or None, optional (default=0)\n        Random seed for reproducibility when shuffling.\n\n    Attributes\n    ----------\n    self.ds: original iterable dataset.\n    self.vocab: token vocabulary.\n    self.char_to_id: character‑to‑ID mapping.\n    self.seq_len: sequence length used for chunking.\n    self.word_len: maximum character width per token.\n    self.batch_size: number of samples per batch to yield.\n    self.shuffle_buffer: size of the shuffle buffer.\n    self.seed: RNG seed.\n    self.token_buffer: internal queue holding pre‑assembled tokens awaiting windowing.\n\n    Yields\n    ------\n    dict\n        A dictionary with three JAX arrays:\n        ``word_ids`` – shape ``(batch_size, seq_len)`` integer IDs for each token,\n        ``char_ids`` – shape ``(batch_size, seq_len, word_len)`` character‑level IDs,\n        ``target_ids`` – shifted version of ``word_ids`` used as language‑model targets.\n\n    Notes\n    -----\n    * The class performs a non‑overlapping stride split (`self.token_buffer =\n      self.token_buffer[self.seq_len:]`) which can be changed to an overlapping stride\n      if desired for better coverage.\n    * All returned arrays are JAX ``jnp`` objects, compatible with downstream ``jax`\n      transformations (e.g., ``vmap``, ``pmap``).\n    \"\"\"\n    def __init__(self, ds, vocab, char_to_id, seq_len, word_len, batch_size, shuffle_buffer=2048, seed=0):\n        self.ds = ds\n        self.vocab = vocab\n        self.char_to_id = char_to_id\n        self.seq_len = seq_len\n        self.word_len = word_len\n        self.batch_size = batch_size\n        self.shuffle_buffer = shuffle_buffer\n        self.seed = seed\n\n        self.token_buffer = []\n\n    def _get_text_field(self, row):\n        for key in [\"text\", \"sentence\", \"review\", \"content\"]:\n            if key in row:\n                return row[key]\n        raise KeyError(f\"No text field found in row keys: {list(row.keys())}\")\n\n\n    def _encode_window(self, toks):\n        word_ids = [self.vocab.get(w, self.vocab.get(UNK_WORD)) for w in toks]\n\n        char_ids = np.full(\n            (self.seq_len, self.word_len),\n            self.char_to_id[PAD_CHAR],\n            dtype=np.int32,\n        )\n\n        for i, w in enumerate(toks):\n            cids = [self.char_to_id.get(c, self.char_to_id[UNK_CHAR]) for c in w[:self.word_len]]\n            char_ids[i, :len(cids)] = cids\n\n        return {\n            \"word_ids\": np.array(word_ids, dtype=np.int32),\n            \"char_ids\": char_ids,\n        }\n\n    def _shuffle_buffer_iter(self, ds):\n        buf = []\n        for row in ds:\n            buf.append(row)\n            if len(buf) >= self.shuffle_buffer:\n                random.shuffle(buf)\n                while buf:\n                    yield buf.pop()\n        random.shuffle(buf)\n        while buf:\n            yield buf.pop()\n\n    def __iter__(self):\n        self.token_buffer = []\n        batch_words, batch_chars, batch_targets = [], [], []\n\n        for row in self._shuffle_buffer_iter(self.ds):\n            text = self._get_text_field(row)\n\n            new_toks = text.split()\n            self.token_buffer.extend(new_toks)\n\n            while len(self.token_buffer) >= self.seq_len + 1:\n                x_toks = self.token_buffer[:self.seq_len]\n                y_toks = self.token_buffer[1:self.seq_len + 1]\n\n                # Non-overlapping stride (for speed) ideally you want overlapping\n                self.token_buffer = self.token_buffer[self.seq_len:]\n\n                x_enc = self._encode_window(x_toks)\n                y_ids = np.array(\n                    [self.vocab.get(w, self.vocab.get(UNK_WORD)) for w in y_toks],\n                    dtype=np.int32,\n                )\n\n                batch_words.append(x_enc[\"word_ids\"])\n                batch_chars.append(x_enc[\"char_ids\"])\n                batch_targets.append(y_ids)\n\n                if len(batch_words) == self.batch_size:\n                    yield {\n                        \"word_ids\": jnp.stack(batch_words),\n                        \"char_ids\": jnp.stack(batch_chars),\n                        \"target_ids\": jnp.stack(batch_targets),\n                    }\n                    batch_words, batch_chars, batch_targets = [], [], []\n\n```````````\n:::\n\n\n## Build Vocabulary\n\n::: {#0064829f .cell execution_count=6}\n``` {.python .cell-code code-fold=\"true\"}\nhf_ds = load_from_disk(\"c4_train\")\nvocab, _ = build_word_vocab(hf_ds)\n```\n:::\n\n\n## Define Architecture Components\n\n::: {#f2c66d9e .cell execution_count=7}\n````````````` {.python .cell-code}\nclass Highway(nnx.Module):\n    \"\"\"\n    A simple ``Highway`` network module as introduced in *Hierarchical Multi‑Scale\n    Recurrent Neural Networks* (Srivastava & Salakhutdinov, 2013).\n\n    The layer implements the classic gating mechanism that controls how much of the\n    transformed input should be let through versus left for a direct residual shortcut.\n\n    Parameters\n    ----------\n    dim : int\n        Dimensionality of the input and output vectors.  This size determines both\n        ``proj`` (feed‑forward) and ``trans`` (gate) linear transformations, which map\n        ``dim`` → ``dim``.\n    rngs : nnx.Rngs\n        Random number generators used by the underlying ``nnx.Linear`` layers to\n        create their weights.  The caller typically passes ``nnx.Rngs(seed=...)`` or\n        a sub‑generator (e.g., ``rngs['weights']``) so that model initialisation can be\n        reproducible and/or partitioned across devices.\n\n    Returns\n    -------\n    nnx.Module\n        A callable module whose ``__call__(self, x)`` method returns\n\n        .. code-block:: python\n\n            H * T + x * (1 - T)\n\n        where\n\n        * ``proj(x)`` → ``H = relu(proj(x))`` is the transformed (candidate) signal,\n        * ``trans(x)`` → ``T = sigmoid(trans(x))`` is a gate in ``[0, 1]``,\n        * ``x * (1 - T)`` passes through the original input weighted by the complement of\n          the gate.\n\n        The result can be interpreted as:  \n\n        - If the gate ``T`` ≈ 1 → output ≈ H (the transformed path dominates),  \n        - If the gate ``T`` ≈ 0 → output ≈ x (the residual path dominates).\n\n    Notes\n    -----\n    * This implementation deliberately uses **nnx.Linear** (instead of JAX's built‑in\n      ``nn.Dense``) to stay compatible with the ``nnx.Module`` API and to expose a user-\n      supplied RNG for deterministic initialisation.\n    \"\"\"\n    def __init__(self, dim, *, rngs: nnx.Rngs):\n        # linear transforms are nnx.Linear\n        self.proj = nnx.Linear(dim, dim, rngs=rngs)\n        self.trans = nnx.Linear(dim, dim, rngs=rngs)\n    def __call__(self, x):\n        H = jax.nn.relu(self.proj(x))\n        T = jax.nn.sigmoid(self.trans(x))\n        return H * T + x * (1 - T)\n\n\nclass CharCNN(nnx.Module):\n    \"\"\"\n    Character‑level convolutional encoder that produces a dense\n    representation for each token in a sequence.\n\n    The module first embeds each input character, applies a set of 1‑D\n    convolutions across the character dimension, performs a global\n    max‑pool, concatenates the filter responses, and optionally\n    processes them through Highway layers and a final projection\n    layer.\n\n    Parameters\n    ----------\n    vocab_size : int\n        Size of the character vocabulary.  Must be at least the number\n        of distinct characters (plus any padding/unknown tokens) used\n        in the input data.\n\n    char_dim : int\n        Dimensionality of the learned character embeddings.\n\n    filters : Sequence[tuple[int, int]]\n        A list of `(width, out_channels)` pairs that specify the\n        kernel width and number of output channels for each 1‑D\n        convolution.  For example, ``[(3, 50), (4, 50), (5, 50)]``\n        creates three separate convolutions with widths 3, 4, and 5\n        respectively.  The concatenated output dimension of the\n        convolution stack equals the sum of all ``out_channels``.\n\n    highway_layers : int\n        Number of Highway layers applied after the convolution\n        stack.  Each Highway layer implements a gated residual\n        connection and is useful for learning identity mappings\n        while still providing non‑linear transformations.\n\n    proj_dim : int | None, default ``None``\n        If given, a final linear projection is applied to the\n        concatenated convolution+highway features to reduce (or\n        expand) the dimensionality to ``proj_dim``.  If ``None``,\n        the output dimensionality equals the total number of\n        convolution channels.\n\n    rngs : nnx.Rngs\n        Random number generator pool used to initialize weights and\n        biases of the embedding, convolution, Highway, and\n        projection layers.\n\n    Notes\n    -----\n    * **Input shape**: ``char_ids`` must have shape\n      ``[B, T, W]`` where ``B`` is the batch size, ``T`` the number of\n      tokens per sequence, and ``W`` the maximum number of characters\n      per token (words are padded to this length).\n    * Each convolution operates on the character dimension.\n      After convolution it is followed by a ReLU activation and a\n      channel‑wise global max‑pool over the remaining character\n      positions, resulting in a single scalar per filter channel.\n    * If ``proj_dim`` is ``None`` the output shape will be\n      ``[B, T, total_filters]`` where\n      ``total_filters`` is the sum of all ``out_channels`` across\n      the filters.  If a projection is used the output shape is\n      ``[B, T, proj_dim]``.\n\n    Returns\n    -------\n    jnp.ndarray\n        Character‑derived embedding of shape ``[B, T, proj_dim]`` if a\n        projection layer is supplied, otherwise\n        ``[B, T, total_filters]``.\n    \"\"\"\n    def __init__(self, vocab_size, char_dim, filters, highway_layers, proj_dim=None, *, rngs: nnx.Rngs):\n        # embedding\n        self.emb = nnx.Embed(vocab_size, char_dim, rngs=rngs)\n        # convolution filters: we'll create a small nnx.Conv for each width\n        self.convs = nnx.List([nnx.Conv(\n            in_features=char_dim,\n            out_features=out_channels,\n            kernel_size=(width,),\n            feature_group_count=1,\n            use_bias=True,\n            rngs=rngs\n        )\n        for (width, out_channels) in filters])\n        # highway layers\n        total_filters = sum(out for _, out in filters)\n        self.highways = nnx.List([Highway(total_filters, rngs=rngs) for _ in range(highway_layers)])\n        self.proj_dim = proj_dim\n        if proj_dim is not None:\n            self.proj = nnx.Linear(total_filters, proj_dim, rngs=rngs)\n\n    def __call__(self, char_ids):  # char_ids: [B, T, W] ints\n        B, T, W = char_ids.shape\n        # embed -> [B, T, W, D]\n        x = self.emb(char_ids)  # embed handles indexing for you\n        # apply convs along the word-length axis: we first reshape to merge batch/time\n        x_flat = x.reshape((B*T, W, x.shape[-1]))  # [B*T, W, D]\n        conv_outs = []\n        for conv in self.convs:\n            # nnx.Conv expects [..., length, channels] then kernel over length\n            y = conv(x_flat)  # [B*T, new_len, out_ch]\n            y = jax.nn.relu(y)\n            y = jnp.max(y, axis=1)  # max-pool over positions -> [B*T, out_ch]\n            conv_outs.append(y)\n        x_cat = jnp.concatenate(conv_outs, axis=-1)  # [B*T, total_filters]\n        # highways\n        for h in self.highways:\n            x_cat = h(x_cat)\n        if self.proj_dim is not None:\n            x_cat = self.proj(x_cat)  # [B*T, proj_dim]\n        return x_cat.reshape((B, T, -1))  # [B, T, embed_dim]\n\n\nclass LSTMCell(nnx.Module):\n    \"\"\"\n    A single LSTM cell implemented in ``flax.nnx`` that supports\n    dropout on both the input and the recurrent connection.\n\n    Parameters\n    ----------\n    input_dim : int\n        Dimensionality of the input vector ``x_t`` (e.g. word embedding\n        size).\n    hidden_dim : int\n        Number of LSTM hidden units; defines the size of the hidden\n        state ``h`` and cell state ``c``.\n    dropout : float, default 0.0\n        Dropout probability applied to both the input vector and the\n        recurrent hidden state when ``deterministic=False``.  Valid\n        values are in ``[0, 1)``; ``0`` disables dropout.\n    rngs : nnx.Rngs | None, default ``None``\n        RNG pool used for initializing the weight matrices\n        ``Wx`` and ``Wh``.  If ``None`` the default RNG pool from\n        the parent module is used.\n\n    Notes\n    -----\n    * Internal weight maps:\n\n      ``Wx : input_dim: 4·hidden_dim``  \n      ``Wh : hidden_dim: 4·hidden_dim``\n\n      The four output channels correspond respectively to the\n      *input*, *forget*, *output*, and *candidate* gates.\n\n    * Dropout is applied according to the standard\n      “inverted” scheme (`mask / (1‑p)`), where the mask is drawn\n      from a Bernoulli distribution with probability `1‑dropout`.\n      Two independent masks are used: one for the current input\n      ``x_t`` and one for the recurrent hidden state ``h``.  A\n      distinct RNG key must be passed via ``jax_rng`` when\n      ``deterministic=False``.\n\n    * The cell state is a tuple ``(h, c)`` where each component has\n      shape ``[batch, hidden_dim]``.  The method returns a tuple\n      ``((h_new, c_new), h_new)``; the outer tuple contains the\n      updated cell state, and the inner ``h_new`` is the output\n      vector that can be consumed by ``nnx.scan`` or another\n      sequence wrapper.\n\n    Returns\n    -------\n    tuple\n        * ``((h_new, c_new), h_new)``  \n          where ``h_new`` and ``c_new`` are arrays of shape\n          ``[batch, hidden_dim]`` representing the updated\n          hidden and cell states, respectively.  The second\n          ``h_new`` in the outer tuple is the output of this\n          cell and matches the batch dimension of x_t``.\n    \"\"\"\n    def __init__(self, input_dim, hidden_dim, dropout=0.0, *, rngs=None):\n        self.Wx = nnx.Linear(input_dim, 4 * hidden_dim, rngs=rngs)\n        self.Wh = nnx.Linear(hidden_dim, 4 * hidden_dim, rngs=rngs)\n        self.hidden_dim = hidden_dim\n        self.dropout = dropout\n\n    def __call__(self, carry, x_t, deterministic=True, jax_rng=None):\n        h, c = carry\n\n        if not deterministic:\n            assert jax_rng is not None, \"RNG key must be passed for dropout\"\n            rng_inp, rng_rec = jax.random.split(jax_rng)\n            x_mask = jax.random.bernoulli(rng_inp, 1.0 - self.dropout, x_t.shape)\n            x_t = x_t * x_mask / (1.0 - self.dropout)\n\n            h_mask = jax.random.bernoulli(rng_rec, 1.0 - self.dropout, h.shape)\n            h = h * h_mask / (1.0 - self.dropout)\n\n        gates = self.Wx(x_t) + self.Wh(h)\n        i, f, o, g = jnp.split(gates, 4, axis=-1)\n        i = jax.nn.sigmoid(i)\n        f = jax.nn.sigmoid(f)\n        o = jax.nn.sigmoid(o)\n        g = jnp.tanh(g)\n        c_new = f * c + i * g\n        h_new = o * jnp.tanh(c_new)\n        return (h_new, c_new), h_new\n\n\nclass BiLSTMLayer(nnx.Module):\n    \"\"\"\n    A bidirectional LSTM layer built on top of :class:`LSTMCell`.\n\n    Each input sequence is processed in two directions:\n    * a forward LSTM that runs from the first to the last token,\n    * a backward LSTM that runs from the last to the first token.\n    The outputs of the two directions are returned separately and also\n    concatenated along the feature dimension.\n\n    Parameters\n    ----------\n    in_dim : int\n        Dimensionality of input tokens (`D` in the shape `(B, T, D)`).\n    hidden_dim : int\n        Size of the hidden state in each direction (`h` and `c`).\n    dropout : float, default 0.0\n        Dropout probability applied inside each :class:`LSTMCell`.  The\n        same dropout probability is used for both forward and backward\n        streams.  Valid values are in ``[0, 1)``; ``0`` disables dropout.\n    rngs : nnx.Rngs | None, default ``None``\n        RNG pool used to initialise the two :class:`LSTMCell` instances.\n        If ``None`` the module inherits the RNG from its parent.\n\n    Notes\n    -----\n    * **RNG handling** –  \n      When ``deterministic=False`` a single ``jax_rng`` is split once\n      into two keys (for forward and backward).  These keys are then\n      split further into a per‑time‑step key that is passed to\n      :class:`LSTMCell`.  For deterministic execution ``rngs_fwd`` and\n      ``rngs_bwd`` are lists of ``None``.\n    * **State management** –  \n      Each direction starts from a zero initial hidden and cell state of\n      shape ``(B, hidden_dim)``.\n    * **Output dimensions** –  \n      For an input of shape ``(B, T, D)`` the three returned tensors have\n      shapes:\n        * ``hs_fwd``: ``(B, T, hidden_dim)``\n        * ``hs_bwd``: ``(B, T, hidden_dim)``\n        * ``hs_concat``: ``(B, T, 2 * hidden_dim)``\n\n    Returns\n    -------\n    tuple\n        ``(hs_fwd, hs_bwd, hs_concat)``\n        * ``hs_fwd`` – hidden states produced by the forward LSTM,\n        * ``hs_bwd`` – hidden states produced by the backward LSTM,\n        * ``hs_concat`` – concatenation of ``hs_fwd`` and ``hs_bwd``.\n\n    \"\"\"\n    def __init__(self, in_dim, hidden_dim, dropout=0.0, *, rngs=None):\n        self.fwd = LSTMCell(in_dim, hidden_dim, dropout=dropout, rngs=rngs)\n        self.bwd = LSTMCell(in_dim, hidden_dim, dropout=dropout, rngs=rngs)\n\n    def __call__(self, inputs, deterministic=True, jax_rng=None):\n        B, T, D = inputs.shape\n        h0 = jnp.zeros((B, self.fwd.hidden_dim))\n        c0 = jnp.zeros((B, self.fwd.hidden_dim))\n\n        if not deterministic and jax_rng is not None:\n            # Pre-split RNGs for each time step\n            rng_fwd, rng_bwd = jax.random.split(jax_rng)\n            rngs_fwd = jax.random.split(rng_fwd, T)\n            rngs_bwd = jax.random.split(rng_bwd, T)\n\n        else:\n            rngs_fwd = [None] * T\n            rngs_bwd = [None] * T\n\n        def fwd_scan(carry, x_and_rng):\n            x_t, rng_t = x_and_rng\n            return self.fwd(carry, x_t, deterministic=deterministic, jax_rng=rng_t)\n\n        def bwd_scan(carry, x_and_rng):\n            x_t, rng_t = x_and_rng\n            return self.bwd(carry, x_t, deterministic=deterministic, jax_rng=rng_t)\n\n        xs_fwd = (inputs.swapaxes(0, 1), rngs_fwd)\n        xs_bwd = (jnp.flip(inputs, axis=1).swapaxes(0, 1), rngs_bwd)\n\n        _, hs_fwd = jax.lax.scan(fwd_scan, (h0, c0), xs_fwd)\n        hs_fwd = hs_fwd.swapaxes(0, 1)\n\n        _, hs_bwd_rev = jax.lax.scan(bwd_scan, (h0, c0), xs_bwd)\n        hs_bwd = jnp.flip(hs_bwd_rev.swapaxes(0, 1), axis=1)\n\n        return hs_fwd, hs_bwd, jnp.concatenate([hs_fwd, hs_bwd], axis=-1)\n\n\nclass StackedBiLSTM(nnx.Module):\n    \"\"\"\n    A stack of bidirectional LSTM layers implemented with :class:`BiLSTMLayer`.\n\n    Each layer receives the concatenated hidden states of its predecessors\n    (``h_fwd`` || ``h_bwd``) as input, thus the input dimensionality of\n    subsequent layers doubles relative to the first layer. This design is\n    common in sequence‑to‑sequence and encoder‑decoder architectures where\n    both forward and backward context are preserved.\n\n    Parameters\n    ----------\n    input_dim : int\n        Dimensionality of the raw token embeddings fed to the first\n        :class:`BiLSTMLayer`.  The dimensionality of all following layers\n        will be `2 * hidden_dim`.\n    hidden_dim : int\n        Hidden size of each unidirectional LSTM within every\n        :class:`BiLSTMLayer`.  The actual output of a layer therefore\n        has shape ``(B, T, 2 * hidden_dim)``.\n    num_layers : int\n        Number of stacked bidirectional layers.\n    dropout : float, default 0.0\n        Dropout probability applied inside each :class:`LSTMCell`.  Dropout\n        is *only* applied when ``deterministic=False``; otherwise it is\n        bypassed.\n    rngs : nnx.Rngs | None, default ``None``\n        RNG pool used to initialise all child modules.  ``None`` defaults\n        to the parent module’s RNG.\n\n    Notes\n    -----\n    * **State propagation** –  \n      Each layer starts with its own zero initial hidden/cell states\n      internally when calling its :class:`BiLSTMLayer`. The forward and\n      backward states are returned separately for potential analysis or\n      debugging.\n    * **RNG workflow** –  \n      When ``deterministic=False`` the input ``jax_rng`` is split once\n      per layer, providing each :class:`BiLSTMLayer` with a distinct\n      RNG.  When ``deterministic=True`` no RNG is used and all\n      dropout calls are bypassed.\n\n    Returns\n    ------- \n      1. ``outs`` – A list containing the *raw input* followed by the\n         concatenated output of each layer.  Hence ``len(outs) ==\n         num_layers + 1`` and the last element has shape\n         ``(B, T, 2 * hidden_dim)``.\n      2. ``fwd_states`` – A list of the forward hidden states from each\n         layer (shape ``(B, T, hidden_dim)``).\n      3. ``bwd_states`` – A list of the backward hidden states from each\n         layer (shape ``(B, T, hidden_dim)``).\n\n    \"\"\"\n    def __init__(self, input_dim, hidden_dim, num_layers, dropout=0.0, *, rngs=None):\n        self.layers = nnx.List([\n            BiLSTMLayer(input_dim if i == 0 else 2*hidden_dim, hidden_dim, dropout=dropout, rngs=rngs)\n            for i in range(num_layers)\n        ])\n\n    def __call__(self, x, deterministic=False, jax_rng=None):\n        outs = [x]\n        fwd_states, bwd_states = [], []\n\n        if not deterministic and jax_rng is not None:\n            rngs = jax.random.split(jax_rng, len(self.layers))\n        else:\n            rngs = [None] * len(self.layers)\n\n        for layer, r in zip(self.layers, rngs):\n            fwd, bwd, x = layer(x, deterministic=deterministic, jax_rng=r)\n            fwd_states.append(fwd)\n            bwd_states.append(bwd)\n            outs.append(x)\n        return outs, fwd_states, bwd_states\n\n\nclass LMHead(nnx.Module):\n    \"\"\"\n    Language‑model output head that projects hidden states to logits over a\n    target vocabulary.\n\n    The module consists of a single linear transformation that maps the\n    last hidden dimension of the model to a vector of size ``vocab_size``.\n    It is intended to be used as the final layer of a causal or\n    bidirectional language model.\n\n    Parameters\n    ----------\n    hidden_dim : int\n        Dimensionality of the input hidden states ``h``.\n    vocab_size : int\n        Size of the target vocabulary (number of output logits).\n    rngs : nnx.Rngs\n        RNG pool used to initialise the underlying linear layer.\n\n    Notes\n    -----\n    * **Output shape** – If ``h`` has shape ``(B, T, hidden_dim)``, the\n      method returns logits of shape ``(B, T, vocab_size)``.\n    * The implementation uses :class:`flax.nnx.Linear`, which applies a\n      bias term internally.  No activation is applied; the logits are\n      typically passed to ``jax.nn.log_softmax`` or cross‑entropy loss\n      during training.\n\n    Returns\n    -------\n    jax.numpy.ndarray\n        Logits of shape ``(B, T, vocab_size)`` ready for loss\n        computation or inference.\n\n    \"\"\"\n    def __init__(self, hidden_dim, vocab_size, *, rngs: nnx.Rngs):\n        self.linear = nnx.Linear(hidden_dim, vocab_size, rngs=rngs)\n    def __call__(self, h):\n        return self.linear(h)  # [B,T,V]\n`````````````\n:::\n\n\n::: {#c52fa6a3 .cell execution_count=8}\n``````````````` {.python .cell-code}\nclass ElmoModel(nnx.Module):\n    \"\"\"\n    ELMo‑style character‑level language model.\n\n    The model implements the core components of the original ELMo\n    architecture: a character‑level CNN that produces sub‑word\n    representations, a stack of bidirectional LSTMs that encode the\n    sentence, and forward/backward language‑model heads.  A\n    *scalar‑mix* (parameterised by a softmax over learnable weights)\n    combines the character‑CNN output and every LSTM layer into a\n    fixed‑dimensional semantic vector (`common_dim`).  This vector can\n    be used as contextualised word embeddings downstream.\n\n    Parameters\n    ----------\n    char_vocab_size : int\n        Vocabulary size for character indices.\n    char_dim : int\n        Size of the character embedding vectors.\n    filters : Sequence[Tuple[int, int]]\n        List of ``(num_filters, filter_width)`` tuples that define the\n        convolutional channels in the character CNN.\n    highway_layers : int\n        Number of highway network layers in the character CNN.\n    proj_dim : int\n        Dimensionality of the output of the projection layer that comes\n        after the character CNN.\n    common_dim : int\n        Dimensionality of the final ELMo embedding (the “common space”).\n    hidden_dim : int\n        Hidden state size of each BiLSTM cell.\n    num_layers : int\n        Number of stacked BiLSTM layers.\n    word_vocab_size : int\n        Size of the vocabulary for the forward and backward language‑model\n        heads.\n    input_dropout : float, default 0.1\n        Dropout probability applied to the output of the character CNN\n        during training.\n    lstm_dropout : float, default 0.1\n        Dropout probability applied within each BiLSTM layer during\n        training.\n    output_dropout : float, default 0.1\n        Dropout probability applied to the top‑layer LSTM states before\n        they are fed to the language‑model heads.\n    rngs : nnx.Rngs\n        JAX random number generator state used to initialise parameters.\n\n    Notes\n    -----\n    * The character‑CNN (`self.char_cnn`) maps the raw character IDs to\n      a vector of dimensionality ``proj_dim``.  This vector is then\n      projected to the common embedding space (`common_dim`) by a\n      linear layer.\n    * Each BiLSTM layer produces a forward state of shape\n      `(batch, seq_len, hidden_dim)` and a backward state of the\n      same shape.  The states of a layer are concatenated along the\n      feature dimension and projected to ``common_dim``.\n    * The scalar mix treats the character‑CNN output as layer 0 and\n      each BiLSTM layer as a subsequent layer.  The weight vector\n      (`self.scalar_weights`) is soft‑maxed so that the weights sum\n      to 1.  The result is scaled by the learnable `gamma` parameter.\n    * During evaluation (``deterministic=True``) drop‑outs are disabled\n      and the same provided RNG is used to keep the computation\n      deterministic.\n\n    Methods\n    -------\n    forward_backbone(char_ids, jax_rng, deterministic=True)\n        Compute the character embeddings, forward and backward LSTM states.\n\n        Returns\n        -------\n        char_embs : jnp.ndarray\n            The raw output of the character CNN, shape\n            `(batch, seq_len, word_len, proj_dim)`.\n        fwd_states : List[jnp.ndarray]\n            Forward LSTM states for each of the ``num_layers`` layers,\n            each of shape `(batch, seq_len, hidden_dim)`.\n        bwd_states : List[jnp.ndarray]\n            Backward LSTM states for each layer, each of shape\n            `(batch, seq_len, hidden_dim)`.\n\n    forward_logits(char_ids, jax_rng, deterministic=True)\n        Return the forward and backward language‑model logits together\n        with the intermediate representations.\n\n        Returns\n        -------\n        fwd_logits : jnp.ndarray\n            Forward language‑model logits, shape\n            `(batch, seq_len, word_vocab_size)`.\n        bwd_logits : jnp.ndarray\n            Backward language‑model logits (time‑reversed), same shape as\n            ``fwd_logits``.\n        char_embs : jnp.ndarray\n            Raw character‑CNN embeddings (as in ``forward_backbone``).\n        fwd_states : List[jnp.ndarray]\n            Forward LSTM states.\n        bwd_states : List[jnp.ndarray]\n            Backward LSTM states.\n\n    forward_embeddings(char_embs, fwd_states, bwd_states)\n        Produce the final contextualised embedding vector.\n\n        Returns\n        -------\n        x : jnp.ndarray\n            Contextualised ELMo embedding of shape\n            `(batch, seq_len, common_dim)`.  It is a weighted sum of the\n            projected character‑CNN output and each concatenated\n            forward/backward LSTM layer, scaled by `gamma`.\n\n    \"\"\"\n    def __init__(self, char_vocab_size, char_dim, filters, highway_layers,\n                 proj_dim, common_dim, hidden_dim, num_layers, word_vocab_size,\n                 input_dropout=0.1, lstm_dropout=0.1, output_dropout=0.1, *,\n                 rngs: nnx.Rngs):\n        # --- Submodules ---\n        self.char_cnn = CharCNN(char_vocab_size, char_dim, filters, highway_layers, proj_dim=proj_dim, rngs=rngs)\n        \n        self.bilstm = StackedBiLSTM(proj_dim, hidden_dim, num_layers, dropout=lstm_dropout, rngs=rngs)\n        self.fwd_head = LMHead(hidden_dim, word_vocab_size, rngs=rngs)\n        self.bwd_head = LMHead(hidden_dim, word_vocab_size, rngs=rngs)\n\n        # --- Scalar mix for ELMo embeddings ---\n        self.common_dim = common_dim\n        self.scalar_weights = nnx.Param(jnp.zeros(num_layers + 1))\n        self.gamma = nnx.Param(jnp.array(1.0))\n\n        # --- Projection layers to common_dim (NNX-safe) ---\n        self.layer_projections = nnx.List()\n\n        # CharCNN output: proj_dim → common_dim\n        self.layer_projections.append(\n            nnx.Linear(proj_dim, common_dim, rngs=rngs)\n        )\n\n        # BiLSTM layers: (2 * hidden_dim) → common_dim\n        for _ in range(num_layers):\n            self.layer_projections.append(\n                nnx.Linear(2 * hidden_dim, common_dim, rngs=rngs)\n            )\n\n        assert len(self.layer_projections) == len(self.scalar_weights.value)\n\n        # Dropout layers\n        self.input_dropout = nnx.Dropout(rate=input_dropout, rngs=rngs)\n        self.output_dropout = nnx.Dropout(rate=output_dropout, rngs=rngs)\n\n    def forward_backbone(self, char_ids, jax_rng, deterministic: bool = True):\n        char_embs = self.char_cnn(char_ids)\n\n        if not deterministic:\n            assert jax_rng is not None\n            rng_in, rng_lstm = jax.random.split(jax_rng)\n            x = self.input_dropout(char_embs, rngs=rng_in)\n        else:\n            x = char_embs\n            rng_lstm = jax_rng\n\n        _, fwd_states, bwd_states = self.bilstm(x, deterministic=deterministic, jax_rng=rng_lstm)\n\n        return char_embs, fwd_states, bwd_states\n\n    def forward_logits(self, char_ids, jax_rng, deterministic: bool = True):\n        char_embs, fwd_states, bwd_states = self.forward_backbone(\n            char_ids, deterministic=deterministic, jax_rng=jax_rng\n        )\n\n        top_fwd = fwd_states[-1]\n        top_bwd = bwd_states[-1]\n\n        top_fwd = self.output_dropout(top_fwd)\n        top_bwd = self.output_dropout(top_bwd)\n\n        fwd_logits = self.fwd_head(top_fwd)\n        bwd_logits = jnp.flip(\n            self.bwd_head(jnp.flip(top_bwd, axis=1)), axis=1\n        )\n\n        return fwd_logits, bwd_logits, char_embs, fwd_states, bwd_states\n\n    def forward_embeddings(self, char_embs, fwd_states, bwd_states):\n        layers = [char_embs] + [\n            jnp.concatenate([fwd, bwd], axis=-1)\n            for fwd, bwd in zip(fwd_states, bwd_states)\n        ]\n\n        w = jax.nn.softmax(self.scalar_weights.value)\n\n        projected = [\n            proj(layer) for proj, layer in zip(self.layer_projections, layers)\n        ]\n\n        x = sum(w_i * p for w_i, p in zip(w, projected))\n        x = self.gamma.value * x\n\n        return x\n\n```````````````\n:::\n\n\n## Loss Function\n\n::: {#d014d3ed .cell execution_count=9}\n``````````` {.python .cell-code}\ndef masked_cross_entropy(logits, targets, pad_id=0):\n    \"\"\"\n    Computes the average cross‑entropy loss for a batch while ignoring\n    padding tokens.\n\n    The function applies a *mask* to the per‑token loss so that any token\n    whose target index equals ``pad_id`` is dropped from the loss\n    calculation.\n\n    Parameters\n    ----------\n    logits : jnp.ndarray\n        Logits produced by the model.  Expected shape\n        ``(batch, seq_len, vocab_size)``.\n    targets : jnp.ndarray\n        Ground‑truth token indices.  Expected shape ``(batch, seq_len)``\n        with integer values in ``[0, vocab_size)``.  Positions that\n        contain ``pad_id`` are treated as padding.\n    pad_id : int, default=0\n        The integer value used to mark padding positions in ``targets``.\n\n    Returns\n    -------\n    loss : float\n        The mean cross‑entropy loss over all non‑padding tokens in the\n        batch.  The denominator is the total number of non‑padding\n        tokens plus a small constant ``1e-12`` to avoid division by\n        zero.\n\n    Notes\n    -----\n    * ``jax.nn.log_softmax`` is applied along the last dimension\n      (the vocabulary axis).\n    * ``jax.nn.one_hot`` is used to create a one‑hot representation of\n      ``targets`` which allows broadcasting against the log‑probabilities.\n    * The mask is cast to ``float32``; any token whose target equals\n      ``pad_id`` will contribute a multiplier of ``0`` to the loss and\n      therefore will not affect the final average.\n\n    \"\"\"\n    vocab_size = logits.shape[-1]\n    log_probs = jax.nn.log_softmax(logits, axis=-1)\n    targets_onehot = jax.nn.one_hot(targets, vocab_size)\n    per_token_loss = -jnp.sum(targets_onehot * log_probs, axis=-1)\n    mask = (targets != pad_id).astype(jnp.float32)\n    return jnp.sum(per_token_loss * mask) / (jnp.sum(mask) + 1e-12)\n```````````\n:::\n\n\n## Define Training Loop\n\n::: {#8766f357 .cell execution_count=10}\n``````````` {.python .cell-code code-fold=\"true\"}\n@jax.jit\ndef train_step(optimizer, batch, jax_rng):\n    \"\"\"\n    Performs a single optimisation update on the ELMo‐style model.\n\n    The function runs a forward pass of the model to obtain forward and\n    backward language‑model logits, computes the masked cross‑entropy\n    loss for each direction, sums the two losses, back‑propagates the\n    gradients, and finally applies the optimiser's update rule.\n\n    Parameters\n    ----------\n    optimizer : nnx.optim.Optimizer\n        An *nnx.optim.Optimizer* that owns the model to be trained.\n        The optimiser must expose a ``model`` attribute and provide an\n        ``update`` method that accepts the gradient dictionary.\n    batch : Mapping[str, jnp.ndarray]\n        A batch of training data mapping the following keys to\n        integer arrays:\n        * ``\"char_ids\"``: shape ``(batch, seq_len, word_len)``\n          containing character indices for each token.\n        * ``\"target_ids\"``: shape ``(batch, seq_len)``\n          containing the target token indices for the language‑model head.\n    jax_rng : jax.random.PRNGKey\n        Random number generator used for dropout and other stochastic\n        components of the model.  A new key will be split and passed to\n        the model during the forward pass.\n\n    Returns\n    -------\n    optimizer : nnx.optim.Optimizer\n        The optimiser after the update.  It holds the freshly\n        updated model parameters.\n    loss : float\n        The scalar loss value that was optimised.  It is the sum of the\n        forward and backward masked cross‑entropy losses.\n\n    Notes\n    -----\n    * Dropout is enabled for the forward pass by passing ``deterministic=False``.\n      During inference or evaluation the optimiser should call the model\n      with ``deterministic=True``.\n    * The loss function internally masks padding tokens using\n      :func:`masked_cross_entropy`.  The forward loss uses the target\n      token at position *i+1* to predict the token at *i*, while the\n      backward loss predicts the preceding token.  This mirrors the\n      bidirectional language‑model objective used in the original\n      ELMo paper.\n    * Gradients are computed using the `nnx.grad` helper which returns\n      a dictionary mapping each leaf parameter in the model to its\n      gradient.  This dictionary is directly handed to\n      ``optimizer.update``.\n    * The optimiser should store the model in the ``optimizer.model``\n      attribute.  After the update, modifications to ``optimizer.model``\n      reflect the new parameters.\n\n    \"\"\"\n    char_ids = batch[\"char_ids\"]\n    targets  = batch[\"target_ids\"]\n\n    def loss_fn(model):\n        fwd_logits, bwd_logits, _, _, _ = model.forward_logits(\n            char_ids, deterministic=False, jax_rng=jax_rng\n        )\n\n        fwd_loss = masked_cross_entropy(\n            fwd_logits[:, :-1, :], targets[:, 1:]\n        )\n\n        bwd_loss = masked_cross_entropy(\n            bwd_logits[:, 1:, :], targets[:, :-1]\n        )\n\n        return fwd_loss + bwd_loss\n\n    loss = loss_fn(optimizer.model)\n    grads = nnx.grad(loss_fn)(optimizer.model)\n    optimizer.update(grads)\n\n    return optimizer, loss\n```````````\n:::\n\n\n::: {#64f69c59 .cell execution_count=11}\n``` {.python .cell-code code-fold=\"true\"}\n# RNG setup\nrngs = nnx.Rngs(0)\n\n# Hyperparameters\nchar_vocab_size = len(char_to_id)\nchar_dim = 16\nfilters = [\n    (1, 64),\n    (2, 128),\n    (3, 256),\n    (4, 256),\n    (5, 256),\n    (6, 256),\n]\nhighway_layers = 2\nproj_dim = 512\nhidden_dim = 512\nnum_layers = 2\n\nword_vocab_size = len(vocab)\ncommon_dim = 512\n\n# Model instantiation\nmodel = ElmoModel(\n    char_vocab_size=char_vocab_size,\n    char_dim=char_dim,\n    filters=filters,\n    highway_layers=highway_layers,\n    proj_dim=proj_dim,\n    common_dim=common_dim,\n    hidden_dim=hidden_dim,\n    num_layers=num_layers,\n    word_vocab_size=word_vocab_size,\n    input_dropout=0.1, \n    lstm_dropout=0.3, \n    output_dropout=0.1,\n    rngs=rngs\n)\n\n# Optimizer instantiation\ntx = optax.adamw(1e-3, weight_decay=1e-2)\noptimizer = nnx.ModelAndOptimizer(model, tx)\n```\n:::\n\n\n::: {#50c337a0 .cell execution_count=12}\n``````````` {.python .cell-code code-fold=\"true\"}\nclass ELMoTrainer:\n    \"\"\"\n    A training wrapper for an ELMobidirectional language model\n    implemented with **NNX**.  It handles epoch‑wise training, validation\n    loss computation, and an early‑stopping strategy.\n\n    Parameters\n    ----------\n    optimizer : nnx.optim.Optimizer\n        The optimiser that owns the model to be trained.  It must expose a\n        ``model`` attribute (the trainable model) and an ``update`` method.\n    patience : int, optional (default=3)\n        Number of consecutive validation epochs without improvement on the\n        loss after which training is stopped early and the best model\n        parameters are restored.\n    rng_seed : int, optional (default=0)\n        Seed for the JAX random number generator.  A fresh RNG key is\n        created in the constructor and split during each training step to\n        keep the random streams deterministic across runs.\n\n    Attributes\n    ----------\n    optimizer : nnx.optim.Optimizer\n        The optimiser being used.\n    patience : int\n        See ``patience`` above.\n    best_loss : float\n        Best validation loss observed so far.  Initialized to ``∞``.\n    wait : int\n        Number of epochs since the last improvement.\n    best_params : nnx.State | None\n        The model parameters that produced the best validation loss.\n        Stored as a JAX ``Mutable`` state so that they can be copied back\n        into ``optimizer.model`` on early‑stopping.\n    jax_rng : jax.random.PRNGKey\n        Current random key.  Split once per batch in :meth:`train_epoch`.\n\n    Methods\n    -------\n    train_epoch(train_loader)\n        Runs one epoch of training on ``train_loader``.\n    validate(model, val_loader)\n        Computes the average loss over ``val_loader``.\n    validate_and_stop(val_loader)\n        Performs validation, logs results and checks the early‑stopping\n        criterion.  Returns ``True`` if training should stop.\n    \"\"\"\n    def __init__(self, optimizer, patience=3, rng_seed=0):\n        self.optimizer = optimizer\n        self.patience = patience\n        self.best_loss = float(\"inf\")\n        self.wait = 0\n        self.best_params = None\n        self.jax_rng = jax.random.PRNGKey(rng_seed)\n\n    def train_epoch(self, train_loader):\n        total_loss = 0.0\n        n = 0\n        for batch in train_loader:\n            self.jax_rng, subkey = jax.random.split(self.jax_rng)\n            self.optimizer, loss = train_step(self.optimizer, batch, jax_rng=subkey)\n            total_loss += float(loss)\n            n += 1\n        return total_loss / max(1, n)\n\n    def validate(self, model, val_loader):\n        total_loss = 0.0\n        n = 0\n\n        for batch in val_loader:\n            targets  = batch[\"target_ids\"]\n\n            fwd_logits, bwd_logits, _, _, _ = model.forward_logits(\n                batch[\"char_ids\"], jax_rng=self.jax_rng\n            )\n\n            # Forward predicts t+1\n            fwd_loss = masked_cross_entropy(fwd_logits[:, :-1, :], targets[:, 1:])\n\n            # Backward predicts t-1\n            bwd_loss = masked_cross_entropy(bwd_logits[:, 1:, :], targets[:, :-1])\n\n            loss = fwd_loss + bwd_loss\n\n            total_loss += float(loss)\n            n += 1\n\n        mean_loss = total_loss / max(1, n)\n        return mean_loss\n\n\n    def validate_and_stop(self, val_loader):\n        val_loss = self.validate(self.optimizer.model, val_loader)\n        print(f\"  val_loss={val_loss:.4f}\")\n\n        if val_loss < self.best_loss:\n            self.best_loss = val_loss\n            self.best_params = nnx.state(self.optimizer.model)\n            self.wait = 0\n            print(\"  New best model saved.\")\n            return False  # continue training\n\n        self.wait += 1\n        print(f\"  No improvement ({self.wait}/{self.patience})\")\n\n        if self.wait >= self.patience:\n            print(\"Early stopping triggered!\")\n            nnx.update(self.optimizer.model, self.best_params)  # restore best params\n            return True  # stop training\n\n        return False\n```````````\n:::\n\n\n::: {#24f19c2b .cell execution_count=13}\n``` {.python .cell-code code-fold=\"true\"}\nckpt_dir = \"./checkpoints/elmo/state/\"\ncheckpointer = ocp.StandardCheckpointer()\n```\n:::\n\n\n::: {#64b15829 .cell execution_count=14}\n``` {.python .cell-code code-fold=\"true\"}\ntrainer = ELMoTrainer(optimizer, patience=100) # No need to early stop with pre-training\n\ntrain_ds = load_from_disk(\"c4_train\")\nval_ds = load_from_disk(\"c4_val\")\n\nepochs = 5\n\nfor epoch in range(epochs):\n    print(f\"Epoch {epoch + 1}\")\n\n    # reset streaming ds\n    train_loader = StreamingTextDataLoader(train_ds, vocab, char_to_id,\n                                    seq_len=128, word_len=50,\n                                    batch_size=20, shuffle_buffer=2048)\n\n    train_loss = trainer.train_epoch(train_loader)\n\n    if (epoch + 1) % 2 == 0:\n        # reset streaming ds\n        val_loader = StreamingTextDataLoader(val_ds, vocab, char_to_id,\n                                        seq_len=128, word_len=50,\n                                        batch_size=20, shuffle_buffer=2048)\n\n        stop = trainer.validate_and_stop(val_loader)\n\n    # Save checkpoint each epoch\n    _, state = nnx.split(trainer.optimizer.model)\n    checkpointer.save(\n        os.path.abspath(\n            os.path.join(ckpt_dir, f\"epoch{ epoch + 1 }\")\n        ), \n        state\n    )\n\n    if stop:\n        break\n```\n:::\n\n\n## Evaluate Learned Embeddings on Downstream Task\n\n### Compare Random Weights to Pretrained Model\n\n::: {#8cd44d6b .cell execution_count=15}\n``` {.python .cell-code code-fold=\"true\"}\n# Initialize a random weights model\n\n# RNG setup\nrng = jax.random.PRNGKey(0)\nrngs = nnx.Rngs(rng)\n\n# Hyperparameters\nchar_vocab_size = len(char_to_id)\nchar_dim = 16\nfilters = [\n    (1, 64),\n    (2, 128),\n    (3, 256),\n    (4, 256),\n    (5, 256),\n    (6, 256),\n]\nhighway_layers = 2\nproj_dim = 512\nhidden_dim = 512\nnum_layers = 2\n\nword_vocab_size = len(vocab)\ncommon_dim = 512\n\n# Model instantiation\nmodel_random = ElmoModel(\n    char_vocab_size=char_vocab_size,\n    char_dim=char_dim,\n    filters=filters,\n    highway_layers=highway_layers,\n    proj_dim=proj_dim,\n    common_dim=common_dim,\n    hidden_dim=hidden_dim,\n    num_layers=num_layers,\n    word_vocab_size=word_vocab_size,\n    input_dropout=0.1, \n    lstm_dropout=0.3, \n    output_dropout=0.1,\n    rngs=rngs\n)\n```\n:::\n\n\n::: {#7cbf3bd9 .cell execution_count=16}\n``` {.python .cell-code code-fold=\"true\"}\n# Load checkpointed model\n\n# Construct an abstract version of the model (this is an empty scaffold so memory utilization is minimal)\nabstract_model = nnx.eval_shape(\n    lambda: ElmoModel(\n        char_vocab_size=char_vocab_size,\n        char_dim=char_dim,\n        filters=filters,\n        highway_layers=highway_layers,\n        proj_dim=proj_dim,\n        common_dim=common_dim,\n        hidden_dim=hidden_dim,\n        num_layers=num_layers,\n        word_vocab_size=word_vocab_size,\n        input_dropout=0.1,\n        lstm_dropout=0.3,\n        output_dropout=0.1,\n        rngs=nnx.Rngs(0)\n    )\n)\n\n# Split to get graphdef and an abstract state\ngraphdef, abstract_state = nnx.split(abstract_model)\n\n# Restore into that abstract state\nckpt_dir = \"./checkpoints/elmo/state/\"\nepoch = 5\ncheckpointer = ocp.StandardCheckpointer()\nrestored_state = checkpointer.restore(\n    os.path.abspath(os.path.join(ckpt_dir, f\"epoch{epoch}\"))\n)\n\n# Merge to produce a real model with pretrained weights\nmodel_trained = nnx.merge(graphdef, restored_state)\n```\n:::\n\n\n::: {#fd85fb96 .cell execution_count=17}\n``` {.python .cell-code code-fold=\"true\"}\n# To load the model that was trained using GPU onto a CPU only device use this:\n\n# Ensure abstract_state is placed on the current local devices\ncpu_device = jax.devices('cpu')[0]\nsharding = jax.sharding.SingleDeviceSharding(cpu_device)\n\n# Construct an abstract version of the model\nabstract_model = nnx.eval_shape(\n    lambda: ElmoModel(\n        char_vocab_size=char_vocab_size,\n        char_dim=char_dim,\n        filters=filters,\n        highway_layers=highway_layers,\n        proj_dim=proj_dim,\n        common_dim=common_dim,\n        hidden_dim=hidden_dim,\n        num_layers=num_layers,\n        word_vocab_size=word_vocab_size,\n        input_dropout=0.1,\n        lstm_dropout=0.3,\n        output_dropout=0.1,\n        rngs=nnx.Rngs(0)\n    )\n)\n\n# Split to get graphdef and an abstract state\ngraphdef, abstract_state = nnx.split(abstract_model)\n\n# Map the sharding onto your abstract state leaves\nabstract_state = jax.tree.map(\n    lambda x: jax.ShapeDtypeStruct(x.shape, x.dtype, sharding=sharding),\n    abstract_state\n)\n\n# 2. Initialize and restore\nckpt_dir = \"./checkpoints/elmo/state/\"\nepoch = 5\ncheckpoint_path = os.path.abspath(os.path.join(ckpt_dir, f\"epoch{epoch}\"))\n\ncheckpointer = ocp.StandardCheckpointer()\n\n# Pass abstract_state as the second positional argument ('target')\nrestored_state = checkpointer.restore(\n    checkpoint_path,\n    abstract_state \n)\n\n# 3. Finalize the model\nmodel_trained = nnx.merge(graphdef, restored_state)\n```\n:::\n\n\n### Classifier Architecture\n\n::: {#e6bd9080 .cell execution_count=18}\n````````` {.python .cell-code}\nclass AttnPool(nnx.Module):\n    \"\"\"\n    Attention‑based pooling layer that collapses a sequence of vectors into a\n    single representation using a learnable attention weight.\n\n    The layer is equivalent to\n\n    .. math::   y = \\sum_{t=1}^{T} \\alpha_t \\, x_t , \\qquad\n                 \\alpha_t = \\\\operatorname{softmax}(A\\,x_t)_t\n\n    where ``A`` is a trainable linear mapping from the input dimension ``dim``\n    to a single scalar.  The attention scores are normalised along the\n    temporal (sequence) dimension, so the output has the same feature\n    dimensionality as each element in the input sequence.\n\n    Parameters\n    ----------\n    dim : int\n        Dimensionality of each input vector (``x.shape[-1]``).  Must equal the\n        feature dimension of the last axis of the input tensor.\n    rngs : jax.random.PRNGKey\n        PRNG key or collection of keys used to initialise the linear\n        projection.  The key is passed directly to :class:`nnx.Linear`.\n\n    Notes\n    -----\n    * **Input shape** – ``x`` must be a 3‑D tensor of shape\n      ``(B, T, D)``, where ``B`` is the batch size, ``T`` is the sequence\n      length, and ``D == dim`` is the feature dimension.\n    * **Output shape** – the module returns a tensor of shape\n      ``(B, D)``, i.e. one vector per batch element that summarises the\n      entire sequence.\n    * **Differentiability** – All operations are JAX‑compatible; gradients\n      flow through the linear projection, the softmax, and the weighted\n      sum.  Consequently the module can be inserted into any neural network\n      that is optimised with ``jax``/NNX.\n    * **Initialisation** – The linear projection is implemented with\n      default Xavier normal weights (as used by :class:`nnx.Linear`), which\n      results in zero‑mean attention scores at the start of training.\n      The ``rngs`` argument allows a user to seed this random initialisation\n      for reproducible experiments.\n\n    \"\"\"\n    def __init__(self, dim, *, rngs):\n        self.proj = nnx.Linear(dim, 1, rngs=rngs)\n\n    def __call__(self, x):\n        # x: [B, T, D]\n        scores = self.proj(x).squeeze(-1)\n        weights = jax.nn.softmax(scores, axis=1)\n        return jnp.sum(x * weights[..., None], axis=1)\n\nclass ElmoClassifier(nnx.Module):\n    \"\"\"\n    Sequence classifier that builds on a pre‑trained ELMo backbone.\n\n    The network follows the classic ELMo‑to‑text‑classification pipeline:\n\n    1. **ELMo backbone** – A shared ELMo `ElmoModel` is used to generate\n       contextual embeddings for each token (`char_ids`).\n    2. **Mask‑aware attention pooling** – The token‑wise embeddings are\n       weighted with an attention mechanism that respects the padding mask.\n    3. **MLP classifier** – A two‑layer MLP with dropout and ReLU non‑linearity\n       produces the final logits for *n_classes*.\n\n    Parameters\n    ----------\n    elmo_model : ElmoModel\n        Pre‑trained or fine‑tuned ELMo backbone that exposes two forward\n        stages:\n          * ``forward_backbone(char_ids, deterministic, jax_rng)``\n            returns word‑level embeddings and forward/backward LSTM states.\n          * ``forward_embeddings(char_embs, fwd_states, bwd_states)``\n        The backbone must expose ``common_dim`` – the dimensionality of the ELMo\n        embeddings that the classifier consumes.\n    num_classes : int\n        Number of target classes for the downstream classification task.\n    dropout_rate : float, default 0.1\n        Dropout probability applied before and after the hidden MLP layer.\n    rngs : nnx.Rngs\n        PRNG key(s) used for all random operations within the module\n        (e.g., dropout, linear weight initialisation).\n\n    Attributes\n    ----------\n    backbone : ElmoModel\n        Reference to the ELMo backbone used for feature extraction.\n    dropout : nnx.Dropout\n        Dropout layer applied to the pooled representation and to the hidden\n        MLP output.\n    attn_pool : AttnPool\n        Attention pooling head that weights tokens based on the ELMo output.\n    classifier_hidden : nnx.Linear\n        First linear layer of the classification MLP.\n    classifier : nnx.Linear\n        Final linear layer producing logits of shape ``(B, num_classes)``.\n\n    Forward Pass\n    ------------\n    The module expects a 3‑D integer array of character IDs\n    ``char_ids`` with shape ``(B, T, C)`` where:\n\n    * **B** – batch size\n    * **T** – sequence length (token count for each example)\n    * **C** – number of character embeddings per token\n\n    **Deterministic flag** – When ``deterministic=True`` the dropout\n    layers are disabled (useful for evaluation).  When ``False`` and\n    ``jax_rng`` is provided, the RNG seed is used for stochasticity.\n\n    **Masking** – A binary mask is inferred by checking for rows of all\n    zeros in ``char_ids`` (treated as padding).  The mask is added to the\n    raw attention scores before softmax, ensuring that padded positions\n    receive negligible weight.\n\n    Returns\n    -------\n    logits : jnp.ndarray\n        Unnormalised class scores with shape ``(B, num_classes)``.\n\n\n    The returned ``logits`` can be fed to a standard cross‑entropy loss\n    during training.\n\n    Notes\n    -----\n    * The attention pooling performs **softmax over the sequence dimension**\n      and uses a large positive constant to mask out padding after softmax\n      (effectively treating those positions as having negligible weight),\n      which is a stable and differentiable alternative to masking in the\n      exponent step.\n    * The model relies on the ELMo backbone providing *common_dim*‑dimensional\n      embeddings.  If the backbone uses a different dimensionality, the\n      attributes and the MLP width need adjustment accordingly.\n    \"\"\"\n    def __init__(self, elmo_model: ElmoModel, num_classes: int, dropout_rate: float = 0.1, *, rngs: nnx.Rngs):\n        self.backbone = elmo_model\n        self.dropout = nnx.Dropout(rate=dropout_rate, rngs=rngs)\n        self.attn_pool = AttnPool(self.backbone.common_dim, rngs=rngs)\n        self.classifier_hidden = nnx.Linear(self.backbone.common_dim, self.backbone.common_dim, rngs=rngs)\n        self.classifier = nnx.Linear(self.backbone.common_dim, num_classes, rngs=rngs)\n\n    def __call__(self, char_ids, deterministic: bool = False, jax_rng=None):\n        char_embs, fwd_states, bwd_states = self.backbone.forward_backbone(char_ids, deterministic=deterministic, jax_rng=jax_rng)\n        elmo_embs = self.backbone.forward_embeddings(char_embs, fwd_states, bwd_states)\n\n        mask = jnp.logical_not(jnp.all(char_ids == 0, axis=-1)).astype(jnp.float32)  # [B, T]\n        scores = self.attn_pool.proj(elmo_embs).squeeze(-1)  # Raw scores before softmax\n        scores = scores + (mask - 1) * 1e9  # Mask out pads by adding large positive (since softmax, but invert for min)\n        weights = jax.nn.softmax(scores, axis=1)\n        pooled = jnp.sum(elmo_embs * weights[..., None], axis=1)\n\n        x = self.dropout(pooled, deterministic=deterministic)\n        x = jax.nn.relu(self.classifier_hidden(x))\n        x = self.dropout(x, deterministic=deterministic)\n        logits = self.classifier(x)\n        return logits\n`````````\n:::\n\n\n### Prepare Dataset\n\n::: {#61394944 .cell execution_count=19}\n``````````` {.python .cell-code code-fold=\"true\"}\nhf_ds = load_dataset(\"glue\", \"sst2\")\ntrain_stream = hf_ds[\"train\"]\nval_stream = hf_ds[\"validation\"]\n\n# DataLoader setup\nbatch_size = 256\nseq_len = 64\nword_len = 50\n\ndef encode_batch(texts, vocab, char_to_id, seq_len, word_len):\n    \"\"\"\n    Encode a batch of raw text strings into fixed‑size integer tensors.\n\n    Words and characters that are unseen in the supplied dictionaries are\n    replaced with the special unknown token.  Sequences longer than the\n    requested limits are truncated, while shorter ones are padded with the\n    special padding token.\n\n    Parameters\n    ----------\n    texts : Iterable[str]\n        A sequence (list, tuple, generator, …) of raw text strings.  Each\n        element is split on whitespace to produce a list of words.\n    vocab : Mapping[str, int]\n        Word‑to‑ID dictionary.  Should contain the special tokens ``\"<pad>\"``\n        and ``\"<unk>\"``;  if absent, they are assigned indices 0 and 1\n        implicitly via the mapping fall‑backs used in the function.\n    char_to_id : Mapping[str, int]\n        Character‑to‑ID dictionary.  It must contain ``\"<pad>\"`` and\n        ``\"<unk>\"`` for padding and unknown characters respectively.\n    seq_len : int\n        Maximum number of words per sentence that will be encoded.  All\n        sentences are truncated to this length or padded with the word\n        padding token.\n    word_len : int\n        Maximum number of characters per word that will be encoded.  Words\n        longer than this length are truncated; shorter words are padded\n        with the character padding token.\n\n    Returns\n    -------\n    dict\n        A dictionary with two keys:\n        ``\"word_ids\"`` : numpy.ndarray of shape ``(batch_size, seq_len)``\n        ``\"char_ids\"`` : numpy.ndarray of shape ``(batch_size, seq_len,\n        word_len)``\n\n        * ``word_ids[i, j]`` holds the ID of the *j*-th word in the\n          *i*-th input string;  ``vocab[PAD_WORD]`` if the position is\n          padded.\n        * ``char_ids[i, j, k]`` holds the ID of the *k*-th character of\n          the *j*-th word in the *i*-th input string;  ``char_to_id[PAD_CHAR]``\n          if the position is padded.\n\n    \"\"\"\n    PAD_WORD = \"<pad>\"\n    UNK_WORD = \"<unk>\"\n    PAD_CHAR = \"<pad>\"\n    UNK_CHAR = \"<unk>\"\n\n    batch_size = len(texts)\n    word_ids = np.full((batch_size, seq_len), vocab.get(PAD_WORD), dtype=np.int32)\n    char_ids = np.full((batch_size, seq_len, word_len), char_to_id[PAD_CHAR], dtype=np.int32)\n\n    for i, text in enumerate(texts):\n        toks = text.split()[:seq_len]\n        # Encode word IDs\n        wid = [vocab.get(w, vocab.get(UNK_WORD)) for w in toks]\n        word_ids[i, :len(wid)] = wid\n        # Encode char IDs\n        for j, w in enumerate(toks):\n            cids = [char_to_id.get(c, char_to_id[UNK_CHAR]) for c in w[:word_len]]\n            char_ids[i, j, :len(cids)] = cids\n\n    return {\"word_ids\": word_ids, \"char_ids\": char_ids}\n\n\ndef sst2_loader(train_ds, vocab, char_to_id, seq_len, word_len, batch_size):\n    \"\"\"\n    Yield batched, encoded SST‑2 dataset suitable for JAX/Flax training.\n\n    The function takes a HuggingFace ``datasets.Dataset`` containing the\n    Stanford Sentiment Treebank v2 (SST‑2) data, encodes the textual\n    component into word‑ and character‑ids, and yields a Python generator\n    that returns a dictionary of JAX arrays for each mini‑batch.\n\n    Parameters\n    ----------\n    train_ds : :class:`datasets.Dataset`\n        A HuggingFace ``Dataset`` object that must contain at least two\n        columns:\n        ``\"sentence\"`` – raw text data (a list of strings)\n        ``\"label\"``   – integer labels (0: negative, 1: positive)\n    vocab : Mapping[str, int]\n        Word‑to‑ID vocabulary.  Must contain the special tokens\n        ``\"<pad>\"`` and ``\"<unk>\"`` used by :func:`encode_batch`.\n    char_to_id : Mapping[str, int]\n        Character‑to‑ID mapping.  Must contain the special tokens\n        ``\"<pad>\"`` and ``\"<unk>\"``.\n    seq_len : int\n        Maximum number of words per sentence.  Sentences longer than this\n        limit will be truncated; shorter ones padded to ``seq_len``.\n    word_len : int\n        Maximum number of characters per word.  Characters longer than\n        this limit are truncated; shorter ones padded to ``word_len``.\n    batch_size : int\n        Number of examples per yielded batch.\n\n    Yields\n    ------\n    dict\n        A dictionary with the following JAX array entries (dtype\n        ``jnp.int32``):\n        ``\"char_ids\"`` : shape ``(batch_size, seq_len, word_len)``\n        ``\"word_ids\"`` : shape ``(batch_size, seq_len)``\n        ``\"labels\"``  : shape ``(batch_size,)``\n\n    \"\"\"\n    ds = train_ds.shuffle(seed=0)\n    for i in range(0, len(ds), batch_size):\n        batch = ds[i:i + batch_size]\n\n        # Each field is a list\n        texts = batch[\"sentence\"]\n        labels = batch[\"label\"]\n\n        # Encode text to char IDs\n        enc = encode_batch(texts, vocab, char_to_id, seq_len, word_len)\n\n        yield {\n            \"char_ids\": jnp.array(enc[\"char_ids\"]),\n            \"word_ids\": jnp.array(enc[\"word_ids\"]),\n            \"labels\": jnp.array(labels, dtype=jnp.int32),\n        }\n\ntrain_loader = sst2_loader(\n    train_stream,\n    vocab=vocab,\n    char_to_id=char_to_id,\n    seq_len=seq_len,\n    word_len=word_len,\n    batch_size=batch_size,\n)\n```````````\n:::\n\n\n### Define Training Loop\n\n::: {#21ab661d .cell execution_count=20}\n``````````` {.python .cell-code code-fold=\"true\"}\n@nnx.jit\ndef train_step(model_opt, batch, rng):\n    \"\"\"\n    Perform one training step for a JAX/NNX model using a custom optimizer.\n\n    Parameters\n    ----------\n    model_opt : OptimizerWrapper\n        A lightweight optimizer object that holds the current model\n        parameters \n\n    batch : dict[str, jnp.ndarray]\n        A batch dictionary produced by :func:`sst2_loader`.\n\n    rng : jax.random.PRNGKey\n        Random key forwarded to the model to support stochastic\n        operations (dropout, random augmentation, etc.).\n\n    Returns\n    -------\n    tuple\n        ``(model_opt, loss, acc, grads)``\n\n        * ``model_opt`` – the :class:`OptimizerWrapper` after applying\n          the gradient update.\n        * ``loss`` (float)  – the mean soft‑max cross‑entropy\n          computed on the current batch.\n        * ``acc`` (float)   – accuracy of the model on this batch.\n        * ``grads``         – a PyTree of gradients with the same\n          structure as ``model_opt.model``.\n\n    Notes\n    -----\n    * **DiffState usage** – The function creates a\n      :class:`nnx.DiffState` object, ``diff_state``.  This token is\n      passed to ``nnx.value_and_grad`` through ``argnums`` to\n      ensure that only the part of the model wrapped by\n      ``DiffState`` receives gradients.  The model should be\n      built accordingly\n    * **Deterministic flag** – The call to\n      ``model(..., deterministic=False)`` turns off deterministic\n      behaviour (e.g. dropout) during the training step.  The\n      corresponding evaluation function should set\n      ``deterministic=True`` instead.\n\n    \"\"\"\n\n    # DiffState must match optimizer wrt argument\n    diff_state = nnx.DiffState(0, trainable_phase1)\n\n    def loss_fn(model):\n        logits = model(batch[\"char_ids\"], deterministic=False, jax_rng=rng)\n        loss = optax.softmax_cross_entropy_with_integer_labels(\n            logits, batch[\"labels\"]\n        ).mean()\n        return loss, logits\n\n    (loss, logits), grads = nnx.value_and_grad(\n        loss_fn,\n        has_aux=True,\n        argnums=diff_state,\n    )(model_opt.model)\n\n    model_opt.update(grads)\n\n    preds = jnp.argmax(logits, axis=-1)\n    acc = jnp.mean(preds == batch[\"labels\"])\n\n    return model_opt, loss, acc, grads\n\n\nclassifier_random = ElmoClassifier(\n    model_random,\n    num_classes=2,\n    dropout_rate=0.3,\n    rngs=nnx.Rngs(jax.random.PRNGKey(1))\n)\n\nclassifier_trained = ElmoClassifier(\n    model_trained,\n    num_classes=2,\n    dropout_rate=0.3,\n    rngs=nnx.Rngs(jax.random.PRNGKey(1))\n)\n```````````\n:::\n\n\n### Phased Fine-Tuning\n\n::: {#b4b49218 .cell execution_count=21}\n``` {.python .cell-code}\n# Fine-tuning phase 1 updates the parameters of the classifier and the scalar mix parameters in the backbone\ntrainable_phase1 = nnx.All(\n    nnx.Param,\n    nnx.Any(\n        nnx.PathContains(\"layer_projections\"),\n        nnx.PathContains(\"scalar_weights\"),\n        nnx.PathContains(\"gamma\"),\n        nnx.PathContains(\"attn_pool\"),\n        nnx.PathContains(\"classifier\"),\n    )\n)\n\n# Fine-tuning phase 2 unfreezes the biltsm layers in the backbone\ntrainable_phase2 = nnx.All(\n    nnx.Param,\n    nnx.Any(\n        nnx.PathContains(\"bilstm\"),\n        nnx.PathContains(\"layer_projections\"),\n        nnx.PathContains(\"scalar_weights\"),\n        nnx.PathContains(\"gamma\"),\n        nnx.PathContains(\"attn_pool\"),\n        nnx.PathContains(\"classifier\"),\n    )\n)\n```\n:::\n\n\n::: {#fd4e521a .cell execution_count=22}\n``` {.python .cell-code code-fold=\"true\"}\ntx = optax.adamw(1e-3, weight_decay=1e-2)\n\nmodel_opt_random = nnx.ModelAndOptimizer(\n    classifier_random,\n    tx,\n    wrt=trainable_phase1\n)\n\nmodel_opt_trained = nnx.ModelAndOptimizer(\n    classifier_trained,\n    tx,\n    wrt=trainable_phase1\n)\n```\n:::\n\n\n::: {#4d1a3290 .cell execution_count=23}\n``` {.python .cell-code code-fold=\"true\"}\nnum_epochs = 10\ntrain_rng = jax.random.PRNGKey(0)\n\nfor epoch in range(num_epochs):\n    print(f\"\\n===== Epoch {epoch + 1}/{num_epochs} =====\")\n\n    # reinitialize or reshuffle dataset each epoch\n    train_loader = sst2_loader(\n        train_stream, vocab, char_to_id, seq_len, word_len, batch_size\n    )\n\n    epoch_loss_random = []\n    epoch_loss_trained = []\n    epoch_acc_random = []\n    epoch_acc_trained = []\n\n    for batch in train_loader:\n        \n        train_rng, subkey = jax.random.split(train_rng)\n        model_opt_random, loss_random, acc_random, grads_random = train_step(model_opt_random, batch, rng=subkey)\n        model_opt_trained, loss_trained, acc_trained, grads_trained = train_step(model_opt_trained, batch, rng=subkey)\n        \n        epoch_loss_random.append(float(loss_random))\n        epoch_acc_random.append(float(acc_random))\n        epoch_loss_trained.append(float(loss_trained))\n        epoch_acc_trained.append(float(acc_trained))\n        \n\n    print(f\"Epoch {epoch + 1} | loss={np.mean(epoch_loss_random):.4f} | acc={np.mean(epoch_acc_random):.4f}\")\n    print(f\"Epoch {epoch + 1} | loss={np.mean(epoch_loss_trained):.4f} | acc={np.mean(epoch_acc_trained):.4f}\")\n```\n:::\n\n\n::: {#0c6d9bb0 .cell execution_count=24}\n``` {.python .cell-code code-fold=\"true\"}\n@nnx.jit\ndef train_step(model_opt, batch, rng):\n    # diff_state must match the 'wrt' you used for model_opt\n    diff_state = nnx.DiffState(0, trainable_phase2)\n\n    def loss_fn(model):\n        logits = model(batch[\"char_ids\"], deterministic=False, jax_rng=rng)\n        loss = optax.softmax_cross_entropy_with_integer_labels(logits, batch[\"labels\"]).mean()\n        return loss, logits\n\n    (loss, logits), grads = nnx.value_and_grad(loss_fn, has_aux=True, argnums=diff_state)(model_opt.model)\n\n    model_opt.update(grads)\n\n    preds = jnp.argmax(logits, axis=-1)\n    acc = jnp.mean(preds == batch[\"labels\"])\n    return model_opt, loss, acc, grads\n\ntx = optax.adamw(1e-3, weight_decay=1e-2)\n\nmodel_opt = nnx.ModelAndOptimizer(\n    model_opt_trained.model,\n    tx,\n    wrt=trainable_phase2\n)\n```\n:::\n\n\n::: {#6dc745ad .cell execution_count=25}\n``` {.python .cell-code code-fold=\"true\"}\nnum_epochs = 5\ntrain_rng = jax.random.PRNGKey(0)\n\nfor epoch in range(num_epochs):\n    print(f\"\\n===== Epoch {epoch + 1}/{num_epochs} =====\")\n\n    # reinitialize dataset each epoch\n    train_loader = sst2_loader(\n        train_stream, vocab, char_to_id, seq_len, word_len, batch_size\n    )\n\n    epoch_loss = []\n    epoch_acc = []\n\n    for batch in train_loader:\n        train_rng, subkey = jax.random.split(train_rng)\n        model_opt, loss, acc, grads = train_step(model_opt, batch, rng=subkey)\n        epoch_loss.append(float(loss))\n        epoch_acc.append(float(acc))\n\n    print(f\"Epoch {epoch + 1} | loss={np.mean(epoch_loss):.4f} | acc={np.mean(epoch_acc):.4f}\")\n```\n:::\n\n\n::: {#5bca3c1b .cell execution_count=26}\n``` {.python .cell-code code-fold=\"true\"}\nval_loader = sst2_loader(\n    val_stream,\n    vocab=vocab,\n    char_to_id=char_to_id,\n    seq_len=seq_len,\n    word_len=word_len,\n    batch_size=batch_size,\n)\n\nacc = []\nfor batch in val_loader:\n    raw_preds = model_opt.model(batch[\"char_ids\"], deterministic=True)\n    preds = np.argmax(raw_preds, axis=1)\n    batch_acc = np.sum(batch[\"labels\"] == preds) / preds.shape[0]\n    acc.append(batch_acc)\n\n```\n:::\n\n\n# Conclusion\n\nThis was a long one! Good job making it all the way to the end. In this post we reviewed the differences beween static and contextual embeddings, we delved into the math and architecture of recurrent neural networks, and we implemented ELMo practically from scratch in JAX. In addition to all of that, we also then fine-tined ELMo to perform text classification and we showed the difference in having pre-trained embeddings versus random weights. \n\n# Coming Next\n\nIn the next post of this series, we will go over the transformer architecture. I hope to see you there!\n\n# References\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}