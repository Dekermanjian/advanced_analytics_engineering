[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Welcome to my blog!\nMy name is Jonathan Dekermanjian and I am a data professional with nearly 10 years of experience. I have worked both in academia as a data science researcher as well as in industry as a data scientist & machine learning engineer.\nIf you’d like to learn more about me please click on the LinkedIn link below!\nThe purpose of this blog is to talk about interesting advanced analytics and engineering topics that will benefit others who are also data professionals."
  },
  {
    "objectID": "posts/kedro-framework/index.html",
    "href": "posts/kedro-framework/index.html",
    "title": "Reproducibility & Scalability Part 1The Kedro Framework",
    "section": "",
    "text": "Overview\nIn part one of a four-part series, we are going to set up and walkthrough the Kedro framework. Kedro is an open-source Python framework that provides you with scaffolding to ensure your data projects adhere to software-engineering best practices. In turn, this makes your project reproducible and robust to requirement changes.\n\n\nIntroduction\nIn this post we are going to talk about and walkthrough using the Kedro framework. To keep things simple, we are going to use a clean dataset provided by the National Oceanic and Atmospheric Association (NOAA) that measures the monthly temperature changes across twelve Brazilian cities. You can download the dataset off of Kaggle. We are going to keep things as simple as possible and only go over the key components of a Kedro project which include:\n\nCreating a Kedro project\nDefining a data catalog\nCreating a Kedro pipeline\nIntegrating Kedro plugins\n\nIf you want to learn how to use Kedro’s more advanced features check out the official documention.\n\n\nGetting Started\nLet’s start off by creating a clean environment. We are going to use Conda in this example but you can you whatever virtual environment you prefer. After installing Anaconda or Miniconda, you can create a new environment by executing the following command in your terminal:\n# Creates a new virtual environment\nconda create -c conda-forge -n my_new_environment python=3.12 -y\nThe argument -c conda-forge tells conda to put the conda-forge repository at a higher priority over the default repository. We also installed python version 3.12 in this new environment. Now that we have a clean environment we need to install Kedro into that environment. Activate your new environment and install Kedro with:\n# Installs kedro\npip install -U kedro==0.19.11\nYou can verify your installation by executing:\n# Displays installed kedro version and extensions\nkedro info\n\n\nCreating a New Kedro Project\nNow that we have Kedro installed, we need to create a new Kedro project. You can use the Kedro command-line interface (CLI):\n# Creates a new Kedro project\nkedro new\nYou will be prompted to pick a name for your project. Let’s call our project Climate-Brazil. Next, you will be prompted to select the Kedro tools you want to utilize. For this project, let’s go with 1, 3, 5, 7 which corresponds to Linting, Logging, Data Folder, and Kedro-Viz. Once you have created your project, you’ll notice that Kedro created a project directory, using the project name we selected, with subdirectories and files that serve as the scaffolding of your project.\n\n\n\n\n\n\nNote\n\n\n\nNote that there are some files in the image below that you won’t have at this moment. These will be added/generated as you continue through this walkthrough.\n\n\n\n\n\nKedro Project Structure\n\n\n\n\nKedro Project Structure\nLet’s briefly walkthrough the 5 subdirectories that Kedro created for us.\n\nconf: This directory contains your project configurations. These can be things from API/Database credentials to Model parameters. This is also where you can create multiple project environments like Dev and Prod.\ndata: This directory is where you will store your data and your project artifacts.\nnotebooks: This directory is where you can interactively develop your project before creating modular nodes and pipelines\nsrc: This directory is where you will put your source code defined as nodes and chained together into pipelines\ntests: This directory is where you will write your unit tests\n\nI know that was really brief but don’t worry we will learn more specifics as we develop the project.\n\n\nDeclare Project Dependencies\nNow that we have a project created, let’s add any packages that our project will depend on. In the root of our project directory Kedro created a requirements.txt file for us. Kedro will add some dependencies into the file by default but we will need some additional packages for our project. We are going to need polars, plotly, and pre-commit so let’s modify requirements.txt to contain the following:\n# ./requirements.txt\nipython&gt;=8.10\njupyterlab&gt;=3.0\nkedro-datasets&gt;=3.0\nkedro-viz&gt;=6.7.0\nkedro[jupyter]~=0.19.11\nnotebook\npre-commit~=3.8.0\npolars~=1.22.0\npyarrow~=19.0.1\nplotly~=5.24.1\nopenpyxl~=3.1.5\nNow that we have declared our dependencies, let’s go ahead and install them into our environment.\npip install -r requirements.txt\n\n\nDefining a Data Catalog\nAll Input/Output (IO) operations are defined in a data catalog, when using Kedro. This allows you to declaratively define your data sources whether they are stored as local files, or stored remotely in a SQL/NoSQL database, or just in some form of remote storage like S3.\n\n\n\n\n\n\nTip\n\n\n\nIn our simple example, all the data are stored locally. Therefore, we don’t need to define any credentials to access the data. However, in practice it is likely that you need to access data from a database or cloud storage. In that case, you would define your credentials in /conf/local/credentials.yml.\n\n\nBefore we edit our catalog, go ahead and download the data to /data/01_raw/. You can get the data files from Kaggle.\nWe declare our base catalog by modifying /conf/base/catalog.yml. The base catalog will be inhereted by all other Kedro enviornments you create. For example, if you create a dev environment you don’t need to repeat the catalog entries that are in /conf/base/catalog.yml inside of /conf/dev/catalog.yml.\n\n\n\n\n\n\nCaution\n\n\n\nIf you declare a catalog entry in /conf/dev/catalog.yml that shares the same key as an entry in /conf/base/catalog.yml you will override the base catalog when you are working in the dev environment.\n\n\nThere are several ways that we can add our data to the catalog. The simplest way is to define one entry for each Brazilian city:\n#/conf/base/catalog.yml\nbelem_dataset:\n  type: polars.LazyPolarsDataset\n  file_format: csv\n  filepath: data/01_raw/station_belem.csv\n\ncuritiba_dataset:\n  type: polars.LazyPolarsDataset\n  file_format: csv\n  filepath: data/01_raw/station_curitiba.csv\n\n# Rest of the cities follow-suit\nIf you have a lot of cities it can get really tedious. Luckily, Kedro has a feature called Dataset Factories that assist you in reducing repeated code in your data catalog. We can declare all of our cities with one block like this:\n#/conf/base/catalog.yml\n# Dataset Factory\n\"{city}_dataset\":\n  type: polars.LazyPolarsDataset\n  file_format: csv\n  filepath: data/01_raw/station_{city}.csv\nKedro is smart enough that during runtime it will use Regex to match {city} to the corresponding dataset city. Finally, the approach we will use is to declare the data as a PartitionedDataset. You can think of each Brazilian city as a partition of the entire dataset that would be composed of all the Brazilian cities. Kedro will return a dictionary object with each file’s name as the key and the load method as the corresponding value.\n\n\n\n\n\n\nImportant\n\n\n\nNote that when using a PartitionedDataset the data is loaded lazily. This means that the data is actually not loaded until you call the load method. This prevents Out-Of-Memory (OOM) errors if your data can’t fit into memory.\n\n\nWe can declare our data as a PartitionedDataset like this:\n# /conf/base/catalog.yml\ncities_temperatures:\n  type: partitions.PartitionedDataset\n  path: data/01_raw/\n  dataset: \n    type: polars.LazyPolarsDataset\n    file_format: csv\nNow we can load our dataset by simply starting up a Kedro session (Kedro does this for you) and calling catalog.load(\"cities_temperatures\"). You can even work with the data interactively in a jupyter notebook. Kedro automatically loads the session and context when you run kedro jupyter lab on the command-line. This means that once the jupyter kernel is running you already have your catalog object loaded in your environment.\n\n\nCreating a New Kedro Pipeline\nTypically, once you have defined your data catalog you’d want to work interactively on your cleaning and wrangling your data into a usable format. As mentioned earlier you can do that using kedor jupyter lab. Once you have your processing/modeling/reporting code in check you should write modular functions that when chained together would constitute a pipeline. That could be a data processing pipeline, a machine learning pipeline, a monitoring pipeline, etc… This is where Kedro pipelines come in to play. We can create a new pipeline using the Kedro CLI by executing:\nkedro pipeline create data_processing\nKedro will create the structure for your new pipeline in /src/climate_brazil/pipelines/data_processing. We will define all your modular code inside of /src/climate_brazil/pipelines/data_processing/nodes.py and then we will declare our pipeline inside /src/climate_brazil/pipelines/data_processing/pipeline.py. Let’s start with the nodes.\nEach of our Brazilian cities dataset is in a wide format where we have years across the rows and the months within that year across the columns. We need to merge all of our cities together and unpivot the data so that we are in long format where both year and month are across the rows. The following function does what we just described:\n# /src/climate_brazil/pipelines/data_processing/nodes.py\ndef process_datasets(partitioned_dataset: dict) -&gt; pl.DataFrame:\n    \"\"\"\n    Combine all cities into one dataframe and unpivot the data into long format\n    ---\n    params:\n        partitioned_dataset: Our data partiotioned into key (filename) value(load method) pairs\n    \"\"\"\n    # Missing values encoding\n    mv_code = 999.9\n\n    # Add a city column to each partiotion so that when we merge them all together we can identify each city\n    datasets = [\n        v().with_columns(city=pl.lit(re.findall(r\"(?&lt;=_).*(?=\\.)\", k)[0]))\n        for k, v in partitioned_dataset.items()\n    ]\n\n    df_merged = pl.concat(datasets)\n\n    df_processed = (\n        df_merged.drop(\"D-J-F\", \"M-A-M\", \"J-J-A\", \"S-O-N\", \"metANN\")\n        .rename({\"YEAR\": \"year\"})\n        .collect()  # Need to collect because can't unpivot a lazyframe\n        .unpivot(\n            on=[\n                \"JAN\",\n                \"FEB\",\n                \"MAR\",\n                \"APR\",\n                \"MAY\",\n                \"JUN\",\n                \"JUL\",\n                \"AUG\",\n                \"SEP\",\n                \"OCT\",\n                \"NOV\",\n                \"DEC\",\n            ],\n            index=[\"city\", \"year\"],\n            variable_name=\"month\",\n            value_name=\"temperature\",\n        )\n        .with_columns(\n            pl.col(\"month\")\n            .str.to_titlecase()\n            .str.strptime(dtype=pl.Date, format=\"%b\")\n            .dt.month()\n        )\n        .with_columns(\n            date=pl.date(year=pl.col(\"year\"), month=pl.col(\"month\"), day=1),\n        )\n        .with_columns(\n            pl.when(\n                pl.col(\"temperature\")\n                == mv_code  # This is how missing data is coded in the dataset\n            )\n            .then(None)\n            .otherwise(pl.col(\"temperature\"))\n            .name.keep(),\n            pl.col(\"city\").str.to_titlecase(),\n        )\n        .drop(\"year\", \"month\")\n    )\n    return df_processed\nLet’s also define a function that will plot our time series data.\ndef timeseries_plot(processed_dataframe: pl.DataFrame) -&gt; go.Figure:\n    \"\"\"\n    Plots each Brazilian city temperature time series\n    \"\"\"\n    fig = go.Figure()\n    for city in processed_dataframe.select(\"city\").unique(maintain_order=True).to_series():\n        fig.add_trace(\n            go.Scatter(\n                x = processed_dataframe.filter(pl.col(\"city\")==city).sort('date')['date'],\n                y = processed_dataframe.filter(pl.col(\"city\")==city).sort('date')['temperature'],\n                name = city,\n                hovertemplate=\"&lt;b&gt;Date&lt;/b&gt;: %{x}&lt;br&gt;&lt;b&gt;Temperature&lt;/b&gt;: %{y}\"\n            )\n        )\n    fig.update_layout(\n        title = \"Temperature Measurements of Brazilian Cities\",\n        xaxis=dict(\n        title = \"Date\",\n        rangeselector=dict(\n            buttons=list([\n                dict(count=1,\n                     label=\"1y\",\n                     step=\"year\",\n                     stepmode=\"backward\"),\n                dict(count=5,\n                     label=\"5y\",\n                     step=\"year\",\n                     stepmode=\"backward\"),\n                dict(count=10,\n                     label=\"10y\",\n                     step=\"year\",\n                     stepmode=\"backward\"),\n                dict(step=\"all\", label=\"All\")\n            ])\n        ),\n        rangeslider=dict(\n            visible=True\n        ),\n        type=\"date\",\n        rangeselector_font_color='black',\n        rangeselector_activecolor='hotpink',\n        rangeselector_bgcolor='lightblue',\n    ),\n        yaxis=dict(\n            title = \"Temperature in Celsius\"\n        )\n    )\n    return fig\nThis will produce the following figure:\n\n\n                                                \n\n\n\n\n                                                \n\n\nNow that we have defined our nodes, let’s see how we can chain them together into a simple pipeline. We define our data processing pipeline in the pipeline.py file that Kedro creates for us in the data_processing directory.\n# /src/climate_brazil/pipelines/data_processing/pipeline.py\nfrom kedro.pipeline import node, Pipeline, pipeline  # noqa\nfrom .nodes import process_datasets, timeseries_plot\n\ndef create_pipeline(**kwargs) -&gt; Pipeline:\n    return pipeline([\n        node(\n            func=process_datasets,\n            inputs=dict(partitioned_dataset = \"cities_temperatures\"),\n            outputs=\"cities_temperatures_processed\",\n            name=\"process_datasets\"\n        ),\n        node(\n            func=timeseries_plot,\n            inputs=dict(processed_dataframe = \"cities_temperatures_processed\"),\n            outputs=\"cities_ts_plot\",\n            name=\"timeseries_plot\"\n        ),\n    ])\nNotice that the input to the process_datasets function is the name that we chose for our dataset when we defined it inside of our catalog. Also note that we are choosing to name the output from the process_datasets function as cities_temperatures_processed and we are passing that as the input to the function timeseries_plot.\n\n\n\n\n\n\nCaution\n\n\n\nKedro automatically infers the dependencies of your pipeline and will run it in an order that may not be the same order as you defined.\n\n\nBefore we move on to the next section, it is important to know that both our outputs will be MemoryDatasets this mean that they will only exist while the Kedro session is still active. In order to persist the outputs we need to add them to our catalog.\n# conf/base/catalog.yml\n# Raw data ---------------------------------------\ncities_temperatures:\n  type: partitions.PartitionedDataset\n  path: data/01_raw/\n  dataset: \n    type: polars.LazyPolarsDataset\n    file_format: csv\n\n# Processed data ---------------------------------------\ncities_temperatures_processed:\n  type: polars.EagerPolarsDataset\n  file_format: parquet\n  filepath: data/02_intermediate/cities_temperatures_processed.pq\n\ncities_ts_plot:\n  type: plotly.JSONDataset\n  filepath: data/08_reporting/cities_ts_plot.json\nWe are going to store our processed data locally as a parquet file and our figure as a json file. Great, we are now ready to run our pipeline!\n\n\nRunning a Kedro Pipeline\nTo run the data processing pipeline we just defined you can execute:\nkedro run\nThat will run all your defined pipelines in the order that you defined them and since we only have one pipeline you will run the data processing pipeline. Okay, so how do we run a specific pipeline? For that you can use:\nkedro run --pipeline data_processing\nKedro also allows you to run specific nodes. For example, if we just wanted to process the data without generating a plot we could run:\nkedro run --nodes process_datasets\nWhere process_datasets is the name we gave the node when we defined the pipeline.\nKedro is very flexible and allows you to run in a variety of options. You can checkout the Kedro run documentation for more information.\n\n\nPipeline Graphs with Kedro-Viz\nWhen we created a new Kedro project earlier, we told kedro to add to our dependencies the kedro-viz plugin. This plugin allows you to visualize your pipeline or pipelines as a graph. To view the simple pipeline we built earlier you can execute at the command-line the following:\nkedro viz run\nYour browser should automatically launch and you will be greeted by the following screen.\n\n\n\nKedro-Viz Pipeline Graph\n\n\nOur simple example doesn’t do Kedro-Viz justice. In practice, you’ll have many datasets coming from disparate sources that will require more complex processing and joining. In such a scenario, being able to see the lineage and dependencies of your data becomes very useful. Kedro-Viz is interactive in that you can optionally preview the data, you can view static or interactive figures, and you can view the code of the nodes all in the user interface. I recommend that you try this plugin for you next project!\n\n\nSource Control & Pre-Commit\nI am sure that everyone reading this already understands the importance of source/version control, so I will keep this breif. When we created our project Kedro was nice enough to create a .gitignore file and .gitkeep files. The .gitignore file makes sure that you don’t accidentally commit any data or any credentials that you store in conf/local/credentials.yml to a remote repository. Kedro does not, unfortunately, set up any pre-commit configuration so you need to do that manually. Here is an example of a pre-commit configuration that includes linting with ruff checking your codebase for missing docstrings with interrogate and stripping out any confidentiat data from notebooks with nbstripout.\n# .pre-commit-config.yaml\nrepos:\n  - repo: https://github.com/astral-sh/ruff-pre-commit\n    # Ruff version.\n    rev: v0.11.2\n    hooks:\n      # Run the linter.\n      - id: ruff\n        types_or: [ python, pyi ]\n        args: [ --fix ]\n      # Run the formatter.\n      - id: ruff-format\n        types_or: [ python, pyi ]\n  - repo: https://github.com/econchick/interrogate\n    rev: 1.7.0\n    hooks:\n    - id: interrogate\n      args: [--config=pyproject.toml]\n      pass_filenames: false\n  - repo: https://github.com/kynan/nbstripout\n    rev: 0.7.1\n    hooks:\n      - id: nbstripout\nYou also need to specify interrogate’s configuration within pyproject.toml you can add the following at the bottom of your file:\n[tool.interrogate]\nignore-init-method = false\nignore-init-module = false\nignore-magic = false\nignore-semiprivate = false\nignore-private = false\nignore-property-decorators = false\nignore-module = false\nignore-nested-functions = false\nignore-nested-classes = false\nignore-setters = false\nignore-overloaded-functions = false\nfail-under = 80\nexclude = [\"tests\", \"docs\", \"build\", \"src/climate_brazil/__main__.py\"]\next = []\nverbose = 2\nquiet = false\nwhitelist-regex = []\ncolor = true\nomit-covered-files = false\nAfter you’re done setting up your configuration you need to initialize a local git repository and install your pre-commit configurations.\ngit init\npre-commit install\nWonderul, we are all set now with source control and mitigating the possibility of commiting anything confidential to your repository.\n\n\nRunning Pipelines in Containers\nWe can also run the pipeline we just built inside of a container. Kedro maintains the kedro-docker plugin which facilitates getting your Kedro project running inside a container.\n\n\n\n\n\n\nNote\n\n\n\nWhile the plugin is named kedro-docker you can use it with other containerization frameworks such as Podman\n\n\nFirst, we need to install the plugin. You can add the following to your ./requirements.txt file:\nkedro-docker~=0.6.2\nThen execute:\npip install kedro-docker~=0.6.2\nWith the plugin installed, Kedro will generate a Dockerfile, .dockerignore, and .dive-ci that corresponds to your Kedro project by executing:\nkedro docker init\n\n\n\n\n\n\nCaution\n\n\n\nMake sure you Docker Engine is running otherwise the previous step will fail.\n\n\nLet’s take a look at the generated Dockerfile:\n#./Dockerfile\nARG BASE_IMAGE=python:3.9-slim\nFROM $BASE_IMAGE as runtime-environment\n\n# update pip and install uv\nRUN python -m pip install -U \"pip&gt;=21.2\"\nRUN pip install uv\n\n# install project requirements\nCOPY requirements.txt /tmp/requirements.txt\nRUN uv pip install --system --no-cache-dir -r /tmp/requirements.txt && rm -f /tmp/requirements.txt\n\n# add kedro user\nARG KEDRO_UID=999\nARG KEDRO_GID=0\nRUN groupadd -f -g ${KEDRO_GID} kedro_group && \\\n    useradd -m -d /home/kedro_docker -s /bin/bash -g ${KEDRO_GID} -u ${KEDRO_UID} kedro_docker\n\nWORKDIR /home/kedro_docker\nUSER kedro_docker\n\nFROM runtime-environment\n\n# copy the whole project except what is in .dockerignore\nARG KEDRO_UID=999\nARG KEDRO_GID=0\nCOPY --chown=${KEDRO_UID}:${KEDRO_GID} . .\n\nEXPOSE 8888\n\nCMD [\"kedro\", \"run\"]\nKedro will automatically assume that you want to run all your pipelines (or your defualt pipeline) but you can, quite easily, change the specified command in the generated docker file.\nNow that we have a Dockerfile and .dockerignore we can build our image.\nkedro docker build\nFinally, you can run your pipeline from within the container by either using your normal docker-cli commands or you can use the kedro-docker plugin and execute:\nkedro docker run\n\n\nSummary\nIn this post we walked through using the Kedro framework and learned how to follow software engineering best practices that ensure that your projects are reproducible, modular, and easy to maintain as a project matures. We learned about the data catalog, how to define nodes and subsequently linking nodes together to build pipelines. We then looked at a couple Kedro plugins like kedro-viz and kedro-docker that expanded the functionality of our project. We also talked about and walked through good practices to follow when implementing source control with git and pre-commit. All this in just part one of our series! We have a long ways to go still but I hope you are excited for what comes next.\n\n\nComing Next\nIf you use a popular/mainstream machine learning framework like PyTorch, TensorFlow, Scikit-Learn, or XGBoost then reproducibility and scalability are quite easy because you’ll typically find that most MLOPs and distributed frameworks natively support these tools. What do you do if you have a custom solution? Let’s say you are using a probabilistics programming language like Stan or PyMC and there isn’t native support for these tools. Well, that is what we are going to do in the following parts of this series. In part two we will fit a time series model using PyMC and talk about how to use ModelBuilder from pymc-extras to build a production ready model. I hope to see you there!"
  },
  {
    "objectID": "posts/kedro-pymc-modelbuilder/index.html",
    "href": "posts/kedro-pymc-modelbuilder/index.html",
    "title": "Reproducibility & Scalability Part 2PyMC ModelBuilder",
    "section": "",
    "text": "In part two of this series, we go over the necessary steps to take a time-series model into production by integrating the PyMC ModelBuilder class with Kedro."
  },
  {
    "objectID": "posts/kedro-pymc-modelbuilder/index.html#train-test-split",
    "href": "posts/kedro-pymc-modelbuilder/index.html#train-test-split",
    "title": "Reproducibility & Scalability Part 2PyMC ModelBuilder",
    "section": "Train-Test Split",
    "text": "Train-Test Split\nLet’s define how we are going split our dataset so that we can evaluate our model’s performance later. If you recall, we are dealing with time-series data across twelve Brazilian cities. Our interest is in forecasting the average monthly temperatures of these cities. Looking at the data you will notice that different cities started collecting data at different time points but the last measurement of October 2019 is shared by all cities (even if the measurement is missing). So, let’s hold out one year of data (12 measurements) for each city to be our test set.\n# /src/climate_brazil/pipelines/ML/nodes.py\ndef train_test_split(data: pl.DataFrame, testing_window: int) -&gt; tuple[pl.DataFrame, pl.DataFrame]:\n    \"\"\"\n    Splits time-series dataset into test and train splits\n    ---\n    Params:\n        data: The time-series dataset\n        testing_window: The size of the testing set\n    \"\"\"\n\n    testing_dataset = (\n        data\n        .sort(\"city\", \"date\")\n        .group_by(\"city\")\n        .agg(\n            pl.all().tail(testing_window)\n        )\n        .explode(\"temperature\", \"date\")\n        .sort(\"city\", \"date\")\n    )\n\n    training_dataset = (\n        data\n        .sort(\"city\", \"date\")\n        .group_by(\"city\")\n        .agg(\n            pl.all().slice(0, pl.len() - testing_window)\n        )\n        .explode(\"temperature\", \"date\")\n        .sort(\"city\", \"date\")\n    )\n\n    return training_dataset, testing_dataset\nOur train_test_split() function takes a parameter defining the testing set window size. This is a configuration that you may want to change in the future. Kedro generates configuration files for every pipeline that you created and that is where we will put all of our configuration parameters. Let’s define our testing_window parameter now.\n#/conf/base/parameters_ML.yml\ntesting_window: 12"
  },
  {
    "objectID": "posts/kedro-pymc-modelbuilder/index.html#bayesian-structural-time-series",
    "href": "posts/kedro-pymc-modelbuilder/index.html#bayesian-structural-time-series",
    "title": "Reproducibility & Scalability Part 2PyMC ModelBuilder",
    "section": "Bayesian Structural Time Series",
    "text": "Bayesian Structural Time Series\nWe are going to build a simple Bayesian structural time-series model to forecast the monthly average temperatures. This series is focused on engineering so I won’t delve too deeply into the mathematics behind the model. Briefly, our model is going to have two components, a trend component that we are going to estimate using a one-dimensional Gaussian process and a seasonality component that we are going to estimate as deterministic seasonality using fourier features."
  },
  {
    "objectID": "posts/kedro-pymc-modelbuilder/index.html#defining-the-model",
    "href": "posts/kedro-pymc-modelbuilder/index.html#defining-the-model",
    "title": "Reproducibility & Scalability Part 2PyMC ModelBuilder",
    "section": "Defining The Model",
    "text": "Defining The Model\nWe will represent our model as the following:\n\\[ temperature \\sim N(\\mu, \\sigma) \\] \\[ \\mu = f(x) + \\beta_{fourier}X_{fourier\\_terms} \\] \\[ f(x) \\sim GP(0, K(x, x'; \\eta, \\ell)) \\] \\[ \\beta_{fourier} \\sim N(0, 0.5) \\] \\[ \\eta \\sim HalfNormal(1.0) \\] \\[ \\ell \\sim Gamma(48, 2) \\] \\[ \\sigma \\sim HalfNormal(0.2) \\]\nWhere our Gaussian Process covariance matrix \\(K\\) uses the Exponentiated Quadratic kernel. \\[ K(x, x') = exp[-\\frac{(x - x')^{2}}{2\\ell^{2}}] \\]\nIn PyMC this would look like this:\nwith pm.Model() as model:\n    model.add_coord(\"obs_id\", train_time_range)\n    model.add_coord(\n        \"fourier_features\",\n        np.arange(len(train_fourier_series.to_numpy().T)),\n    )\n\n    t = pm.Data(\"time_range\", train_time_range, dims=\"obs_id\")\n    fourier_terms = pm.Data(\n        \"fourier_terms\", train_fourier_series.to_numpy().T\n    )\n\n    error = pm.HalfNormal(\"error\", 0.2)\n\n    # Trend component\n    amplitude_trend = pm.HalfNormal(\"amplitude_trend\", 1.0)\n    ls_trend = pm.Gamma(\"ls_trend\", alpha=48, beta=2)\n    cov_trend = amplitude_trend * pm.gp.cov.ExpQuad(1, ls_trend)\n    \n    gp_trend = pm.gp.HSGP(\n        m=[10], \n        c=5.,\n        cov_func=cov_trend\n    )\n    trend = gp_trend.prior(\"trend\", X=t[:, None], dims=\"obs_id\")\n\n    # Seasonality components\n    beta_fourier = pm.Normal(\n        \"beta_fourier\", mu=0, sigma=0.5, dims=\"fourier_features\"\n    )\n    seasonality = pm.Deterministic(\n        \"seasonal\", pm.math.dot(beta_fourier, fourier_terms), dims=\"obs_id\"\n    )\n\n    # Combine components\n    mu = trend + seasonality\n\n    pm.Normal(\n        \"temperature\",\n        mu=mu,\n        sigma=error,\n        observed=temperature_normalized,\n        dims=\"obs_id\",\n        )\nAt this point we could pass in the expected data (time_range, fourier_terms, and temperature_normalized) for a city and train the time-series. However, there would be less back-tracking if we go ahead and define our model along with the necessary processing of the inputs, the method for generating forecasts, and the methods for saving and loading our trained model."
  },
  {
    "objectID": "posts/kedro-pymc-modelbuilder/index.html#configuring-modelbuilder",
    "href": "posts/kedro-pymc-modelbuilder/index.html#configuring-modelbuilder",
    "title": "Reproducibility & Scalability Part 2PyMC ModelBuilder",
    "section": "Configuring ModelBuilder",
    "text": "Configuring ModelBuilder\nThe ModelBuilder class from the pymc-extras package gives us a nice scaffolding that facilitates taking PyMC models into production. There are seven required methods that we need to define inside of our model class that inherits from ModelBuilder. However, it is often the case that we need to override some default methods in accordance to our modeling needs. Let’s go over the required methods first.\nWe need to implement _generate_and_preprocess_model_data() which is a method that performs our final processing of the data before it is fed into the model. In our case, we normalize the data and define our fourier features to capture yearly (12 months) seasonality.\n# ModelBuilder Required Method Implementation\ndef _generate_and_preprocess_model_data(self, X: pl.DataFrame, y: pl.Series, normalize: bool = False):\n    \"\"\"\n    Last mile data processing of inputs expected by the model\n    ---\n    Params:\n        X: The matrix of predictor variables expected by our model\n        y: The target variable\n        normalize: Whether to Z normalize the variables\n    \"\"\"\n    self.train_time_range = np.arange(X.shape[0])\n    self.n_modes = 10\n    periods = np.array(self.train_time_range) / (12)\n    self.train_fourier_series = pl.DataFrame(\n        {\n            f\"{func}_order_{order}\": getattr(np, func)(\n                2 * np.pi * periods * order\n            )\n            for order in range(1, self.n_modes + 1)\n            for func in (\"sin\", \"cos\")\n        }\n    )\n    if normalize:\n        self.y_mean = np.nanmean(y)\n        self.y_std = np.nanstd(y)\n        self.y_normalized = (y - self.y_mean) / self.y_std\n    else:\n        self.y_normalized = y\n\n\n\n\n\n\nCaution\n\n\n\nSince we are only including endogenous variables in our model the normalization only applies to the target variable\n\n\nNext we need to define how we are going to feed new data to our model when we are ready to make forecasts. Since we are building a time-series model with no exogenous predictor variables, we only need to define how far out into the future we want to generate forecasts.\n# ModelBuilder Required Method Implementation\ndef _data_setter(self, n_ahead: int):\n    \"\"\"\n    Generates required data for producing forecasts\n    ---\n    Params:\n        n_ahead: How many periods (months) to forecast future temperatures\n    \"\"\"\n    self.start = self.train_time_range[-1]\n    self.end = self.start + n_ahead\n\n    new_periods = np.arange(self.start, self.end, 1) / (12)\n    self.test_fourier_series = pl.DataFrame(\n        {\n            f\"{func}_order_{order}\": getattr(np, func)(\n                2 * np.pi * new_periods * order\n            )\n            for order in range(1, self.n_modes + 1)\n            for func in (\"sin\", \"cos\")\n        }\n    )\nCertainly we will need to define how to build our model by using the build_model() method. You’ll notice that we call our previously defined method _generate_and_preprocess_model_data() here.\n# ModelBuilder Required Method Implementation\ndef build_model(self, X: pl.DataFrame, y: pl.Series, normalize_target: bool = False, **kwargs):\n    \"\"\"\n    Defines the PyMC model structure\n    ---\n    Params:\n        X: Dataframe of features\n        y: Array of target values\n    \"\"\"\n\n    self._generate_and_preprocess_model_data(X, y, normalize=normalize_target)\n\n    with pm.Model() as self.model:\n        self.model.add_coord(\"obs_id\", self.train_time_range)\n        self.model.add_coord(\n            \"fourier_features\",\n            np.arange(len(self.train_fourier_series.to_numpy().T)),\n        )\n\n        t = pm.Data(\"time_range\", self.train_time_range, dims=\"obs_id\")\n        fourier_terms = pm.Data(\n            \"fourier_terms\", self.train_fourier_series.to_numpy().T\n        )\n\n        error = pm.HalfNormal(\"error\", self.model_config['error'])\n\n        # Trend component\n        amplitude_trend = pm.HalfNormal(\"amplitude_trend\", self.model_config['amplitude_trend'])\n        ls_trend = pm.Gamma(\"ls_trend\", alpha=self.model_config['ls_trend']['alpha'], beta=self.model_config['ls_trend']['beta'])\n        cov_trend = amplitude_trend * pm.gp.cov.ExpQuad(1, ls_trend)\n\n        gp_trend = pm.gp.HSGP(\n            m=[10], \n            c=5., \n            cov_func=cov_trend\n        )\n        trend = gp_trend.prior(\"trend\", X=t[:, None], dims=\"obs_id\")\n\n        # Seasonality components\n        beta_fourier = pm.Normal(\n            \"beta_fourier\", mu=self.model_config['beta_fourier']['mu'], sigma=self.model_config['beta_fourier']['sigma'], dims=\"fourier_features\"\n        )\n        seasonality = pm.Deterministic(\n            \"seasonal\", pm.math.dot(beta_fourier, fourier_terms), dims=\"obs_id\"\n        )\n\n        # Combine components\n        mu = trend + seasonality\n\n        pm.Normal(\n            \"temperature_normalized\",\n            mu=mu,\n            sigma=error,\n            observed=self.y_normalized,\n            dims=\"obs_id\",\n            )\nFinally, the last three required methods are all methods that define configurations for our model. As we mentioned earlier, all configurations should be handled by Kedro in our projects’ conf/ directory. So we are going to return empty dicts as our default configurations.\n# ModelBuilder Required Methods Implementations\n@staticmethod\ndef get_default_model_config() -&gt; dict:\n    model_config = {}\n    return model_config\n\n@staticmethod\ndef get_default_sampler_config() -&gt; dict:\n    sampler_config= {}\n    return sampler_config\n\n@property\ndef output_var(self):\n    \"\"\"\n    This is basically the name of the target variable. This is used when loading in a saved model.\n    \"\"\"\n    return \"temperature\"\n\n@property\n@abstractmethod\ndef _serializable_model_config(self) -&gt; dict[str, int | float | dict]:\n    \"\"\"\n    Only needs to be implemented if your model configurations contain unserializable objects.\n    You need to convert the non-serializable objects to serializable ones here.\n    \"\"\"\n    return self.model_config\nLet’s go ahead and define our model and sampler configurations in the parameters_ML.yml file.\n#/conf/base/parameters_ML.yml\ntesting_window: 12\n\nmodel_config:\n  error: 0.2\n  amplitude_trend: 1.0\n  ls_trend:\n    alpha: 48\n    beta: 2\n  beta_fourier:\n    mu: 0\n    sigma: 0.5\n\nsampler_config:\n  draws: 1000\n  tune: 1000\n  chains: 4\n  target_accept: 0.95\nOur model configurations contain the priors that we defined for our model above. Whereas, the sampler configurations define parameters related to our MCMC sampler.\nWith the required ModelBuilder methods out of the way, let’s talk about some of the methods that we will override to accomodate our specific use case.\nFirst, since we are using a time-series model our predictions are generated differently than how you’d generate predictions from a typical regression or classification task. So we need to make changes to the sample_posterior_predictive() method to accomdate a time-series model. We take the approach of defining a new model specifically for forecasting purposes. Notice that we are calling _data_setter() to generate our future model input, which is our forecast horizon.\n# ModelBuilder Override method\ndef sample_posterior_predictive(self, n_ahead: int, extend_idata: bool, combined: bool, **kwargs):\n    self._data_setter(n_ahead)\n\n    with pm.Model() as self.model:\n        self.model.add_coord(\"obs_id\", self.train_time_range)\n        self.model.add_coord(\n            \"fourier_features\",\n            np.arange(len(self.train_fourier_series.to_numpy().T)),\n        )\n\n        t = pm.Data(\"time_range\", self.train_time_range, dims=\"obs_id\")\n        fourier_terms = pm.Data(\n            \"fourier_terms\", self.train_fourier_series.to_numpy().T\n        )\n\n        error = pm.HalfNormal(\"error\", self.model_config['error'])\n\n        # Trend component\n        amplitude_trend = pm.HalfNormal(\"amplitude_trend\", self.model_config['amplitude_trend'])\n        ls_trend = pm.Gamma(\"ls_trend\", alpha=self.model_config['ls_trend']['alpha'], beta=self.model_config['ls_trend']['beta'])\n        cov_trend = amplitude_trend * pm.gp.cov.ExpQuad(1, ls_trend)\n\n        gp_trend = pm.gp.HSGP(\n            m=[10], \n            c=5., \n            cov_func=cov_trend\n        )\n        trend = gp_trend.prior(\"trend\", X=t[:, None], dims=\"obs_id\")\n\n        # Seasonality components\n        beta_fourier = pm.Normal(\n            \"beta_fourier\", mu=self.model_config['beta_fourier']['mu'], sigma=self.model_config['beta_fourier']['sigma'], dims=\"fourier_features\"\n        )\n        seasonality = pm.Deterministic(\n            \"seasonal\", pm.math.dot(beta_fourier, fourier_terms), dims=\"obs_id\"\n        )\n\n        # Combine components\n        mu = trend + seasonality\n\n        pm.Normal(\n            \"temperature_normalized\",\n            mu=mu,\n            sigma=error,\n            observed=self.y_normalized,\n            dims=\"obs_id\",\n            )\n        \n        self.model.add_coords({\"obs_id_fut\": np.arange(self.start, self.end, 1)})\n\n        t_fut = pm.Data(\"time_range_fut\", np.arange(self.start, self.end, 1))\n        fourier_terms_fut = pm.Data(\"fourier_terms_fut\", self.test_fourier_series.to_numpy().T)\n\n        # Trend future component\n        trend_fut = gp_trend.conditional(\"trend_fut\", Xnew=t_fut[:, None], dims=\"obs_id_fut\")\n\n        # Seasonality components\n        seasonality_fut = pm.Deterministic(\n            \"seasonal_fut\", pm.math.dot(beta_fourier, fourier_terms_fut), dims=\"obs_id_fut\"\n        )\n\n        mu_fut = trend_fut + seasonality_fut\n\n        pm.Normal(\n            \"temperature_normalized_fut\",\n            mu=mu_fut,\n            sigma=error,\n            dims=\"obs_id_fut\",\n            )\n\n    with self.model:  # sample with new input data\n        post_pred = pm.sample_posterior_predictive(self.idata, var_names=[\"temperature_normalized_fut\"], predictions=True, **kwargs)\n        if extend_idata:\n            self.idata.extend(post_pred, join=\"right\")\n\n    posterior_predictive_samples = az.extract(\n        post_pred, \"predictions\", combined=combined\n    )\n\n    return posterior_predictive_samples\nWe also need to update the fit() method for a couple of reasons. The first is because we implemented a normalize_target method and we need to propogate that into the fit() method which calls our build_model() method. Second, because the defualt fit() method defines additional sampler configurations that we don’t want because we have already centralized all of our configurations to be handled by Kedro.\n# ModelBuilder Override method\ndef fit(self, X: pl.DataFrame, y: pl.Series, normalize_target: bool = False) -&gt; az.InferenceData:\n    \"\"\"\n    Fits the model to the provided dataset\n    ---\n    Params:\n        X: The dataset container predictor variables\n        y: The target variable\n        normalize_target: Whether to Z normalize the target variable\n    \"\"\"\n    self.build_model(X, y, normalize_target=normalize_target)\n    self.idata = self.sample_model(**self.sampler_config)\n\n    X_df = X.to_pandas()\n    combined_data = pd.concat([X_df, y.to_pandas()], axis=1)\n    assert all(combined_data.columns), \"All columns must have non-empty names\"\n    with warnings.catch_warnings():\n        warnings.filterwarnings(\n            \"ignore\",\n            category=UserWarning,\n            message=\"The group fit_data is not defined in the InferenceData scheme\",\n        )\n        self.idata.add_groups(fit_data=combined_data.to_xarray())\n\n    return self.idata\nLastly (I promise), we need to define a new method that will handle our model evaluations. We are going to use the root mean squared error as a metric to evaluate our model. We will also measure the coverage of our 80% Highest Density Interval (HDI). Additionally, it is a good idea to include in our model serialization the city the model is fitted to, and if we are standardizing the data then we should also include the mean and standard deviation of our target temperature values.\n# ModelBuilder override Init\ndef __init__(\n        self,\n        model_config: dict | None = None,\n        sampler_config: dict | None = None,\n        city: str = None\n    ):\n        \"\"\"\n        Initializes model configuration and sampler configuration for the model\n\n        Parameters\n        ----------\n        model_config : Dictionary, optional\n            dictionary of parameters that initialise model configuration. Class-default defined by the user default_model_config method.\n        sampler_config : Dictionary, optional\n            dictionary of parameters that initialise sampler configuration. Class-default defined by the user default_sampler_config method.\n        city: The Brazilian city we are modeling monthly average temperatures of\n        \"\"\"\n        sampler_config = (\n            self.get_default_sampler_config() if sampler_config is None else sampler_config\n        )\n        self.sampler_config = sampler_config\n        model_config = self.get_default_model_config() if model_config is None else model_config\n\n        self.model_config = model_config  \n        self.model = None  \n        self.idata: az.InferenceData | None = None\n        self.is_fitted_ = False\n        self.city = city # This is our Addition\n\n# ModelBuilder new method\ndef evaluate(self, y_true: pl.Series, forecasts: xr.Dataset, back_transform: bool = False) -&gt; dict:\n    \"\"\"\n    Evaluate our forecasts posterior predictive mean using the root mean squared error (RMSE) as the metric and evaluate our highest density interval's (HDI)s coverage\n    ---\n    Params:\n        y_true: The ground truth temperatures\n        forecasts: The forecasts\n        back_transform: Whether we need to transform our forecasts back to the original scale\n    \"\"\"\n    if back_transform:\n        try:\n            y_mean = self.y_mean\n            y_std = self.y_std\n        except AttributeError:\n            y_mean = self.idata.attrs['y_mean']\n            y_std = self.idata.attrs['y_std']\n        posterior_predictive_mean = forecasts[f'{self.output_var}_normalized_fut'].mean((\"chain\", \"draw\")).values * y_std + y_mean\n        hdi = az.hdi(forecasts[f'{self.output_var}_normalized_fut'], hdi_prob=0.8) * y_std + y_mean\n    else:\n        posterior_predictive_mean = forecasts[f'{self.output_var}_normalized_fut'].mean((\"chain\", \"draw\")).values\n        hdi = az.hdi(forecasts[f'{self.output_var}_normalized_fut'], hdi_prob=0.8)\n\n    error = y_true.to_numpy() - posterior_predictive_mean\n    RMSE = np.sqrt(\n        np.nanmean(\n            np.square(error)\n        )\n    )\n\n    coverage_df = pl.DataFrame(\n        {\n            \"hdi_lower\": hdi[f'{self.output_var}_normalized_fut'][:, 0].values,\n            \"hdi_upper\": hdi[f'{self.output_var}_normalized_fut'][:, 1].values,\n            \"y_true\": y_true\n        }\n    )\n\n    COVERAGE = (\n        coverage_df\n            .filter(\n                pl.col(\"y_true\").is_not_null()\n            )\n            .with_columns(\n                pl.when(\n                    (pl.col(\"y_true\") &lt;= pl.col(\"hdi_upper\")) &\n                    (pl.col(\"y_true\") &gt;= pl.col(\"hdi_lower\"))\n                )\n                .then(1.)\n                .otherwise(0.)\n                .alias(\"coverage\")\n            )\n            .select(pl.col(\"coverage\").mean()).item()\n    )\n    \n    return {\"RMSE\": RMSE, \"HDI_COVERAGE\": COVERAGE}\n\ndef _save_input_params(self, idata) -&gt; None:\n    \"\"\"\n    Saves any additional model parameters (other than the dataset) to the idata object.\n    \"\"\"\n    idata.attrs[\"city\"] = self.city\n    idata.attrs[\"y_mean\"] = self.y_mean\n    idata.attrs[\"y_std\"] = self.y_std\nCombining everything together we have our time-series model implemented as:\n# /src/climate_brazil/pipelines/ML/ts_model.py\nimport warnings\nfrom abc import abstractmethod\n\nimport arviz as az\nimport numpy as np\nimport pandas as pd\nimport polars as pl\nimport pymc as pm\nimport xarray as xr\nfrom pymc_extras.model_builder import ModelBuilder\n\n\nclass TSModel(ModelBuilder):\n    _model_type = \"TimeSeries\"\n    version = \"0.1\"\n\n    def __init__(\n        self,\n        model_config: dict | None = None,\n        sampler_config: dict | None = None,\n        city: str = None\n    ):\n        \"\"\"\n        Initializes model configuration and sampler configuration for the model\n\n        Parameters\n        ----------\n        model_config : Dictionary, optional\n            dictionary of parameters that initialise model configuration. Class-default defined by the user default_model_config method.\n        sampler_config : Dictionary, optional\n            dictionary of parameters that initialise sampler configuration. Class-default defined by the user default_sampler_config method.\n        city: The Brazilian city we are modeling monthly average temperatures of\n        \"\"\"\n        sampler_config = (\n            self.get_default_sampler_config() if sampler_config is None else sampler_config\n        )\n        self.sampler_config = sampler_config\n        model_config = self.get_default_model_config() if model_config is None else model_config\n\n        self.model_config = model_config  # parameters for priors etc.\n        self.model = None  # Set by build_model\n        self.idata: az.InferenceData | None = None  # idata is generated during fitting\n        self.is_fitted_ = False\n        self.city = city\n\n    def _generate_and_preprocess_model_data(self, X: pl.DataFrame, y: pl.Series, normalize: bool = False):\n        \"\"\"\n        Last mile data processing of inputs expected by the model\n        ---\n        Params:\n            X: The matrix of predictor variables expected by our model\n            y: The target variable\n            normalize: Whether to Z normalize the variables\n        \"\"\"\n        self.train_time_range = np.arange(X.shape[0])\n        self.n_modes = 10\n        periods = np.array(self.train_time_range) / (12)\n        self.train_fourier_series = pl.DataFrame(\n            {\n                f\"{func}_order_{order}\": getattr(np, func)(\n                    2 * np.pi * periods * order\n                )\n                for order in range(1, self.n_modes + 1)\n                for func in (\"sin\", \"cos\")\n            }\n        )\n        if normalize:\n            self.y_mean = np.nanmean(y)\n            self.y_std = np.nanstd(y)\n            self.y_normalized = (y - self.y_mean) / self.y_std\n        else:\n            self.y_normalized = y\n\n    def _data_setter(self, n_ahead: int):\n        \"\"\"\n        Generates required data for producing forecasts\n        ---\n        Params:\n            n_ahead: How many periods (months) to forecast future temperatures\n        \"\"\"\n        self.start = self.train_time_range[-1]\n        self.end = self.start + n_ahead\n\n        new_periods = np.arange(self.start, self.end, 1) / (12)\n        self.test_fourier_series = pl.DataFrame(\n            {\n                f\"{func}_order_{order}\": getattr(np, func)(\n                    2 * np.pi * new_periods * order\n                )\n                for order in range(1, self.n_modes + 1)\n                for func in (\"sin\", \"cos\")\n            }\n        )\n\n    def build_model(self, X: pl.DataFrame, y: pl.Series, normalize_target: bool = False, **kwargs):\n        \"\"\"\n        Defines the PyMC model structure\n        ---\n        Params:\n            X: Dataframe of features\n            y: Array of target values\n        \"\"\"\n\n        self._generate_and_preprocess_model_data(X, y, normalize=normalize_target)\n\n        with pm.Model() as self.model:\n            self.model.add_coord(\"obs_id\", self.train_time_range)\n            self.model.add_coord(\n                \"fourier_features\",\n                np.arange(len(self.train_fourier_series.to_numpy().T)),\n            )\n\n            t = pm.Data(\"time_range\", self.train_time_range, dims=\"obs_id\")\n            fourier_terms = pm.Data(\n                \"fourier_terms\", self.train_fourier_series.to_numpy().T\n            )\n\n            error = pm.HalfNormal(\"error\", self.model_config['error'])\n\n            # Trend component\n            amplitude_trend = pm.HalfNormal(\"amplitude_trend\", self.model_config['amplitude_trend'])\n            ls_trend = pm.Gamma(\"ls_trend\", alpha=self.model_config['ls_trend']['alpha'], beta=self.model_config['ls_trend']['beta'])\n            cov_trend = amplitude_trend * pm.gp.cov.ExpQuad(1, ls_trend)\n\n            gp_trend = pm.gp.HSGP(\n                m=[10],\n                c=5.,\n                cov_func=cov_trend\n            )\n            trend = gp_trend.prior(\"trend\", X=t[:, None], dims=\"obs_id\")\n\n            # Seasonality components\n            beta_fourier = pm.Normal(\n                \"beta_fourier\", mu=self.model_config['beta_fourier']['mu'], sigma=self.model_config['beta_fourier']['sigma'], dims=\"fourier_features\"\n            )\n            seasonality = pm.Deterministic(\n                \"seasonal\", pm.math.dot(beta_fourier, fourier_terms), dims=\"obs_id\"\n            )\n\n            # Combine components\n            mu = trend + seasonality\n\n            pm.Normal(\n                \"temperature_normalized\",\n                mu=mu,\n                sigma=error,\n                observed=self.y_normalized,\n                dims=\"obs_id\",\n                )\n\n    def sample_posterior_predictive(self, n_ahead: int, extend_idata: bool, combined: bool, **kwargs):\n        self._data_setter(n_ahead)\n\n        with pm.Model() as self.model:\n            self.model.add_coord(\"obs_id\", self.train_time_range)\n            self.model.add_coord(\n                \"fourier_features\",\n                np.arange(len(self.train_fourier_series.to_numpy().T)),\n            )\n\n            t = pm.Data(\"time_range\", self.train_time_range, dims=\"obs_id\")\n            fourier_terms = pm.Data(\n                \"fourier_terms\", self.train_fourier_series.to_numpy().T\n            )\n\n            error = pm.HalfNormal(\"error\", self.model_config['error'])\n\n            # Trend component\n            amplitude_trend = pm.HalfNormal(\"amplitude_trend\", self.model_config['amplitude_trend'])\n            ls_trend = pm.Gamma(\"ls_trend\", alpha=self.model_config['ls_trend']['alpha'], beta=self.model_config['ls_trend']['beta'])\n            cov_trend = amplitude_trend * pm.gp.cov.ExpQuad(1, ls_trend)\n\n            gp_trend = pm.gp.HSGP(\n                m=[10],\n                c=5.,\n                cov_func=cov_trend\n            )\n            trend = gp_trend.prior(\"trend\", X=t[:, None], dims=\"obs_id\")\n\n            # Seasonality components\n            beta_fourier = pm.Normal(\n                \"beta_fourier\", mu=self.model_config['beta_fourier']['mu'], sigma=self.model_config['beta_fourier']['sigma'], dims=\"fourier_features\"\n            )\n            seasonality = pm.Deterministic(\n                \"seasonal\", pm.math.dot(beta_fourier, fourier_terms), dims=\"obs_id\"\n            )\n\n            # Combine components\n            mu = trend + seasonality\n\n            pm.Normal(\n                \"temperature_normalized\",\n                mu=mu,\n                sigma=error,\n                observed=self.y_normalized,\n                dims=\"obs_id\",\n                )\n\n            self.model.add_coords({\"obs_id_fut\": np.arange(self.start, self.end, 1)})\n\n            t_fut = pm.Data(\"time_range_fut\", np.arange(self.start, self.end, 1))\n            fourier_terms_fut = pm.Data(\"fourier_terms_fut\", self.test_fourier_series.to_numpy().T)\n\n            # Trend future component\n            trend_fut = gp_trend.conditional(\"trend_fut\", Xnew=t_fut[:, None], dims=\"obs_id_fut\")\n\n            # Seasonality components\n            seasonality_fut = pm.Deterministic(\n                \"seasonal_fut\", pm.math.dot(beta_fourier, fourier_terms_fut), dims=\"obs_id_fut\"\n            )\n\n            mu_fut = trend_fut + seasonality_fut\n\n            pm.Normal(\n                \"temperature_normalized_fut\",\n                mu=mu_fut,\n                sigma=error,\n                dims=\"obs_id_fut\",\n                )\n\n        with self.model:  # sample with new input data\n            post_pred = pm.sample_posterior_predictive(self.idata, var_names=[f\"{self.output_var}_normalized_fut\"], predictions=True, **kwargs)\n            if extend_idata:\n                self.idata.extend(post_pred, join=\"right\")\n\n        posterior_predictive_samples = az.extract(\n            post_pred, \"predictions\", combined=combined\n        )\n\n        return posterior_predictive_samples\n\n    def fit(self, X: pl.DataFrame, y: pl.Series, normalize_target: bool = False) -&gt; az.InferenceData:\n        \"\"\"\n        Fits the model to the provided dataset\n        ---\n        Params:\n            X: The dataset container predictor variables\n            y: The target variable\n            normalize_target: Whether to Z normalize the target variable\n        \"\"\"\n\n        self.build_model(X, y, normalize_target=normalize_target)\n        self.idata = self.sample_model(**self.sampler_config)\n\n        X_df = X.to_pandas()\n        combined_data = pd.concat([X_df, y.to_pandas()], axis=1)\n        assert all(combined_data.columns), \"All columns must have non-empty names\"\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\n                \"ignore\",\n                category=UserWarning,\n                message=\"The group fit_data is not defined in the InferenceData scheme\",\n            )\n            self.idata.add_groups(fit_data=combined_data.to_xarray())  # type: ignore\n\n        return self.idata  # type: ignore\n\n    @staticmethod\n    def get_default_model_config() -&gt; dict:\n        model_config = {\n            \"error\": 0.2,\n            \"amplitude_trend\": 1.0,\n            \"ls_trend\": {\"alpha\": 48, \"beta\": 2},\n            \"beta_fourier\": {\"mu\": 0, \"sigma\": 0.5},\n        }\n        return model_config\n\n    @staticmethod\n    def get_default_sampler_config() -&gt; dict:\n        \"\"\"\n        Returns a class default sampler dict for model builder if no sampler_config is provided on class initialization.\n        The sampler config dict is used to send parameters to the sampler .\n        It will be used during fitting in case the user doesn't provide any sampler_config of their own.\n        \"\"\"\n        sampler_config= {\n            \"draws\": 1_000,\n            \"tune\": 1_000,\n            \"chains\": 4,\n            \"target_accept\": 0.95,\n        }\n        return sampler_config\n\n    @property\n    def output_var(self):\n        return \"temperature\"\n\n    @property\n    @abstractmethod\n    def _serializable_model_config(self) -&gt; dict[str, int | float | dict]:\n        \"\"\"\n        Converts non-serializable values from model_config to their serializable reversable equivalent.\n        Data types like pandas DataFrame, Series or datetime aren't JSON serializable,\n        so in order to save the model they need to be formatted.\n\n        Returns\n        -------\n        model_config: dict\n        \"\"\"\n        return self.model_config\n\n    def evaluate(self, y_true: pl.Series, forecasts: xr.Dataset, back_transform: bool = False) -&gt; dict:\n        \"\"\"\n        Evaluate our forecasts posterior predictive mean using the root mean squared error (RMSE) as the metric and evaluate our highest density interval's (HDI)s coverage\n        ---\n        Params:\n            y_true: The ground truth temperatures\n            forecasts: The forecasts\n            back_transform: Whether we need to transform our forecasts back to the original scale\n        \"\"\"\n        if back_transform:\n            try:\n                y_mean = self.y_mean\n                y_std = self.y_std\n            except AttributeError:\n                y_mean = self.idata.attrs['y_mean']\n                y_std = self.idata.attrs['y_std']\n            posterior_predictive_mean = forecasts[f'{self.output_var}_normalized_fut'].mean((\"chain\", \"draw\")).values * y_std + y_mean\n            hdi = az.hdi(forecasts[f'{self.output_var}_normalized_fut'], hdi_prob=0.8) * y_std + y_mean\n        else:\n            posterior_predictive_mean = forecasts[f'{self.output_var}_normalized_fut'].mean((\"chain\", \"draw\")).values\n            hdi = az.hdi(forecasts[f'{self.output_var}_normalized_fut'], hdi_prob=0.8)\n\n        error = y_true.to_numpy() - posterior_predictive_mean\n        RMSE = np.sqrt(\n            np.nanmean(\n                np.square(error)\n            )\n        )\n\n        coverage_df = pl.DataFrame(\n            {\n                \"hdi_lower\": hdi[f'{self.output_var}_normalized_fut'][:, 0].values,\n                \"hdi_upper\": hdi[f'{self.output_var}_normalized_fut'][:, 1].values,\n                \"y_true\": y_true\n            }\n        )\n\n        COVERAGE = (\n            coverage_df\n              .filter(\n                  pl.col(\"y_true\").is_not_null()\n              )\n              .with_columns(\n                  pl.when(\n                      (pl.col(\"y_true\") &lt;= pl.col(\"hdi_upper\")) &\n                      (pl.col(\"y_true\") &gt;= pl.col(\"hdi_lower\"))\n                  )\n                    .then(1.)\n                    .otherwise(0.)\n                    .alias(\"coverage\")\n              )\n              .select(pl.col(\"coverage\").mean()).item()\n        )\n\n        return {\"RMSE\": RMSE, \"HDI_COVERAGE\": COVERAGE}\n\n    def _save_input_params(self, idata: az.InferenceData) -&gt; None:\n        \"\"\"\n        Saves any additional model parameters (other than the dataset) to the idata object.\n        \"\"\"\n        idata.attrs[\"city\"] = self.city\n        idata.attrs[\"y_mean\"] = self.y_mean\n        idata.attrs[\"y_std\"] = self.y_std"
  },
  {
    "objectID": "posts/kedro-pymc-modelbuilder/index.html#training-node",
    "href": "posts/kedro-pymc-modelbuilder/index.html#training-node",
    "title": "Reproducibility & Scalability Part 2PyMC ModelBuilder",
    "section": "Training Node",
    "text": "Training Node\nNow that we have our model configured with ModelBuilder it is time to define our training node:\n#/src/climate_brazil/pipelines/ML/nodes.py\ndef train(training_dataset: pl.DataFrame, model_config: dict, sampler_config: dict) -&gt; dict:\n    \"\"\"\n    Train time-series models\n    ---\n    Params:\n        training_dataset: Training data\n        model_config: Model Configurations\n        sampler_config: MCMC sampler configurations\n    \"\"\"\n    city_models = {}\n    for city in training_dataset['city'].sort().unique(maintain_order=True):\n        city_dataset = training_dataset.filter(pl.col(\"city\")==city).sort(\"date\")\n        model = TSModel(model_config=model_config, sampler_config=sampler_config, city=city)\n        model.fit(X=city_dataset['date'], y=city_dataset['temperature'], normalize_target=True)\n        city_models[f\"{city}_model\"] = model\n\n    return city_models\n\n\n\n\n\n\nImportant\n\n\n\nDO NOT use this training node in production! We are going to refactor this code in the last part of this series to scale more efficiently.\n\n\nWe need to be sure to define catalog entries for our training output otherwise Kedro will designate our trained models as MemoryDataset’s. However, there is no built-in dataset in Kedro that knows how to store our model. So we need to define a custom Kedro dataset to accomodate our needs.\n\nCreating a Custom Kedro Dataset\nCreating a custom dataset can be quite simple. All we need to do is inheret from Kedro’s AbstractDataset and define the init, save, load, and describe methods. In our case, the ModelBuilder class has save and load methods that accept a filepath and we will use those methods in our custom dataset:\n# /src/climate_brazil/datasets/pymc_model_dataset.py\nfrom pathlib import Path, PurePosixPath\nfrom typing import Any\n\nfrom kedro.io import AbstractDataset\n\nfrom kedro_framework.pipelines.ML.ts_model import TSModel\n\n\nclass PyMCModelDataset(AbstractDataset):\n    \"\"\"\n    ``PyMCDataset`` loads / save PyMC models from a given filepath as a TSModel object.\n    \"\"\"\n\n    def __init__(self, filepath: str):\n        \"\"\"Creates a new instance of PyMCDataset to load / save PyMC models for a given filepath.\n\n        Args:\n            filepath: The location of the model netcdf file to load / save data.\n        \"\"\"\n        self._filepath = PurePosixPath(filepath)\n        self._path = Path(self._filepath.parent)\n\n    def load(self) -&gt; TSModel:\n        \"\"\"Loads data from the netcdf file.\n\n        Returns:\n            loaded TSModel\n        \"\"\"\n        model = TSModel.load(self._filepath)\n        return model\n\n    def save(self, model: TSModel) -&gt; None:\n        \"\"\"Saves PyMC model to the specified filepath.\"\"\"\n        self._path.mkdir(parents=True,  exist_ok=True)\n        model.save(self._filepath)\n\n    def _describe(self) -&gt; dict[str, Any]:\n        \"\"\"Returns a dict that describes the attributes of the dataset.\"\"\"\n        return dict(filepath=self._filepath)\n\n\nTrain Catalog Entry\nWith our custom dataset defined let’s define entries for our training and testing data as well as for our trained models.\n# conf/base/catalog.yml\n# Raw data ---------------------------------------\ncities_temperatures:\n  type: partitions.PartitionedDataset\n  path: data/01_raw/\n  dataset: \n    type: polars.LazyPolarsDataset\n    file_format: csv\n\n# Processed data ---------------------------------------\ncities_temperatures_processed:\n  type: polars.EagerPolarsDataset\n  file_format: parquet\n  filepath: data/02_intermediate/cities_temperatures_processed.pq\n\ncities_ts_plot:\n  type: plotly.JSONDataset\n  filepath: data/08_reporting/cities_ts_plot.json\n\n# Model inputs ------------------------------------------\ntraining_dataset:\n  type: polars.EagerPolarsDataset\n  file_format: parquet\n  filepath: data/05_model_input/training_dataset.pq\n\ntesting_dataset:\n  type: polars.EagerPolarsDataset\n  file_format: parquet\n  filepath: data/05_model_input/testing_dataset.pq\n\n# Models -----------------------------------------\nBelem_model:\n  type: climate_brazil.datasets.pymc_model_dataset.PyMCModelDataset\n  filepath: data/06_models/Belem_model.nc\n\nCuritiba_model:\n  type: climate_brazil.datasets.pymc_model_dataset.PyMCModelDataset\n  filepath: data/06_models/Curitiba_model.nc\n\nFortaleza_model:\n  type: climate_brazil.datasets.pymc_model_dataset.PyMCModelDataset\n  filepath: data/06_models/Fortaleza_model.nc\n\nGoiania_model:\n  type: climate_brazil.datasets.pymc_model_dataset.PyMCModelDataset\n  filepath: data/06_models/Goiania_model.nc\n\nMacapa_model:\n  type: climate_brazil.datasets.pymc_model_dataset.PyMCModelDataset\n  filepath: data/06_models/Macapa_model.nc\n\nManaus_model:\n  type: climate_brazil.datasets.pymc_model_dataset.PyMCModelDataset\n  filepath: data/06_models/Manaus_model.nc\n\nRecife_model:\n  type: climate_brazil.datasets.pymc_model_dataset.PyMCModelDataset\n  filepath: data/06_models/Recife_model.nc\n\nRio_model:\n  type: climate_brazil.datasets.pymc_model_dataset.PyMCModelDataset\n  filepath: data/06_models/Rio_model.nc\n\nSalvador_model:\n  type: climate_brazil.datasets.pymc_model_dataset.PyMCModelDataset\n  filepath: data/06_models/Salvador_model.nc\n\nSao_Luiz_model:\n  type: climate_brazil.datasets.pymc_model_dataset.PyMCModelDataset\n  filepath: data/06_models/Sao_Luiz_model.nc\n\nSao_Paulo_model:\n  type: climate_brazil.datasets.pymc_model_dataset.PyMCModelDataset\n  filepath: data/06_models/Sao_Paulo_model.nc\n\nVitoria_model:\n  type: climate_brazil.datasets.pymc_model_dataset.PyMCModelDataset\n  filepath: data/06_models/Vitoria_model.nc\n\n\n\n\n\n\nNote\n\n\n\nFor the sake of clarity, we added one entry for each city. You can use a Kedro’s dataset factory feature to capture all city models with one entry. We went over an example of how to use a dataset factory in part one of this series."
  },
  {
    "objectID": "posts/kedro-pymc-modelbuilder/index.html#forecasting-node",
    "href": "posts/kedro-pymc-modelbuilder/index.html#forecasting-node",
    "title": "Reproducibility & Scalability Part 2PyMC ModelBuilder",
    "section": "Forecasting Node",
    "text": "Forecasting Node\nFinally, we need to define how to generate our forecasts. We would like our forecasting node to have the ability to perform evaluations if we pass in the testing dataset as an argument and for the moment we just want to log out our evaluation metrics to stdout.\n# /src/climate_brazil/datasets/pymc_forecast_dataset.py\ndef forecast(model: TSModel, n_ahead: int, ground_truth: Optional[pl.DataFrame] = None) -&gt; xr.Dataset:\n    \"\"\"\n    Generates forecasts from trained time-series and produces evaluations if ground truth is passed in.\n    ---\n    Params:\n        model: The trained time-series model\n        n_ahead: The forecast horizon\n        ground_truth: The actual values to be compared with the forecasts\n    \"\"\"\n    forecasts = model.sample_posterior_predictive(n_ahead=n_ahead, extend_idata=True, combined=False)\n    if ground_truth is not None:\n        evaluations = model.evaluate(y_true=ground_truth.filter(pl.col(\"city\")==model.idata.attrs['city'])[\"temperature\"], forecasts=forecasts, back_transform=True)\n        logger.info(f\"{model.idata.attrs['city']} Evaluations: {evaluations}\")\n    return forecasts\nNotice that we need to pass in an n_ahead parameter that defines the forecast horizon. Now is a good time to add that parameter into /conf/base/parameters_ML.yml\ntesting_window: 12\nn_ahead: 12\n\nmodel_config:\n  error: 0.2\n  amplitude_trend: 1.0\n  ls_trend:\n    alpha: 48\n    beta: 2\n  beta_fourier:\n    mu: 0\n    sigma: 0.5\n\nsampler_config:\n  draws: 1000\n  tune: 1000\n  chains: 4\n  target_accept: 0.95\n  mp_ctx: spawn # might need this if you are running on MacOS\nOur forecast output will be an xarray dataset and we will go ahead and save that to disk in NETCDF format. Once again, we will create a custom Kedro dataset for that.\n# /src/climate_brazil/datasets/pymc_forecast_dataset.py\nfrom pathlib import Path, PurePosixPath\nfrom typing import Any\n\nimport xarray as xr\nfrom kedro.io import AbstractDataset\n\n\nclass PyMCForecastDataset(AbstractDataset):\n    \"\"\"\n    ``PyMCForecastDataset`` loads / save PyMC forecasts from a given filepath as an xarray Dataset.\n    \"\"\"\n\n    def __init__(self, filepath: str):\n        \"\"\"Creates a new instance of PyMCForecastDataset to load / save PyMC forecasts for a given filepath.\n\n        Args:\n            filepath: The location of the forecast netcdf file to load / save.\n        \"\"\"\n        self._filepath = PurePosixPath(filepath)\n        self._path = Path(self._filepath.parent)\n\n    def load(self) -&gt; xr.Dataset:\n        \"\"\"Loads data from the netcdf file.\n\n        Returns:\n            loaded forecasts\n        \"\"\"\n        return xr.load_dataset(self._filepath)\n\n    def save(self, forecast: xr.Dataset) -&gt; None:\n        \"\"\"Saves PyMC forecasts to the specified filepath.\"\"\"\n        self._path.mkdir(parents=True,  exist_ok=True)\n        forecast.to_netcdf(path=self._filepath)\n\n    def _describe(self) -&gt; dict[str, Any]:\n        \"\"\"Returns a dict that describes the attributes of the dataset.\"\"\"\n        return dict(filepath=self._filepath)"
  },
  {
    "objectID": "posts/kedro-pymc-modelbuilder/index.html#train",
    "href": "posts/kedro-pymc-modelbuilder/index.html#train",
    "title": "Reproducibility & Scalability Part 2PyMC ModelBuilder",
    "section": "Train",
    "text": "Train\nHere we are going to introduce tags, another Kedro feature that allows you to run specific nodes by tag category. At this stage our pipeline will be the training split node followed by the training node.\n# /src/climate_brazil/pipelines/ML/pipeline.py\nfrom kedro.pipeline import node, Pipeline, pipeline  # noqa\nfrom .nodes import train_test_split, train, forecast\n\ndef create_pipeline(**kwargs) -&gt; Pipeline:\n    split_train_nodes =  [\n        node(\n            func=train_test_split,\n            inputs=dict(\n                data = \"cities_temperatures_processed\",\n                testing_window=\"params:testing_window\"\n            ),\n            outputs=[\"training_dataset\", \"testing_dataset\"],\n            name=\"train_test_split\",\n            tags=[\"train\"]\n        ),\n        node(\n            func=train,\n            inputs=dict(\n                training_dataset=\"training_dataset\",\n                model_config=\"params:model_config\",\n                sampler_config=\"params:sampler_config\"\n            ),\n            outputs=dict(\n                Belem_model = \"Belem_model\",\n                Curitiba_model = \"Curitiba_model\",\n                Fortaleza_model = \"Fortaleza_model\",\n                Goiania_model = \"Goiania_model\",\n                Macapa_model = \"Macapa_model\",\n                Manaus_model = \"Manaus_model\",\n                Recife_model = \"Recife_model\",\n                Rio_model = \"Rio_model\",\n                Salvador_model = \"Salvador_model\",\n                Sao_Luiz_model = \"Sao_Luiz_model\",\n                Sao_Paulo_model = \"Sao_Paulo_model\",\n                Vitoria_model = \"Vitoria_model\"\n            ),\n            name=\"train\",\n            tags=[\"train\"]\n        )\n    ]\n\n\n\n\n\n\nCaution\n\n\n\nAlways make sure that your inputs and outputs match the keys you chose in catalog.yml"
  },
  {
    "objectID": "posts/kedro-pymc-modelbuilder/index.html#forecast",
    "href": "posts/kedro-pymc-modelbuilder/index.html#forecast",
    "title": "Reproducibility & Scalability Part 2PyMC ModelBuilder",
    "section": "Forecast",
    "text": "Forecast\nThis is where we are going to leverage the flexibility of tags. We are going to create two sub-pipelines for our forecasts. The first sub-pipeline will be for training purposes, this is when we have ground-truth data to pass in for evaluations. The second will be when we are using our time-series model in production and we do not have the ground-truth available. Our full ML pipeline will look like this:\n# /src/climate_brazil/pipelines/ML/pipeline.py\nfrom kedro.pipeline import node, Pipeline, pipeline  # noqa\nfrom .nodes import train_test_split, train, forecast\n\ncities = [\"Belem\", \"Curitiba\", \"Fortaleza\", \"Goiania\", \"Macapa\", \"Manaus\", \"Recife\", \"Rio\", \"Salvador\", \"Sao_Luiz\", \"Sao_Paulo\", \"Vitoria\"]\n\ndef create_pipeline(**kwargs) -&gt; Pipeline:\n    split_train_nodes =  [\n        node(\n            func=train_test_split,\n            inputs=dict(\n                data = \"cities_temperatures_processed\",\n                testing_window=\"params:testing_window\"\n            ),\n            outputs=[\"training_dataset\", \"testing_dataset\"],\n            name=\"train_test_split\",\n            tags=[\"train\"]\n        ),\n        node(\n            func=train,\n            inputs=dict(\n                training_dataset=\"training_dataset\",\n                model_config=\"params:model_config\",\n                sampler_config=\"params:sampler_config\"\n            ),\n            outputs=dict(\n                Belem_model = \"Belem_model\",\n                Curitiba_model = \"Curitiba_model\",\n                Fortaleza_model = \"Fortaleza_model\",\n                Goiania_model = \"Goiania_model\",\n                Macapa_model = \"Macapa_model\",\n                Manaus_model = \"Manaus_model\",\n                Recife_model = \"Recife_model\",\n                Rio_model = \"Rio_model\",\n                Salvador_model = \"Salvador_model\",\n                Sao_Luiz_model = \"Sao_Luiz_model\",\n                Sao_Paulo_model = \"Sao_Paulo_model\",\n                Vitoria_model = \"Vitoria_model\"\n            ),\n            name=\"train\",\n            tags=[\"train\"]\n        )\n    ]\n\n    forecast_training_nodes = [\n        node(\n            func=forecast,\n            inputs=dict(\n                model=f\"{city}_model\",\n                n_ahead=\"params:n_ahead\",\n                ground_truth=\"testing_dataset\"\n            ),\n            outputs=f\"{city}_forecasts_evaluation\",\n            name=f\"{city}_forecasts_evaluation\",\n            tags=[\"train\"]\n        )\n        for city in cities\n    ]\n\n    forecast_nodes = [\n        node(\n            func=forecast,\n            inputs=dict(\n                model=f\"{city}_model\",\n                n_ahead=\"params:n_ahead\"\n            ),\n            outputs=f\"{city}_forecasts\",\n            name=f\"{city}_forecast\",\n            tags=[\"forecast\"]\n        )\n        for city in cities\n    ]\n\n    return pipeline(split_train_nodes + forecast_training_nodes + forecast_nodes)"
  },
  {
    "objectID": "posts/kedro-pymc-modelbuilder/index.html#defining-custom-pipelines",
    "href": "posts/kedro-pymc-modelbuilder/index.html#defining-custom-pipelines",
    "title": "Reproducibility & Scalability Part 2PyMC ModelBuilder",
    "section": "Defining Custom Pipelines",
    "text": "Defining Custom Pipelines\nPreviously, we let Kedro automatically register our pipelines based on the project pipelines we created. However, we now have sub-pipelines that we would like to define based on our tags. We can do so in src/climate_brazil/pipeline_registry.py:\n#src/climate_brazil/pipeline_registry.py\n\"\"\"Project pipelines.\"\"\"\n\nfrom kedro.framework.project import find_pipelines\nfrom kedro.pipeline import Pipeline\n\nfrom climate_brazil.pipelines.data_processing import create_pipeline as dp\nfrom climate_brazil.pipelines.ML import create_pipeline as ml\n\ndef register_pipelines() -&gt; dict[str, Pipeline]:\n    \"\"\"Register the project's pipelines.\n\n    Returns:\n        A mapping from pipeline names to ``Pipeline`` objects.\n    \"\"\"\n    dp_pipeline = dp()\n    ml_pipeline = ml()\n    training_pipeline = ml_pipeline.only_nodes_with_tags(\"train\")\n    forecast_pipeline = ml_pipeline.only_nodes_with_tags(\"forecast\")\n\n    return {\n        \"data_processing\": dp_pipeline,\n        \"train\": dp_pipeline + training_pipeline,\n        \"forecast\": forecast_pipeline,\n        \"__default__\": dp_pipeline + training_pipeline\n    }\nNotice how we broke down our ML pipeline into two sub-pipelines. Also, notice how we defined the train pipeline to consist of both the data processing pipeline and the training pipeline. We also assigned the train pipeline to be the default pipeline. This means if we execute kedro run without specifying a specific pipeline it will run the train pipeline."
  },
  {
    "objectID": "posts/kedro-pymc-modelbuilder/index.html#training-pipeline",
    "href": "posts/kedro-pymc-modelbuilder/index.html#training-pipeline",
    "title": "Reproducibility & Scalability Part 2PyMC ModelBuilder",
    "section": "Training Pipeline",
    "text": "Training Pipeline\nYou can execute your training pipeline by either executing\nkedro run\nor\nkedro run --pipeline train\nLet’s go ahead and run one of the above to train the model and generate forecasts on the test set with evaluations."
  },
  {
    "objectID": "posts/kedro-pymc-modelbuilder/index.html#forecasting-pipeline",
    "href": "posts/kedro-pymc-modelbuilder/index.html#forecasting-pipeline",
    "title": "Reproducibility & Scalability Part 2PyMC ModelBuilder",
    "section": "Forecasting pipeline",
    "text": "Forecasting pipeline\nIf you already have a trained model and you don’t have the ground truth for evaluations you can execute\nkedro run --pipeline forecast"
  },
  {
    "objectID": "posts/kedro-pymc-modelbuilder/index.html#visualizing-forecasts",
    "href": "posts/kedro-pymc-modelbuilder/index.html#visualizing-forecasts",
    "title": "Reproducibility & Scalability Part 2PyMC ModelBuilder",
    "section": "Visualizing Forecasts",
    "text": "Visualizing Forecasts\nWe’ve seen our evaluation metrics logged to stdout, which by the way is written to /info.log, let’s also look at our fitted model and forecasts against the actual data. To do that let’s add a node that will handle generating these plots in our pipeline.\ndef forecast_plot(training_dataset: pl.DataFrame, testing_dataset: pl.DataFrame, model: TSModel, forecast: xr.Dataset) -&gt; go.Figure:\n    \"\"\"\n    Generates plot showing in-sample posterior predictive performance as well as out-of-sample forecasts\n    ---\n    Params:\n        training_dataset: The training split\n        testing_dataset: The testing split\n        model: the trained model\n        forecasts: the forecast from the trainined model\n    \"\"\"\n    # City specific data\n    city = model.idata.attrs['city']\n    city_training_dataset = training_dataset.filter(pl.col(\"city\")==city)\n    city_testing_dataset = testing_dataset.filter(pl.col(\"city\")==city)\n\n    # Model fit posterior predictive mean and HDI\n    posterior_mean_normalized = model.idata.posterior_predictive['temperature_normalized'].mean(('chain', 'draw'))\n    hdi_normalized = az.hdi(model.idata.posterior_predictive['temperature_normalized'], hdi_prob=0.8)\n    posterior_mean = posterior_mean_normalized * model.idata.attrs['y_std'] + model.idata.attrs['y_mean']\n    hdi = hdi_normalized * model.idata.attrs['y_std'] + model.idata.attrs['y_mean']\n\n    # Forecast posterior predictive mean and HDI\n    posterior_predictive_mean_normalized = forecast['temperature_normalized_fut'].mean(('chain', 'draw'))\n    posterior_predictive_hdi_normalized = az.hdi(forecast['temperature_normalized_fut'], hdi_prob=0.8)\n    posterior_predictive_mean = posterior_predictive_mean_normalized * model.idata.attrs['y_std'] + model.idata.attrs['y_mean']\n    posterior_predictive_hdi = posterior_predictive_hdi_normalized * model.idata.attrs['y_std'] + model.idata.attrs['y_mean']\n\n    fig = go.Figure()\n    fig.add_traces(\n        [\n            go.Scatter(\n                name=\"\", \n                x=city_training_dataset[\"date\"], \n                y=hdi[\"temperature_normalized\"][:, 1], \n                mode=\"lines\", \n                marker=dict(color=\"#eb8c34\"), \n                line=dict(width=0), \n                legendgroup=\"HDI\",\n                showlegend=False\n            ),\n            go.Scatter(\n                name=\"80% HDI\", \n                x=city_training_dataset[\"date\"], \n                y=hdi[\"temperature_normalized\"][:, 0], \n                mode=\"lines\", marker=dict(color=\"#eb8c34\"), \n                line=dict(width=0), \n                legendgroup=\"HDI\", \n                fill='tonexty', \n                fillcolor='rgba(235, 140, 52, 0.5)'\n            ),\n            go.Scatter(\n                x = city_training_dataset[\"date\"],\n                y = city_training_dataset[\"temperature\"],\n                mode=\"markers\",\n                marker_color=\"#48bbf0\",\n                name=\"actuals\",\n                legendgroup=\"actuals\"\n            ),\n            go.Scatter(\n                x = city_training_dataset[\"date\"],\n                y = posterior_mean,\n                marker_color=\"blue\",\n                name=\"posterior_mean\",\n                legendgroup=\"posterior_mean\"\n            ),\n            go.Scatter(\n                name=\"\", \n                x=city_testing_dataset[\"date\"], \n                y=posterior_predictive_hdi[\"temperature_normalized_fut\"][:, 1], \n                mode=\"lines\", \n                marker=dict(color=\"#eb8c34\"), \n                line=dict(width=0), \n                legendgroup=\"HDI\",\n                showlegend=False\n            ),\n            go.Scatter(\n                name=\"\", \n                x=city_testing_dataset[\"date\"], \n                y=posterior_predictive_hdi[\"temperature_normalized_fut\"][:, 0], \n                mode=\"lines\", marker=dict(color=\"#eb8c34\"), \n                line=dict(width=0), \n                legendgroup=\"HDI\", \n                fill='tonexty', \n                fillcolor='rgba(235, 140, 52, 0.5)',\n                showlegend=False\n            ),\n            go.Scatter(\n                x = city_testing_dataset[\"date\"],\n                y = city_testing_dataset[\"temperature\"],\n                mode=\"markers\",\n                marker_color=\"#48bbf0\",\n                name=\"\",\n                legendgroup=\"actuals\",\n                showlegend=False\n            ),\n            go.Scatter(\n                x = city_testing_dataset[\"date\"],\n                y = posterior_predictive_mean,\n                mode=\"lines\",\n                marker_color=\"yellow\",\n                name=\"\",\n                legendgroup=\"posterior_mean\",\n                showlegend=False\n            ),\n        ]\n    )\n    fig.update_layout(\n        title = f\"{city.title()} Temperature Forecast\",\n        xaxis=dict(\n                title=\"Date\",\n                rangeselector=dict(\n                    buttons=list(\n                        [\n                            dict(count=1, label=\"1y\", step=\"year\", stepmode=\"backward\"),\n                            dict(count=5, label=\"5y\", step=\"year\", stepmode=\"backward\"),\n                            dict(count=10, label=\"10y\", step=\"year\", stepmode=\"backward\"),\n                            dict(step=\"all\", label=\"All\"),\n                        ]\n                    )\n                ),\n                rangeslider=dict(visible=True),\n                type=\"date\",\n                rangeselector_font_color=\"black\",\n                rangeselector_activecolor=\"hotpink\",\n                rangeselector_bgcolor=\"lightblue\",\n                autorangeoptions=dict(clipmax=city_testing_dataset['date'].max() + timedelta(days=30), clipmin=city_training_dataset['date'].min() - timedelta(days=30))\n            ),\n        yaxis=dict(\n            title=\"Temperature\"\n        )\n    )\n    return fig\nLet’s make sure we define the outputs from our node so that we persist the figures to disk.\n# Forecast plots: This is a dataset factory that will save all of our figures\n\"{city}_plot\":\n  type: plotly.JSONDataset\n  filepath: data/08_reporting/{city}_plot.json\nNext, we add the node to our pipeline.\n# Add to /src/climate_brazil/pipelines/ML/pipeline.py\nplot_node = [\n    node(\n        func=forecast_plot,\n        inputs=dict(\n            training_dataset=\"training_dataset\",\n            testing_dataset=\"testing_dataset\",\n            model=f\"{city}_model\",\n            forecast=f\"{city}_forecasts_evaluation\"\n        ),\n        outputs=f\"{city}_plot\",\n        name=f\"{city}_forecast_plot\",\n        tags=[\"train\", \"forecast\"]\n    )\n    for city in cities\n]\n\nreturn pipeline(\n    split_train_nodes \n    + forecast_training_nodes \n    + forecast_nodes \n    + plot_node\n)\nGreat! Now, let’s only execute our forecast_plot node since we’ve already ran the pipeline that generates our models and forecasts.\nkedro run --nodes forecast_plot\nThat will generate the following figures:\n\n\n\nBelemCuritibaFortalezaGoianiaMacapaManausRecifeRioSalvadorSao LuizSao PauloVitoria\n\n\n\n\n                                                \n\n\n\n\n                                                \n\n\n\n\n\n\n                                                \n\n\n\n\n                                                \n\n\n\n\n\n\n                                                \n\n\n\n\n                                                \n\n\n\n\n\n\n                                                \n\n\n\n\n                                                \n\n\n\n\n\n\n                                                \n\n\n\n\n                                                \n\n\n\n\n\n\n                                                \n\n\n\n\n                                                \n\n\n\n\n\n\n                                                \n\n\n\n\n                                                \n\n\n\n\n\n\n                                                \n\n\n\n\n                                                \n\n\n\n\n\n\n                                                \n\n\n\n\n                                                \n\n\n\n\n\n\n                                                \n\n\n\n\n                                                \n\n\n\n\n\n\n                                                \n\n\n\n\n                                                \n\n\n\n\n\n\n                                                \n\n\n\n\n                                                \n\n\n\n\n\nLooks good! We now have our training and forecasting pipelines up and running."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to My Blog",
    "section": "",
    "text": "Reproducibility & Scalability Part 2PyMC ModelBuilder\n\n\n\n\n\n\nEngineering\n\n\nAnalytics\n\n\n\n\n\n\n\n\n\nApr 2, 2025\n\n\nJonathan Dekermanjian\n\n\n\n\n\n\n\n\n\n\n\n\nReproducibility & Scalability Part 1The Kedro Framework\n\n\n\n\n\n\nEngineering\n\n\n\n\n\n\n\n\n\nMar 26, 2025\n\n\nJonathan Dekermanjian\n\n\n\n\n\n\nNo matching items"
  }
]