[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Welcome to my blog!\nMy name is Jonathan Dekermanjian and I am a data professional with nearly 10 years of experience. I have worked both in academia as a data science researcher as well as in industry as a data scientist & machine learning engineer.\nIf you’d like to learn more about me please click on the LinkedIn link below!\nThe purpose of this blog is to talk about interesting advanced analytics and engineering topics that will benefit others who are also data professionals."
  },
  {
    "objectID": "posts/kedro-framework/index.html",
    "href": "posts/kedro-framework/index.html",
    "title": "Reproducibility & Scalability Part 1The Kedro Framework",
    "section": "",
    "text": "Overview\nIn part one of a four-part series, we are going to set up and walkthrough the Kedro framework. Kedro is an open-source Python framework that provides you with scaffolding to ensure your data projects adhere to software-engineering best practices. In turn, this makes your project reproducible and robust to requirement changes.\n\n\nIntroduction\nIn this post we are going to talk about and walkthrough using the Kedro framework. To keep things simple, we are going to use a clean dataset provided by the National Oceanic and Atmospheric Association (NOAA) that measures the monthly temperature changes across twelve Brazilian cities. You can download the dataset off of Kaggle. We are going to keep things as simple as possible and only go over the key components of a Kedro project which include:\n\nCreating a Kedro project\nDefining a data catalog\nCreating a Kedro pipeline\nIntegrating Kedro plugins\n\nIf you want to learn how to use Kedro’s more advanced features check out the official documention.\n\n\nGetting Started\nLet’s start off by creating a clean environment. We are going to use Conda in this example but you can you whatever virtual environment you prefer. After installing Anaconda or Miniconda, you can create a new environment by executing the following command in your terminal:\n# Creates a new virtual environment\nconda create -c conda-forge -n my_new_environment python=3.12 -y\nThe argument -c conda-forge tells conda to put the conda-forge repository at a higher priority over the default repository. We also installed python version 3.12 in this new environment. Now that we have a clean environment we need to install Kedro into that environment. Activate your new environment and install Kedro with:\n# Installs kedro\npip install -U kedro==0.19.11\nYou can verify your installation by executing:\n# Displays installed kedro version and extensions\nkedro info\n\n\nCreating a New Kedro Project\nNow that we have Kedro installed, we need to create a new Kedro project. You can use the Kedro command-line interface (CLI):\n# Creates a new Kedro project\nkedro new\nYou will be prompted to pick a name for your project. Let’s call our project Climate-Brazil. Next, you will be prompted to select the Kedro tools you want to utilize. For this project, let’s go with 1, 3, 5, 7 which corresponds to Linting, Logging, Data Folder, and Kedro-Viz. Once you have created your project, you’ll notice that Kedro created a project directory, using the project name we selected, with subdirectories and files that serve as the scaffolding of your project.\n\n\n\n\n\n\nNote\n\n\n\nNote that there are some files in the image below that you won’t have at this moment. These will be added/generated as you continue through this walkthrough.\n\n\n\n\n\nKedro Project Structure\n\n\n\n\nKedro Project Structure\nLet’s briefly walkthrough the 5 subdirectories that Kedro created for us.\n\nconf: This directory contains your project configurations. These can be things from API/Database credentials to Model parameters. This is also where you can create multiple project environments like Dev and Prod.\ndata: This directory is where you will store your data and your project artifacts.\nnotebooks: This directory is where you can interactively develop your project before creating modular nodes and pipelines\nsrc: This directory is where you will put your source code defined as nodes and chained together into pipelines\ntests: This directory is where you will write your unit tests\n\nI know that was really brief but don’t worry we will learn more specifics as we develop the project.\n\n\nDeclare Project Dependencies\nNow that we have a project created, let’s add any packages that our project will depend on. In the root of our project directory Kedro created a requirements.txt file for us. Kedro will add some dependencies into the file by default but we will need some additional packages for our project. We are going to need polars, plotly, and pre-commit so let’s modify requirements.txt to contain the following:\n# ./requirements.txt\nipython&gt;=8.10\njupyterlab&gt;=3.0\nkedro-datasets&gt;=3.0\nkedro-viz&gt;=6.7.0\nkedro[jupyter]~=0.19.11\nnotebook\npre-commit~=3.8.0\npolars~=1.22.0\npyarrow~=19.0.1\nplotly~=5.24.1\nopenpyxl~=3.1.5\nNow that we have declared our dependencies, let’s go ahead and install them into our environment.\npip install -r requirements.txt\n\n\nDefining a Data Catalog\nAll Input/Output (IO) operations are defined in a data catalog, when using Kedro. This allows you to declaratively define your data sources whether they are stored as local files, or stored remotely in a SQL/NoSQL database, or just in some form of remote storage like S3.\n\n\n\n\n\n\nTip\n\n\n\nIn our simple example, all the data are stored locally. Therefore, we don’t need to define any credentials to access the data. However, in practice it is likely that you need to access data from a database or cloud storage. In that case, you would define your credentials in /conf/local.credentials.yml.\n\n\nBefore we edit our catalog, go ahead and download the data to /data/01_raw/. You can get the data files from Kaggle.\nWe declare our base catalog by modifying /conf/base/catalog.yml. The base catalog will be inhereted by all other Kedro enviornments you create. For example, if you create a dev environment you don’t need to repeat the catalog entries that are in /conf/base/catalog.yml inside of /conf/dev/catalog.yml.\n\n\n\n\n\n\nCaution\n\n\n\nIf you declare a catalog entry in /conf/dev/catalog.yml that shares the same key as an entry in /conf/base/catalog.yml you will override the base catalog when you are working in the dev environment.\n\n\nThere are several ways that we can add our data to the catalog. The simplest way is to define one entry for each Brazilian city:\n#/conf/base/catalog.yml\nbelem_dataset:\n  type: polars.LazyPolarsDataset\n  file_format: csv\n  filepath: data/01_raw/station_belem.csv\n\ncuritiba_dataset:\n  type: polars.LazyPolarsDataset\n  file_format: csv\n  filepath: data/01_raw/station_curitiba.csv\n\n# Rest of the cities follow-suit\nIf you have a lot of cities it can get really tedious. Luckily, Kedro has a feature called Dataset Factories that assist you in reducing repeated code in your data catalog. We can declare all of our cities with one block like this:\n#/conf/base/catalog.yml\n# Dataset Factory\n\"{city}_dataset\":\n  type: polars.LazyPolarsDataset\n  file_format: csv\n  filepath: data/01_raw/station_{city}.csv\nKedro is smart enough that during runtime it will use Regex to match {city} to the corresponding dataset city. Finally, the approach we will use is to declare the data as a PartitionedDataset. You can think of each Brazilian city as a partition of the entire dataset that would be composed of all the Brazilian cities. Kedro will return a dictionary object with each file’s name as the key and the load method as the corresponding value.\n\n\n\n\n\n\nImportant\n\n\n\nNote that when using a PartitionedDataset the data is loaded lazily. This means that the data is actually not loaded until you call the load method. This prevents Out-Of-Memory (OOM) errors if your data can’t fit into memory.\n\n\nWe can declare our data as a PartitionedDataset like this:\n# /conf/base/catalog.yml\ncities_temperatures:\n  type: partitions.PartitionedDataset\n  path: data/01_raw/\n  dataset: \n    type: polars.LazyPolarsDataset\n    file_format: csv\nNow we can load our dataset by simply starting up a Kedro session (Kedro does this for you) and calling catalog.load(\"cities_temperatures\"). You can even work with the data interactively in a jupyter notebook. Kedro automatically loads the session and context when you run kedro jupyter lab on the command-line. This means that once the jupyter kernel is running you already have your catalog object loaded in your environment.\n\n\nCreating a New Kedro Pipeline\nTypically, once you have defined your data catalog you’d want to work interactively on your cleaning and wrangling your data into a usable format. As mentioned earlier you can do that using kedor jupyter lab. Once you have your processing/modeling/reporting code in check you should write modular functions that when chained together would constitute a pipeline. That could be a data processing pipeline, a machine learning pipeline, a monitoring pipeline, etc… This is where Kedro pipelines come in to play. We can create a new pipeline using the Kedro CLI by executing:\nkedro pipeline create data_processing\nKedro will create the structure for your new pipeline in /src/climate_brazil/pipelines/data_processing. We will define all your modular code inside of /src/climate_brazil/pipelines/data_processing/nodes.py and then we will declare our pipeline inside /src/climate_brazil/pipelines/data_processing/pipeline.py. Let’s start with the nodes.\nEach of our Brazilian cities dataset is in a wide format where we have years across the rows and the months within that year across the columns. We need to merge all of our cities together and unpivot the data so that we are in long format where both year and month are across the rows. The following function does what we just described:\n# /src/climate_brazil/pipelines/data_processing/nodes.py\ndef process_datasets(partitioned_dataset: dict) -&gt; pl.DataFrame:\n    \"\"\"\n    Combine all cities into one dataframe and unpivot the data into long format\n    ---\n    params:\n        partitioned_dataset: Our data partiotioned into key (filename) value(load method) pairs\n    \"\"\"\n    # Missing values encoding\n    mv_code = 999.9\n\n    # Add a city column to each partiotion so that when we merge them all together we can identify each city\n    datasets = [\n        v().with_columns(city=pl.lit(re.findall(r\"(?&lt;=_).*(?=\\.)\", k)[0]))\n        for k, v in partitioned_dataset.items()\n    ]\n\n    df_merged = pl.concat(datasets)\n\n    df_processed = (\n        df_merged.drop(\"D-J-F\", \"M-A-M\", \"J-J-A\", \"S-O-N\", \"metANN\")\n        .rename({\"YEAR\": \"year\"})\n        .collect()  # Need to collect because can't unpivot a lazyframe\n        .unpivot(\n            on=[\n                \"JAN\",\n                \"FEB\",\n                \"MAR\",\n                \"APR\",\n                \"MAY\",\n                \"JUN\",\n                \"JUL\",\n                \"AUG\",\n                \"SEP\",\n                \"OCT\",\n                \"NOV\",\n                \"DEC\",\n            ],\n            index=[\"city\", \"year\"],\n            variable_name=\"month\",\n            value_name=\"temperature\",\n        )\n        .with_columns(\n            pl.col(\"month\")\n            .str.to_titlecase()\n            .str.strptime(dtype=pl.Date, format=\"%b\")\n            .dt.month()\n        )\n        .with_columns(\n            date=pl.date(year=pl.col(\"year\"), month=pl.col(\"month\"), day=1),\n        )\n        .with_columns(\n            pl.when(\n                pl.col(\"temperature\")\n                == mv_code  # This is how missing data is coded in the dataset\n            )\n            .then(None)\n            .otherwise(pl.col(\"temperature\"))\n            .name.keep(),\n            pl.col(\"city\").str.to_titlecase(),\n        )\n        .drop(\"year\", \"month\")\n    )\n    return df_processed\nLet’s also define a function that will plot our time series data.\ndef timeseries_plot(processed_dataframe: pl.DataFrame) -&gt; go.Figure:\n    \"\"\"\n    Plots each Brazilian city temperature time series\n    \"\"\"\n    fig = go.Figure()\n    for city in processed_dataframe.select(\"city\").unique(maintain_order=True).to_series():\n        fig.add_trace(\n            go.Scatter(\n                x = processed_dataframe.filter(pl.col(\"city\")==city).sort('date')['date'],\n                y = processed_dataframe.filter(pl.col(\"city\")==city).sort('date')['temperature'],\n                name = city,\n                hovertemplate=\"&lt;b&gt;Date&lt;/b&gt;: %{x}&lt;br&gt;&lt;b&gt;Temperature&lt;/b&gt;: %{y}\"\n            )\n        )\n    fig.update_layout(\n        title = \"Temperature Measurements of Brazilian Cities\",\n        xaxis=dict(\n        title = \"Date\",\n        rangeselector=dict(\n            buttons=list([\n                dict(count=1,\n                     label=\"1y\",\n                     step=\"year\",\n                     stepmode=\"backward\"),\n                dict(count=5,\n                     label=\"5y\",\n                     step=\"year\",\n                     stepmode=\"backward\"),\n                dict(count=10,\n                     label=\"10y\",\n                     step=\"year\",\n                     stepmode=\"backward\"),\n                dict(step=\"all\", label=\"All\")\n            ])\n        ),\n        rangeslider=dict(\n            visible=True\n        ),\n        type=\"date\",\n        rangeselector_font_color='black',\n        rangeselector_activecolor='hotpink',\n        rangeselector_bgcolor='lightblue',\n    ),\n        yaxis=dict(\n            title = \"Temperature in Celsius\"\n        )\n    )\n    return fig\nThis will produce the following figure:\n\n\n                                                \n\n\n\n\n                                                \n\n\nNow that we have defined our nodes, let’s see how we can chain them together into a simple pipeline. We define our data processing pipeline in the pipeline.py file that Kedro creates for us in the data_processing directory.\n# /src/climate_brazil/pipelines/data_processing/pipeline.py\nfrom kedro.pipeline import node, Pipeline, pipeline  # noqa\nfrom .nodes import process_datasets, timeseries_plot\n\ndef create_pipeline(**kwargs) -&gt; Pipeline:\n    return pipeline([\n        node(\n            func=process_datasets,\n            inputs=dict(partitioned_dataset = \"cities_temperatures\"),\n            outputs=\"cities_temperatures_processed\",\n            name=\"process_datasets\"\n        ),\n        node(\n            func=timeseries_plot,\n            inputs=dict(processed_dataframe = \"cities_temperatures_processed\"),\n            outputs=\"cities_ts_plot\",\n            name=\"timeseries_plot\"\n        ),\n    ])\nNotice that the input to the process_datasets function is the name that we chose for our dataset when we defined it inside of our catalog. Also note that we are choosing to name the output from the process_datasets function as cities_temperatures_processed and we are passing that as the input to the function timeseries_plot.\n\n\n\n\n\n\nCaution\n\n\n\nKedro automatically infers the dependencies of your pipeline and will run it in an order that may not be the same order as you defined.\n\n\nBefore we move on to the next section, it is important to know that both our outputs will be MemoryDatasets this mean that they will only exist while the Kedro session is still active. In order to persist the outputs we need to add them to our catalog.\n# conf/base/catalog.yml\n# Raw data ---------------------------------------\ncities_temperatures:\n  type: partitions.PartitionedDataset\n  path: data/01_raw/\n  dataset: \n    type: polars.LazyPolarsDataset\n    file_format: csv\n\n# Processed data ---------------------------------------\ncities_temperatures_processed:\n  type: polars.EagerPolarsDataset\n  file_format: parquet\n  filepath: data/02_intermediate/cities_temperatures_processed.pq\n\ncities_ts_plot:\n  type: plotly.JSONDataset\n  filepath: data/08_reporting/cities_ts_plot.json\nWe are going to store our processed data locally as a parquet file and our figure as a json file. Great, we are now ready to run our pipeline!\n\n\nRunning a Kedro Pipeline\nTo run the data processing pipeline we just defined you can execute:\nkedro run\nThat will run all your defined pipelines in the order that you defined them and since we only have one pipeline you will run the data processing pipeline. Okay, so how do we run a specific pipeline? For that you can use:\nkedro run --pipeline data_processing\nKedro also allows you to run specific nodes. For example, if we just wanted to process the data without generating a plot we could run:\nkedro run --nodes process_datasets\nWhere process_datasets is the name we gave the node when we defined the pipeline.\nKedro is very flexible and allows you to run in a variety of options. You can checkout the Kedro run documentation for more information.\n\n\nPipeline Graphs with Kedro-Viz\nWhen we created a new Kedro project earlier, we told kedro to add to our dependencies the kedro-viz plugin. This plugin allows you to visualize your pipeline or pipelines as a graph. To view the simple pipeline we built earlier you can execute at the command-line the following:\nkedro viz run\nYour browser should automatically launch and you will be greeted by the following screen.\n\n\n\nKedro-Viz Pipeline Graph\n\n\nOur simple example doesn’t do Kedro-Viz justice. In practice, you’ll have many datasets coming from disparate sources that will require more complex processing and joining. In such a scenario, being able to see the lineage and dependencies of your data becomes very useful. Kedro-Viz is interactive in that you can optionally preview the data, you can view static or interactive figures, and you can view the code of the nodes all in the user interface. I recommend that you try this plugin for you next project!\n\n\nSource Control & Pre-Commit\nI am sure that everyone reading this already understands the importance of source/version control, so I will keep this breif. When we created our project Kedro was nice enough to create a .gitignore file and .gitkeep files. The .gitignore file makes sure that you don’t accidentally commit any data or any credentials that you store in conf/local/credentials.yml to a remote repository. Kedro does not, unfortunately, set up any pre-commit configuration so you need to do that manually. Here is an example of a pre-commit configuration that includes linting with ruff checking your codebase for missing docstrings with interrogate and stripping out any confidentiat data from notebooks with nbstripout.\n# .pre-commit-config.yaml\nrepos:\n  - repo: https://github.com/astral-sh/ruff-pre-commit\n    # Ruff version.\n    rev: v0.11.2\n    hooks:\n      # Run the linter.\n      - id: ruff\n        types_or: [ python, pyi ]\n        args: [ --fix ]\n      # Run the formatter.\n      - id: ruff-format\n        types_or: [ python, pyi ]\n  - repo: https://github.com/econchick/interrogate\n    rev: 1.7.0\n    hooks:\n    - id: interrogate\n      args: [--config=pyproject.toml]\n      pass_filenames: false\n  - repo: https://github.com/kynan/nbstripout\n    rev: 0.7.1\n    hooks:\n      - id: nbstripout\nYou also need to specify interrogate’s configuration within pyproject.toml you can add the following at the bottom of your file:\n[tool.interrogate]\nignore-init-method = false\nignore-init-module = false\nignore-magic = false\nignore-semiprivate = false\nignore-private = false\nignore-property-decorators = false\nignore-module = false\nignore-nested-functions = false\nignore-nested-classes = false\nignore-setters = false\nignore-overloaded-functions = false\nfail-under = 80\nexclude = [\"tests\", \"docs\", \"build\", \"src/climate_brazil/__main__.py\"]\next = []\nverbose = 2\nquiet = false\nwhitelist-regex = []\ncolor = true\nomit-covered-files = false\nAfter you’re done setting up your configuration you need to initialize a local git repository and install your pre-commit configurations.\ngit init\npre-commit install\nWonderul, we are all set now with source control and mitigating the possibility of commiting anything confidential to your repository.\n\n\nRunning Pipelines in Containers\nWe can also run the pipeline we just built inside of a container. Kedro maintains the kedro-docker plugin which facilitates getting your Kedro project running inside a container.\n\n\n\n\n\n\nNote\n\n\n\nWhile the plugin is named kedro-docker you can use it with other containerization frameworks such as Podman\n\n\nFirst, we need to install the plugin. You can add the following to your ./requirements.txt file:\nkedro-docker~=0.6.2\nThen execute:\npip install kedro-docker~=0.6.2\nWith the plugin installed, Kedro will generate a Dockerfile, .dockerignore, and .dive-ci that corresponds to your Kedro project by executing:\nkedro docker init\n\n\n\n\n\n\nCaution\n\n\n\nMake sure you Docker Engine is running otherwise the previous step will fail.\n\n\nLet’s take a look at the generated Dockerfile:\n#./Dockerfile\nARG BASE_IMAGE=python:3.9-slim\nFROM $BASE_IMAGE as runtime-environment\n\n# update pip and install uv\nRUN python -m pip install -U \"pip&gt;=21.2\"\nRUN pip install uv\n\n# install project requirements\nCOPY requirements.txt /tmp/requirements.txt\nRUN uv pip install --system --no-cache-dir -r /tmp/requirements.txt && rm -f /tmp/requirements.txt\n\n# add kedro user\nARG KEDRO_UID=999\nARG KEDRO_GID=0\nRUN groupadd -f -g ${KEDRO_GID} kedro_group && \\\n    useradd -m -d /home/kedro_docker -s /bin/bash -g ${KEDRO_GID} -u ${KEDRO_UID} kedro_docker\n\nWORKDIR /home/kedro_docker\nUSER kedro_docker\n\nFROM runtime-environment\n\n# copy the whole project except what is in .dockerignore\nARG KEDRO_UID=999\nARG KEDRO_GID=0\nCOPY --chown=${KEDRO_UID}:${KEDRO_GID} . .\n\nEXPOSE 8888\n\nCMD [\"kedro\", \"run\"]\nKedro will automatically assume that you want to run all your pipelines (or your defualt pipeline) but you can, quite easily, change the specified command in the generated docker file.\nNow that we have a Dockerfile and .dockerignore we can build our image.\nkedro docker build\nFinally, you can run your pipeline from within the container by either using your normal docker-cli commands or you can use the kedro-docker plugin and execute:\nkedro docker run\n\n\nSummary\nIn this post we walked through using the Kedro framework and learned how to follow software engineering best practices that ensure that your projects are reproducible, modular, and easy to maintain as a project matures. We learned about the data catalog, how to define nodes and subsequently linking nodes together to build pipelines. We then looked at a couple Kedro plugins like kedro-viz and kedro-docker that expanded the functionality of our project. We also talked about and walked through good practices to follow when implementing source control with git and pre-commit. All this in just part one of our series! We have a long ways to go still but I hope you are excited for what comes next.\n\n\nComing Next\nIf you use a popular/mainstream machine learning framework like PyTorch, TensorFlow, Scikit-Learn, or XGBoost then reproducibility and scalability are quite easy because you’ll typically find that most MLOPs and distributed frameworks natively support these tools. What do you do if you have a custom solution? Let’s say you are using a probabilistics programming language like Stan or PyMC and there isn’t native support for these tools. Well, that is what we are going to do in the following parts of this series. In part two we will fit a time series model using PyMC and talk about how to use ModelBuilder from pymc-extras to build a production ready model. I hope to see you there!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to My Blog",
    "section": "",
    "text": "Reproducibility & Scalability Part 1The Kedro Framework\n\n\n\n\n\n\nEngineering\n\n\n\n\n\n\n\n\n\nMar 26, 2025\n\n\nJonathan Dekermanjian\n\n\n\n\n\n\nNo matching items"
  }
]