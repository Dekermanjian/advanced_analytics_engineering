[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Welcome to my blog!\nMy name is Jonathan Dekermanjian and I am a data professional with nearly 10 years of experience. I have worked both in academia as a data science researcher as well as in industry as a data scientist & machine learning engineer.\nIf you’d like to learn more about me please click on the LinkedIn link below!\nThe purpose of this blog is to talk about interesting advanced analytics and engineering topics that will benefit others who are also data professionals."
  },
  {
    "objectID": "posts/kedro-framework/index.html",
    "href": "posts/kedro-framework/index.html",
    "title": "Reproducibility & Scalability Part 1The Kedro Framework",
    "section": "",
    "text": "Overview\nIn part one of a four-part series, we are going to set up and walkthrough the Kedro framework. Kedro is an open-source Python framework that provides you with scaffolding to ensure your data projects adhere to software-engineering best practices. In turn, this makes your project reproducible and robust to requirement changes.\n\n\nIntroduction\nIn this post we are going to talk about and walkthrough using the Kedro framework. To keep things simple, we are going to use a clean dataset provided by the National Oceanic and Atmospheric Association (NOAA) that measures the monthly temperature changes across twelve Brazilian cities. You can download the dataset off of Kaggle. We are going to keep things as simple as possible and only go over the key components of a Kedro project which include:\n\nCreating a Kedro project\nDefining a data catalog\nCreating a Kedro pipeline\nIntegrating Kedro plugins\n\nIf you want to learn how to use Kedro’s more advanced features check out the official documention.\n\n\nGetting Started\nLet’s start off by creating a clean environment. We are going to use Conda in this example but you can you whatever virtual environment you prefer. After installing Anaconda or Miniconda, you can create a new environment by executing the following command in your terminal:\n# Creates a new virtual environment\nconda create -c conda-forge -n my_new_environment python=3.12 -y\nThe argument -c conda-forge tells conda to put the conda-forge repository at a higher priority over the default repository. We also installed python version 3.12 in this new environment. Now that we have a clean environment we need to install Kedro into that environment. Activate your new environment and install Kedro with:\n# Installs kedro\npip install -U kedro==0.19.11\nYou can verify your installation by executing:\n# Displays installed kedro version and extensions\nkedro info\n\n\nCreating a New Kedro Project\nNow that we have Kedro installed, we need to create a new Kedro project. You can use the Kedro command-line interface (CLI):\n# Creates a new Kedro project\nkedro new\nYou will be prompted to pick a name for your project. Let’s call our project Climate-Brazil. Next, you will be prompted to select the Kedro tools you want to utilize. For this project, let’s go with 1, 3, 5, 7 which corresponds to Linting, Logging, Data Folder, and Kedro-Viz. Once you have created your project, you’ll notice that Kedro created a project directory, using the project name we selected, with subdirectories and files that serve as the scaffolding of your project.\n\n\n\n\n\n\nNote\n\n\n\nNote that there are some files in the image below that you won’t have at this moment. These will be added/generated as you continue through this walkthrough.\n\n\n\n\n\nKedro Project Structure\n\n\n\n\nKedro Project Structure\nLet’s briefly walkthrough the 5 subdirectories that Kedro created for us.\n\nconf: This directory contains your project configurations. These can be things from API/Database credentials to Model parameters. This is also where you can create multiple project environments like Dev and Prod.\ndata: This directory is where you will store your data and your project artifacts.\nnotebooks: This directory is where you can interactively develop your project before creating modular nodes and pipelines\nsrc: This directory is where you will put your source code defined as nodes and chained together into pipelines\ntests: This directory is where you will write your unit tests\n\nI know that was really brief but don’t worry we will learn more specifics as we develop the project.\n\n\nDeclare Project Dependencies\nNow that we have a project created, let’s add any packages that our project will depend on. In the root of our project directory Kedro created a requirements.txt file for us. Kedro will add some dependencies into the file by default but we will need some additional packages for our project. We are going to need polars, plotly, and pre-commit so let’s modify requirements.txt to contain the following:\n# ./requirements.txt\nipython&gt;=8.10\njupyterlab&gt;=3.0\nkedro-datasets&gt;=3.0\nkedro-viz&gt;=6.7.0\nkedro[jupyter]~=0.19.11\nnotebook\npre-commit~=3.8.0\npolars~=1.22.0\npyarrow~=19.0.1\nplotly~=5.24.1\nopenpyxl~=3.1.5\nNow that we have declared our dependencies, let’s go ahead and install them into our environment.\npip install -r requirements.txt\n\n\nDefining a Data Catalog\nAll Input/Output (IO) operations are defined in a data catalog, when using Kedro. This allows you to declaratively define your data sources whether they are stored as local files, or stored remotely in a SQL/NoSQL database, or just in some form of remote storage like S3.\n\n\n\n\n\n\nTip\n\n\n\nIn our simple example, all the data are stored locally. Therefore, we don’t need to define any credentials to access the data. However, in practice it is likely that you need to access data from a database or cloud storage. In that case, you would define your credentials in /conf/local/credentials.yml.\n\n\nBefore we edit our catalog, go ahead and download the data to /data/01_raw/. You can get the data files from Kaggle.\nWe declare our base catalog by modifying /conf/base/catalog.yml. The base catalog will be inhereted by all other Kedro enviornments you create. For example, if you create a dev environment you don’t need to repeat the catalog entries that are in /conf/base/catalog.yml inside of /conf/dev/catalog.yml.\n\n\n\n\n\n\nCaution\n\n\n\nIf you declare a catalog entry in /conf/dev/catalog.yml that shares the same key as an entry in /conf/base/catalog.yml you will override the base catalog when you are working in the dev environment.\n\n\nThere are several ways that we can add our data to the catalog. The simplest way is to define one entry for each Brazilian city:\n#/conf/base/catalog.yml\nbelem_dataset:\n  type: polars.LazyPolarsDataset\n  file_format: csv\n  filepath: data/01_raw/station_belem.csv\n\ncuritiba_dataset:\n  type: polars.LazyPolarsDataset\n  file_format: csv\n  filepath: data/01_raw/station_curitiba.csv\n\n# Rest of the cities follow-suit\nIf you have a lot of cities it can get really tedious. Luckily, Kedro has a feature called Dataset Factories that assist you in reducing repeated code in your data catalog. We can declare all of our cities with one block like this:\n#/conf/base/catalog.yml\n# Dataset Factory\n\"{city}_dataset\":\n  type: polars.LazyPolarsDataset\n  file_format: csv\n  filepath: data/01_raw/station_{city}.csv\nKedro is smart enough that during runtime it will use Regex to match {city} to the corresponding dataset city. Finally, the approach we will use is to declare the data as a PartitionedDataset. You can think of each Brazilian city as a partition of the entire dataset that would be composed of all the Brazilian cities. Kedro will return a dictionary object with each file’s name as the key and the load method as the corresponding value.\n\n\n\n\n\n\nImportant\n\n\n\nNote that when using a PartitionedDataset the data is loaded lazily. This means that the data is actually not loaded until you call the load method. This prevents Out-Of-Memory (OOM) errors if your data can’t fit into memory.\n\n\nWe can declare our data as a PartitionedDataset like this:\n# /conf/base/catalog.yml\ncities_temperatures:\n  type: partitions.PartitionedDataset\n  path: data/01_raw/\n  dataset: \n    type: polars.LazyPolarsDataset\n    file_format: csv\nNow we can load our dataset by simply starting up a Kedro session (Kedro does this for you) and calling catalog.load(\"cities_temperatures\"). You can even work with the data interactively in a jupyter notebook. Kedro automatically loads the session and context when you run kedro jupyter lab on the command-line. This means that once the jupyter kernel is running you already have your catalog object loaded in your environment.\n\n\nCreating a New Kedro Pipeline\nTypically, once you have defined your data catalog you’d want to work interactively on your cleaning and wrangling your data into a usable format. As mentioned earlier you can do that using kedor jupyter lab. Once you have your processing/modeling/reporting code in check you should write modular functions that when chained together would constitute a pipeline. That could be a data processing pipeline, a machine learning pipeline, a monitoring pipeline, etc… This is where Kedro pipelines come in to play. We can create a new pipeline using the Kedro CLI by executing:\nkedro pipeline create data_processing\nKedro will create the structure for your new pipeline in /src/climate_brazil/pipelines/data_processing. We will define all your modular code inside of /src/climate_brazil/pipelines/data_processing/nodes.py and then we will declare our pipeline inside /src/climate_brazil/pipelines/data_processing/pipeline.py. Let’s start with the nodes.\nEach of our Brazilian cities dataset is in a wide format where we have years across the rows and the months within that year across the columns. We need to merge all of our cities together and unpivot the data so that we are in long format where both year and month are across the rows. The following function does what we just described:\n# /src/climate_brazil/pipelines/data_processing/nodes.py\ndef process_datasets(partitioned_dataset: dict) -&gt; pl.DataFrame:\n    \"\"\"\n    Combine all cities into one dataframe and unpivot the data into long format\n    ---\n    params:\n        partitioned_dataset: Our data partiotioned into key (filename) value(load method) pairs\n    \"\"\"\n    # Missing values encoding\n    mv_code = 999.9\n\n    # Add a city column to each partiotion so that when we merge them all together we can identify each city\n    datasets = [\n        v().with_columns(city=pl.lit(re.findall(r\"(?&lt;=_).*(?=\\.)\", k)[0]))\n        for k, v in partitioned_dataset.items()\n    ]\n\n    df_merged = pl.concat(datasets)\n\n    df_processed = (\n        df_merged.drop(\"D-J-F\", \"M-A-M\", \"J-J-A\", \"S-O-N\", \"metANN\")\n        .rename({\"YEAR\": \"year\"})\n        .collect()  # Need to collect because can't unpivot a lazyframe\n        .unpivot(\n            on=[\n                \"JAN\",\n                \"FEB\",\n                \"MAR\",\n                \"APR\",\n                \"MAY\",\n                \"JUN\",\n                \"JUL\",\n                \"AUG\",\n                \"SEP\",\n                \"OCT\",\n                \"NOV\",\n                \"DEC\",\n            ],\n            index=[\"city\", \"year\"],\n            variable_name=\"month\",\n            value_name=\"temperature\",\n        )\n        .with_columns(\n            pl.col(\"month\")\n            .str.to_titlecase()\n            .str.strptime(dtype=pl.Date, format=\"%b\")\n            .dt.month()\n        )\n        .with_columns(\n            date=pl.date(year=pl.col(\"year\"), month=pl.col(\"month\"), day=1),\n        )\n        .with_columns(\n            pl.when(\n                pl.col(\"temperature\")\n                == mv_code  # This is how missing data is coded in the dataset\n            )\n            .then(None)\n            .otherwise(pl.col(\"temperature\"))\n            .name.keep(),\n            pl.col(\"city\").str.to_titlecase(),\n        )\n        .drop(\"year\", \"month\")\n    )\n    return df_processed\nLet’s also define a function that will plot our time series data.\ndef timeseries_plot(processed_dataframe: pl.DataFrame) -&gt; go.Figure:\n    \"\"\"\n    Plots each Brazilian city temperature time series\n    \"\"\"\n    fig = go.Figure()\n    for city in processed_dataframe.select(\"city\").unique(maintain_order=True).to_series():\n        fig.add_trace(\n            go.Scatter(\n                x = processed_dataframe.filter(pl.col(\"city\")==city).sort('date')['date'],\n                y = processed_dataframe.filter(pl.col(\"city\")==city).sort('date')['temperature'],\n                name = city,\n                hovertemplate=\"&lt;b&gt;Date&lt;/b&gt;: %{x}&lt;br&gt;&lt;b&gt;Temperature&lt;/b&gt;: %{y}\"\n            )\n        )\n    fig.update_layout(\n        title = \"Temperature Measurements of Brazilian Cities\",\n        xaxis=dict(\n        title = \"Date\",\n        rangeselector=dict(\n            buttons=list([\n                dict(count=1,\n                     label=\"1y\",\n                     step=\"year\",\n                     stepmode=\"backward\"),\n                dict(count=5,\n                     label=\"5y\",\n                     step=\"year\",\n                     stepmode=\"backward\"),\n                dict(count=10,\n                     label=\"10y\",\n                     step=\"year\",\n                     stepmode=\"backward\"),\n                dict(step=\"all\", label=\"All\")\n            ])\n        ),\n        rangeslider=dict(\n            visible=True\n        ),\n        type=\"date\",\n        rangeselector_font_color='black',\n        rangeselector_activecolor='hotpink',\n        rangeselector_bgcolor='lightblue',\n    ),\n        yaxis=dict(\n            title = \"Temperature in Celsius\"\n        )\n    )\n    return fig\nThis will produce the following figure:\n\n\n                                                \n\n\n\n\n                                                \n\n\nNow that we have defined our nodes, let’s see how we can chain them together into a simple pipeline. We define our data processing pipeline in the pipeline.py file that Kedro creates for us in the data_processing directory.\n# /src/climate_brazil/pipelines/data_processing/pipeline.py\nfrom kedro.pipeline import node, Pipeline, pipeline  # noqa\nfrom .nodes import process_datasets, timeseries_plot\n\ndef create_pipeline(**kwargs) -&gt; Pipeline:\n    return pipeline([\n        node(\n            func=process_datasets,\n            inputs=dict(partitioned_dataset = \"cities_temperatures\"),\n            outputs=\"cities_temperatures_processed\",\n            name=\"process_datasets\"\n        ),\n        node(\n            func=timeseries_plot,\n            inputs=dict(processed_dataframe = \"cities_temperatures_processed\"),\n            outputs=\"cities_ts_plot\",\n            name=\"timeseries_plot\"\n        ),\n    ])\nNotice that the input to the process_datasets function is the name that we chose for our dataset when we defined it inside of our catalog. Also note that we are choosing to name the output from the process_datasets function as cities_temperatures_processed and we are passing that as the input to the function timeseries_plot.\n\n\n\n\n\n\nCaution\n\n\n\nKedro automatically infers the dependencies of your pipeline and will run it in an order that may not be the same order as you defined.\n\n\nBefore we move on to the next section, it is important to know that both our outputs will be MemoryDatasets this mean that they will only exist while the Kedro session is still active. In order to persist the outputs we need to add them to our catalog.\n# conf/base/catalog.yml\n# Raw data ---------------------------------------\ncities_temperatures:\n  type: partitions.PartitionedDataset\n  path: data/01_raw/\n  dataset: \n    type: polars.LazyPolarsDataset\n    file_format: csv\n\n# Processed data ---------------------------------------\ncities_temperatures_processed:\n  type: polars.EagerPolarsDataset\n  file_format: parquet\n  filepath: data/02_intermediate/cities_temperatures_processed.pq\n\ncities_ts_plot:\n  type: plotly.JSONDataset\n  filepath: data/08_reporting/cities_ts_plot.json\nWe are going to store our processed data locally as a parquet file and our figure as a json file. Great, we are now ready to run our pipeline!\n\n\nRunning a Kedro Pipeline\nTo run the data processing pipeline we just defined you can execute:\nkedro run\nThat will run all your defined pipelines in the order that you defined them and since we only have one pipeline you will run the data processing pipeline. Okay, so how do we run a specific pipeline? For that you can use:\nkedro run --pipeline data_processing\nKedro also allows you to run specific nodes. For example, if we just wanted to process the data without generating a plot we could run:\nkedro run --nodes process_datasets\nWhere process_datasets is the name we gave the node when we defined the pipeline.\nKedro is very flexible and allows you to run in a variety of options. You can checkout the Kedro run documentation for more information.\n\n\nPipeline Graphs with Kedro-Viz\nWhen we created a new Kedro project earlier, we told kedro to add to our dependencies the kedro-viz plugin. This plugin allows you to visualize your pipeline or pipelines as a graph. To view the simple pipeline we built earlier you can execute at the command-line the following:\nkedro viz run\nYour browser should automatically launch and you will be greeted by the following screen.\n\n\n\nKedro-Viz Pipeline Graph\n\n\nOur simple example doesn’t do Kedro-Viz justice. In practice, you’ll have many datasets coming from disparate sources that will require more complex processing and joining. In such a scenario, being able to see the lineage and dependencies of your data becomes very useful. Kedro-Viz is interactive in that you can optionally preview the data, you can view static or interactive figures, and you can view the code of the nodes all in the user interface. I recommend that you try this plugin for you next project!\n\n\nSource Control & Pre-Commit\nI am sure that everyone reading this already understands the importance of source/version control, so I will keep this breif. When we created our project Kedro was nice enough to create a .gitignore file and .gitkeep files. The .gitignore file makes sure that you don’t accidentally commit any data or any credentials that you store in conf/local/credentials.yml to a remote repository. Kedro does not, unfortunately, set up any pre-commit configuration so you need to do that manually. Here is an example of a pre-commit configuration that includes linting with ruff checking your codebase for missing docstrings with interrogate and stripping out any confidentiat data from notebooks with nbstripout.\n# .pre-commit-config.yaml\nrepos:\n  - repo: https://github.com/astral-sh/ruff-pre-commit\n    # Ruff version.\n    rev: v0.11.2\n    hooks:\n      # Run the linter.\n      - id: ruff\n        types_or: [ python, pyi ]\n        args: [ --fix ]\n      # Run the formatter.\n      - id: ruff-format\n        types_or: [ python, pyi ]\n  - repo: https://github.com/econchick/interrogate\n    rev: 1.7.0\n    hooks:\n    - id: interrogate\n      args: [--config=pyproject.toml]\n      pass_filenames: false\n  - repo: https://github.com/kynan/nbstripout\n    rev: 0.7.1\n    hooks:\n      - id: nbstripout\nYou also need to specify interrogate’s configuration within pyproject.toml you can add the following at the bottom of your file:\n[tool.interrogate]\nignore-init-method = false\nignore-init-module = false\nignore-magic = false\nignore-semiprivate = false\nignore-private = false\nignore-property-decorators = false\nignore-module = false\nignore-nested-functions = false\nignore-nested-classes = false\nignore-setters = false\nignore-overloaded-functions = false\nfail-under = 80\nexclude = [\"tests\", \"docs\", \"build\", \"src/climate_brazil/__main__.py\"]\next = []\nverbose = 2\nquiet = false\nwhitelist-regex = []\ncolor = true\nomit-covered-files = false\nAfter you’re done setting up your configuration you need to initialize a local git repository and install your pre-commit configurations.\ngit init\npre-commit install\nWonderul, we are all set now with source control and mitigating the possibility of commiting anything confidential to your repository.\n\n\nRunning Pipelines in Containers\nWe can also run the pipeline we just built inside of a container. Kedro maintains the kedro-docker plugin which facilitates getting your Kedro project running inside a container.\n\n\n\n\n\n\nNote\n\n\n\nWhile the plugin is named kedro-docker you can use it with other containerization frameworks such as Podman\n\n\nFirst, we need to install the plugin. You can add the following to your ./requirements.txt file:\nkedro-docker~=0.6.2\nThen execute:\npip install kedro-docker~=0.6.2\nWith the plugin installed, Kedro will generate a Dockerfile, .dockerignore, and .dive-ci that corresponds to your Kedro project by executing:\nkedro docker init\n\n\n\n\n\n\nCaution\n\n\n\nMake sure you Docker Engine is running otherwise the previous step will fail.\n\n\nLet’s take a look at the generated Dockerfile:\n#./Dockerfile\nARG BASE_IMAGE=python:3.9-slim\nFROM $BASE_IMAGE as runtime-environment\n\n# update pip and install uv\nRUN python -m pip install -U \"pip&gt;=21.2\"\nRUN pip install uv\n\n# install project requirements\nCOPY requirements.txt /tmp/requirements.txt\nRUN uv pip install --system --no-cache-dir -r /tmp/requirements.txt && rm -f /tmp/requirements.txt\n\n# add kedro user\nARG KEDRO_UID=999\nARG KEDRO_GID=0\nRUN groupadd -f -g ${KEDRO_GID} kedro_group && \\\n    useradd -m -d /home/kedro_docker -s /bin/bash -g ${KEDRO_GID} -u ${KEDRO_UID} kedro_docker\n\nWORKDIR /home/kedro_docker\nUSER kedro_docker\n\nFROM runtime-environment\n\n# copy the whole project except what is in .dockerignore\nARG KEDRO_UID=999\nARG KEDRO_GID=0\nCOPY --chown=${KEDRO_UID}:${KEDRO_GID} . .\n\nEXPOSE 8888\n\nCMD [\"kedro\", \"run\"]\nKedro will automatically assume that you want to run all your pipelines (or your defualt pipeline) but you can, quite easily, change the specified command in the generated docker file.\nNow that we have a Dockerfile and .dockerignore we can build our image.\nkedro docker build\nFinally, you can run your pipeline from within the container by either using your normal docker-cli commands or you can use the kedro-docker plugin and execute:\nkedro docker run\n\n\nSummary\nIn this post we walked through using the Kedro framework and learned how to follow software engineering best practices that ensure that your projects are reproducible, modular, and easy to maintain as a project matures. We learned about the data catalog, how to define nodes and subsequently linking nodes together to build pipelines. We then looked at a couple Kedro plugins like kedro-viz and kedro-docker that expanded the functionality of our project. We also talked about and walked through good practices to follow when implementing source control with git and pre-commit. All this in just part one of our series! We have a long ways to go still but I hope you are excited for what comes next.\n\n\nComing Next\nIf you use a popular/mainstream machine learning framework like PyTorch, TensorFlow, Scikit-Learn, or XGBoost then reproducibility and scalability are quite easy because you’ll typically find that most MLOPs and distributed frameworks natively support these tools. What do you do if you have a custom solution? Let’s say you are using a probabilistics programming language like Stan or PyMC and there isn’t native support for these tools. Well, that is what we are going to do in the following parts of this series. In part two we will fit a time series model using PyMC and talk about how to use ModelBuilder from pymc-extras to build a production ready model. I hope to see you there!"
  },
  {
    "objectID": "posts/kedro-ray/index.html",
    "href": "posts/kedro-ray/index.html",
    "title": "Reproducibility & Scalability Part 4Scaling With Ray",
    "section": "",
    "text": "Overview\nIn the final part of this series, we focus our attention on scaling the model training task. We begin by highlight the steps necessary to configure a Ray cluster. Followed by a refactoring of our training node to leverage the cluster resources. By the end, you should have no problem training tens of thousands of models even on a modest two-node cluster.\n\n\nIntroduction\nIn the previous post of this series, we configured the components of MLFlow and integrated experiment tracking into our project using Kedro hooks. In this post, we are going to configure a Ray cluster and leverage it to train our models at scale.\n\n\nGetting Started\nThe first thing we need to do is add Ray as a dependency to our project.\n\n\n\n\n\n\nImportant\n\n\n\nWe need to be very careful here because the version of Ray and Python of the client must match the versions running on the cluster.\n\n\nLet’s pin a specific version of Ray and Python to our environment.yml file:\n# ./environment.yml\nname: my_environment_name\nchannels:\n  - conda-forge\n  - defaults\n\ndependencies:\n  - 'pymc=5.20.1'\n  - 'graphviz=12.2.1'\n  - 'numpyro=0.17.0'\n  - 'python=3.12.7' # Pin Python version\n  - pip:\n    - ipython&gt;=8.10\n    - jupyterlab&gt;=3.0\n    - kedro-datasets&gt;=3.0\n    - kedro-viz&gt;=6.7.0\n    - kedro[jupyter]~=0.19.11\n    - notebook\n    - pre-commit~=3.5\n    - polars~=1.23.0\n    - pyarrow~=18.1.0\n    - plotly~=5.24.0\n    - openpyxl~=3.1.0\n    - kedro-docker~=0.6.2\n    - pymc-extras==0.2.3\n    - mlflow==2.19.0\n    - psycopg2-binary==2.9.10\n    - ray[default]==2.42.1 # Pin Ray version\nNext, we need to configure our Ray cluster.\n\n\nConfiguring a Ray Cluster\nIn practice you will want to configure your cluster independently from the current project you are working on because you will want to use this cluster across many projects. However, because this is a blog post, to keep things palatable we are going to configure our cluster coupled to our current project. I will attempt to call out where things differ and what you will need to do to configure your cluster in that situation.\nWe are going to start by defining a single container image that we will use to set up our head and worker(s) nodes so that all of our cluster components are running with the same environment. Notice that we are using the same environment.yml file that we have defined for our main project. However, in practice different projects may require different dependencies. I find it is best to start with a base environment with standard modules that you/your team typically use inside the image that defines the Ray environment and then when submitting the job to the cluster you can specify additional project specific dependencies that Ray will install in a new environment.\n# ./ray.Dockerfile\nARG BASE_IMAGE=python:3.12.7-slim\nFROM $BASE_IMAGE AS runtime-environment\n\n# install base utils\nRUN apt-get update \\\n    && apt-get install -y build-essential \\\n    && apt-get install -y wget \\\n    && apt-get clean \\\n    && rm -rf /var/lib/apt/lists/*\n\n# install miniconda\nENV CONDA_DIR /opt/conda\nRUN wget --quiet https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-aarch64.sh -O ~/miniconda.sh && \\\n    /bin/bash ~/miniconda.sh -b -p /opt/conda\n\n# put conda in path\nENV PATH=$CONDA_DIR/bin:$PATH\n\n# install project requirements\nCOPY environment.yml /tmp/environment.yml\nRUN conda env update --name base --file /tmp/environment.yml --prune\n\n\n\n\n\n\nNote\n\n\n\nIn practice you’ll bring up the head node on one machine and the worker nodes on different machines. Since the cluster components are running on separate machines you will need to configure your network and firewall rules so that these machines can communicate with one another. This may sound daunting at first but after you do it a couple of times it becomes trivial.\n\n\nIn this post, we are going to bring up the head node and one worker node on the same machine using Docker Compose. For our head node we are going to use the default ports of 6379, 8265, and 10001 to listen for other nodes to join the cluster, to host the Ray user interface, and to listen to client calls to submit/run jobs on the cluster respectively. We are also going to specify how much memory our nodes can consume this is a good practice to ensure you don’t end up with Out-Of-Memory errors and crash the machine, especially if in addition to being a part of you cluster the machine is being used for other tasks (not recommended but it happens). To implement our cluster add the following to the docker-compose.yml file that we ended up with in our last post:\n# ./docker-compose.yml\nservices:\n  ray-head:\n    build: \n      dockerfile: ray.Dockerfile\n    shm_size: 24G\n    command: &gt;\n      ray start\n        --head\n        --port 6379\n        --dashboard-host 0.0.0.0\n        --ray-client-server-port 10001\n        --block\n    ports:\n      - \"10001:10001\"\n      - \"6379:6379\"\n      - \"8265:8265\"\n    deploy:\n      resources:\n        limits:\n          memory: 24G\n\n  ray-worker:\n    build: \n      dockerfile: ray.Dockerfile\n    shm_size: 24G\n    command: &gt;\n      ray start\n        --address ray-head:6379\n        --block\n    depends_on:\n      ray-head:\n        condition: service_started\n    deploy:\n      resources:\n        limits:\n          memory: 24G\n\n\n\n\n\n\nPLEASE READ!\n\n\n\nNotice that we are specifying the shared memory size shm_size of our Ray services This is necessary because typically Docker will only allocate 64mb to /dev/shm and this shared memory is what Ray will use to store objects during runtime. If you don’t increase this parameter then Ray will store object on disk using /tmp and this will hit performance.\n\n\nAlright, let’s bring up our services by executing:\ndocker compose up -d\nLet’s visit the Ray dashboard running at localhost/8265 and check in on our cluster. You should see something similar to this:\n\n\n\nRay Dashboard\n\n\nGreat! Now that our cluster is up and running it is time to refactor our training node.\n\n\nRefactor Training\nTo leverage our running cluster we need to refactor our ML pipeline’s train node. The changes that we will need to make are:\n\nInitialize a connection to the cluster\nDefine a Ray task: This is simply a Python function that is decorated with @ray.remote\nPlace our data on the clusters object store: This is so that each worker doesn’t copy the data n-worker times saving on memory utilization\nSubmit your task to the cluster\nAsynchronously do stuff with the completed tasks as the rest are still running\n\n# /src/climate_brazil/ML/nodes.py\nimport ray\nfrom ray.types import ObjectRef\n\ndef train(training_dataset: pl.DataFrame, model_config: dict, sampler_config: dict) -&gt; dict:\n    \"\"\"\n    Train time-series models\n    ---\n    Params:\n        training_dataset: Training data\n        model_config: Model Configurations\n        sampler_config: MCMC sampler configurations\n    \"\"\"\n1    ray.init(\n        address=\"ray://ray-head:10001\",\n        ignore_reinit_error=True,\n        runtime_env={\"working_dir\": \"./src/\"},\n    )\n\n2    data_ref = ray.put(training_dataset.to_pandas())\n\n3    @ray.remote(max_retries=1, num_cpus=4)\n    def train_model(data_ref: ObjectRef, city: str) -&gt; dict[str, TSModel]:\n        \"\"\"\n        trains the time series model\n        ---\n        data_ref: The ray object reference to the data\n        city: The Brazilian city to fit the model on\n        \"\"\"\n        data = pl.from_pandas(data_ref)\n        city_dataset = data.filter(pl.col(\"city\")==city).sort(\"date\")\n        model = TSModel(model_config=model_config, sampler_config=sampler_config, city=city)\n        model.fit(\n            X=city_dataset['date'],\n            y=city_dataset['temperature'],\n            normalize_target=True\n        )\n        return {f\"{city}_model\": model}\n\n    trained_models = {}\n4    trained_models_ref = [\n        train_model.remote(data_ref=data_ref, city=city)\n        for city in training_dataset['city'].unique()\n    ]\n5    while len(trained_models_ref) &gt; 0:\n        ready, trained_models_ref = ray.wait(trained_models_ref, num_returns=1)\n        try:\n            # Here you can do other stuff while waiting\n            # for the rest of your models to finish training\n            model_dict = ray.get(ready) # returns a list of the functions return value\n            key, model = model_dict[0].popitem()\n            trained_models[key] = model\n        except Exception as e:\n            logger.exception(e)\n\n    return trained_models\n\n1\n\nInitialize the connection to the cluster\n\n2\n\nStore the data in the cluster’s object store (Notice conversion to Pandas due to no Polars support at the moment)\n\n3\n\nDefine your Ray task\n\n4\n\nCall your Ray task over your parallelization unit\n\n5\n\nPerform actions on completed tasks as they are ready\n\n\nAt this point, if we only had a handful of models to fit then this would be fine. However, there is still room for additional gain in performance. We are currently setting aside four cpus to handle just one city. In addition, PyMC has to compile every model and becomes costly the more models you need to fit. In order to get around all of that we can train our models on batches of cities, that way we only compile a model and reserve four cpus for each batch. For the sake of simplicity and brevity, we will leave this as an excercise for the interested reader to implement by refactoring the PyMC model.\n\n\n\n\n\n\nNote\n\n\n\nIn a time-series model, in order to vectorise across batches your time-series time range needs to be fixed. In this case, we would be able to fit 7 cities in one batch and the rest as single city batches.\n\n\n\n\nExecute Pipeline\nBefore running our pipeline, let’s make sure our image is built with the newly refactored training code.\ndocker compose build climate-brazil\nSince all of our other services are already up, we can run our pipeline by executing:\ndocker compose up -d climate-brazil\nAs our model is training you can check on the Ray job on the Ray dashboard. You should see something like this:\n\n\n\nRay Job Status\n\n\nAwesome! Our models are being fit on a cluster in parallel.\n\n\nSummary\nIn this shorter post, we walked through configuring a Ray cluster and highlighted some nuances that you may face in a production setting. After which, we proceeded to refactor our previous training implementation to leverage the running cluster. Additionally, we point out that there remains some performance gain to be had by refactoring the model to handle batched cities.\n\n\nConclusion\nWe’ve come to the end of the “Reproducibility & Scalability” series! If you’ve been following along, thank you! You have come a long way and in doing so you have learned:\n\nHow to set up a project that follows software engineering best practices using Kedro\nHow to leverage custom probabilistic machine learning models while using Kedro and adhering to best practices\nHow to integrate tracking your experimentation with Kedro by leveraging hooks that handle log parameters, metrics, and artifacts with MLFlow\nHow to scale your end-to-end machine learning project built with Kedro on a Ray cluster\n\nThat is quite a lot!! I hope this series proved to be valuable to you, and I hope to see you in the next one!"
  },
  {
    "objectID": "posts/kedro-pymc-modelbuilder/index.html",
    "href": "posts/kedro-pymc-modelbuilder/index.html",
    "title": "Reproducibility & Scalability Part 2PyMC ModelBuilder",
    "section": "",
    "text": "In part two of this series, we go over the necessary steps to take a time-series model into production by integrating the PyMC ModelBuilder class with Kedro."
  },
  {
    "objectID": "posts/kedro-pymc-modelbuilder/index.html#train-test-split",
    "href": "posts/kedro-pymc-modelbuilder/index.html#train-test-split",
    "title": "Reproducibility & Scalability Part 2PyMC ModelBuilder",
    "section": "Train-Test Split",
    "text": "Train-Test Split\nLet’s define how we are going split our dataset so that we can evaluate our model’s performance later. If you recall, we are dealing with time-series data across twelve Brazilian cities. Our interest is in forecasting the average monthly temperatures of these cities. Looking at the data you will notice that different cities started collecting data at different time points but the last measurement of October 2019 is shared by all cities (even if the measurement is missing). So, let’s hold out one year of data (12 measurements) for each city to be our test set.\n# /src/climate_brazil/pipelines/ML/nodes.py\ndef train_test_split(data: pl.DataFrame, testing_window: int) -&gt; tuple[pl.DataFrame, pl.DataFrame]:\n    \"\"\"\n    Splits time-series dataset into test and train splits\n    ---\n    Params:\n        data: The time-series dataset\n        testing_window: The size of the testing set\n    \"\"\"\n\n    testing_dataset = (\n        data\n        .sort(\"city\", \"date\")\n        .group_by(\"city\")\n        .agg(\n            pl.all().tail(testing_window)\n        )\n        .explode(\"temperature\", \"date\")\n        .sort(\"city\", \"date\")\n    )\n\n    training_dataset = (\n        data\n        .sort(\"city\", \"date\")\n        .group_by(\"city\")\n        .agg(\n            pl.all().slice(0, pl.len() - testing_window)\n        )\n        .explode(\"temperature\", \"date\")\n        .sort(\"city\", \"date\")\n    )\n\n    return training_dataset, testing_dataset\nOur train_test_split() function takes a parameter defining the testing set window size. This is a configuration that you may want to change in the future. Kedro generates configuration files for every pipeline that you created and that is where we will put all of our configuration parameters. Let’s define our testing_window parameter now.\n#/conf/base/parameters_ML.yml\ntesting_window: 12"
  },
  {
    "objectID": "posts/kedro-pymc-modelbuilder/index.html#bayesian-structural-time-series",
    "href": "posts/kedro-pymc-modelbuilder/index.html#bayesian-structural-time-series",
    "title": "Reproducibility & Scalability Part 2PyMC ModelBuilder",
    "section": "Bayesian Structural Time Series",
    "text": "Bayesian Structural Time Series\nWe are going to build a simple Bayesian structural time-series model to forecast the monthly average temperatures. This series is focused on engineering so I won’t delve too deeply into the mathematics behind the model. Briefly, our model is going to have two components, a trend component that we are going to estimate using a one-dimensional Gaussian process and a seasonality component that we are going to estimate as deterministic seasonality using fourier features."
  },
  {
    "objectID": "posts/kedro-pymc-modelbuilder/index.html#defining-the-model",
    "href": "posts/kedro-pymc-modelbuilder/index.html#defining-the-model",
    "title": "Reproducibility & Scalability Part 2PyMC ModelBuilder",
    "section": "Defining The Model",
    "text": "Defining The Model\nWe will represent our model as the following:\n\\[ temperature \\sim N(\\mu, \\sigma) \\] \\[ \\mu = f(x) + \\beta_{fourier}X_{fourier\\_terms} \\] \\[ f(x) \\sim GP(0, K(x, x'; \\eta, \\ell)) \\] \\[ \\beta_{fourier} \\sim N(0, 0.5) \\] \\[ \\eta \\sim HalfNormal(1.0) \\] \\[ \\ell \\sim Gamma(48, 2) \\] \\[ \\sigma \\sim HalfNormal(0.2) \\]\nWhere our Gaussian Process covariance matrix \\(K\\) uses the Exponentiated Quadratic kernel. \\[ K(x, x') = exp[-\\frac{(x - x')^{2}}{2\\ell^{2}}] \\]\nIn PyMC this would look like this:\nwith pm.Model() as model:\n    model.add_coord(\"obs_id\", train_time_range)\n    model.add_coord(\n        \"fourier_features\",\n        np.arange(len(train_fourier_series.to_numpy().T)),\n    )\n\n    t = pm.Data(\"time_range\", train_time_range, dims=\"obs_id\")\n    fourier_terms = pm.Data(\n        \"fourier_terms\", train_fourier_series.to_numpy().T\n    )\n\n    error = pm.HalfNormal(\"error\", 0.2)\n\n    # Trend component\n    amplitude_trend = pm.HalfNormal(\"amplitude_trend\", 1.0)\n    ls_trend = pm.Gamma(\"ls_trend\", alpha=48, beta=2)\n    cov_trend = amplitude_trend * pm.gp.cov.ExpQuad(1, ls_trend)\n    \n    gp_trend = pm.gp.HSGP(\n        m=[10], \n        c=5.,\n        cov_func=cov_trend\n    )\n    trend = gp_trend.prior(\"trend\", X=t[:, None], dims=\"obs_id\")\n\n    # Seasonality components\n    beta_fourier = pm.Normal(\n        \"beta_fourier\", mu=0, sigma=0.5, dims=\"fourier_features\"\n    )\n    seasonality = pm.Deterministic(\n        \"seasonal\", pm.math.dot(beta_fourier, fourier_terms), dims=\"obs_id\"\n    )\n\n    # Combine components\n    mu = trend + seasonality\n\n    pm.Normal(\n        \"temperature\",\n        mu=mu,\n        sigma=error,\n        observed=temperature_normalized,\n        dims=\"obs_id\",\n        )\nAt this point we could pass in the expected data (time_range, fourier_terms, and temperature_normalized) for a city and train the time-series. However, there would be less back-tracking if we go ahead and define our model along with the necessary processing of the inputs, the method for generating forecasts, and the methods for saving and loading our trained model."
  },
  {
    "objectID": "posts/kedro-pymc-modelbuilder/index.html#configuring-modelbuilder",
    "href": "posts/kedro-pymc-modelbuilder/index.html#configuring-modelbuilder",
    "title": "Reproducibility & Scalability Part 2PyMC ModelBuilder",
    "section": "Configuring ModelBuilder",
    "text": "Configuring ModelBuilder\nThe ModelBuilder class from the pymc-extras package gives us a nice scaffolding that facilitates taking PyMC models into production. There are seven required methods that we need to define inside of our model class that inherits from ModelBuilder. However, it is often the case that we need to override some default methods in accordance to our modeling needs. Let’s go over the required methods first.\nWe need to implement _generate_and_preprocess_model_data() which is a method that performs our final processing of the data before it is fed into the model. In our case, we normalize the data and define our fourier features to capture yearly (12 months) seasonality.\n# ModelBuilder Required Method Implementation\ndef _generate_and_preprocess_model_data(self, X: pl.DataFrame, y: pl.Series, normalize: bool = False):\n    \"\"\"\n    Last mile data processing of inputs expected by the model\n    ---\n    Params:\n        X: The matrix of predictor variables expected by our model\n        y: The target variable\n        normalize: Whether to Z normalize the variables\n    \"\"\"\n    self.train_time_range = np.arange(X.shape[0])\n    self.n_modes = 10\n    periods = np.array(self.train_time_range) / (12)\n    self.train_fourier_series = pl.DataFrame(\n        {\n            f\"{func}_order_{order}\": getattr(np, func)(\n                2 * np.pi * periods * order\n            )\n            for order in range(1, self.n_modes + 1)\n            for func in (\"sin\", \"cos\")\n        }\n    )\n    if normalize:\n        self.y_mean = np.nanmean(y)\n        self.y_std = np.nanstd(y)\n        self.y_normalized = (y - self.y_mean) / self.y_std\n    else:\n        self.y_normalized = y\n\n\n\n\n\n\nCaution\n\n\n\nSince we are only including endogenous variables in our model the normalization only applies to the target variable\n\n\nNext we need to define how we are going to feed new data to our model when we are ready to make forecasts. Since we are building a time-series model with no exogenous predictor variables, we only need to define how far out into the future we want to generate forecasts.\n# ModelBuilder Required Method Implementation\ndef _data_setter(self, n_ahead: int):\n    \"\"\"\n    Generates required data for producing forecasts\n    ---\n    Params:\n        n_ahead: How many periods (months) to forecast future temperatures\n    \"\"\"\n    self.start = self.train_time_range[-1]\n    self.end = self.start + n_ahead\n\n    new_periods = np.arange(self.start, self.end, 1) / (12)\n    self.test_fourier_series = pl.DataFrame(\n        {\n            f\"{func}_order_{order}\": getattr(np, func)(\n                2 * np.pi * new_periods * order\n            )\n            for order in range(1, self.n_modes + 1)\n            for func in (\"sin\", \"cos\")\n        }\n    )\nCertainly we will need to define how to build our model by using the build_model() method. You’ll notice that we call our previously defined method _generate_and_preprocess_model_data() here.\n# ModelBuilder Required Method Implementation\ndef build_model(self, X: pl.DataFrame, y: pl.Series, normalize_target: bool = False, **kwargs):\n    \"\"\"\n    Defines the PyMC model structure\n    ---\n    Params:\n        X: Dataframe of features\n        y: Array of target values\n    \"\"\"\n\n    self._generate_and_preprocess_model_data(X, y, normalize=normalize_target)\n\n    with pm.Model() as self.model:\n        self.model.add_coord(\"obs_id\", self.train_time_range)\n        self.model.add_coord(\n            \"fourier_features\",\n            np.arange(len(self.train_fourier_series.to_numpy().T)),\n        )\n\n        t = pm.Data(\"time_range\", self.train_time_range, dims=\"obs_id\")\n        fourier_terms = pm.Data(\n            \"fourier_terms\", self.train_fourier_series.to_numpy().T\n        )\n\n        error = pm.HalfNormal(\"error\", self.model_config['error'])\n\n        # Trend component\n        amplitude_trend = pm.HalfNormal(\"amplitude_trend\", self.model_config['amplitude_trend'])\n        ls_trend = pm.Gamma(\"ls_trend\", alpha=self.model_config['ls_trend']['alpha'], beta=self.model_config['ls_trend']['beta'])\n        cov_trend = amplitude_trend * pm.gp.cov.ExpQuad(1, ls_trend)\n\n        gp_trend = pm.gp.HSGP(\n            m=[10], \n            c=5., \n            cov_func=cov_trend\n        )\n        trend = gp_trend.prior(\"trend\", X=t[:, None], dims=\"obs_id\")\n\n        # Seasonality components\n        beta_fourier = pm.Normal(\n            \"beta_fourier\", mu=self.model_config['beta_fourier']['mu'], sigma=self.model_config['beta_fourier']['sigma'], dims=\"fourier_features\"\n        )\n        seasonality = pm.Deterministic(\n            \"seasonal\", pm.math.dot(beta_fourier, fourier_terms), dims=\"obs_id\"\n        )\n\n        # Combine components\n        mu = trend + seasonality\n\n        pm.Normal(\n            \"temperature_normalized\",\n            mu=mu,\n            sigma=error,\n            observed=self.y_normalized,\n            dims=\"obs_id\",\n            )\nFinally, the last three required methods are all methods that define configurations for our model. As we mentioned earlier, all configurations should be handled by Kedro in our projects’ conf/ directory. So we are going to return empty dicts as our default configurations.\n# ModelBuilder Required Methods Implementations\n@staticmethod\ndef get_default_model_config() -&gt; dict:\n    model_config = {}\n    return model_config\n\n@staticmethod\ndef get_default_sampler_config() -&gt; dict:\n    sampler_config= {}\n    return sampler_config\n\n@property\ndef output_var(self):\n    \"\"\"\n    This is basically the name of the target variable. This is used when loading in a saved model.\n    \"\"\"\n    return \"temperature\"\n\n@property\n@abstractmethod\ndef _serializable_model_config(self) -&gt; dict[str, int | float | dict]:\n    \"\"\"\n    Only needs to be implemented if your model configurations contain unserializable objects.\n    You need to convert the non-serializable objects to serializable ones here.\n    \"\"\"\n    return self.model_config\nLet’s go ahead and define our model and sampler configurations in the parameters_ML.yml file.\n#/conf/base/parameters_ML.yml\ntesting_window: 12\n\nmodel_config:\n  error: 0.2\n  amplitude_trend: 1.0\n  ls_trend:\n    alpha: 48\n    beta: 2\n  beta_fourier:\n    mu: 0\n    sigma: 0.5\n\nsampler_config:\n  draws: 1000\n  tune: 1000\n  chains: 4\n  target_accept: 0.95\nOur model configurations contain the priors that we defined for our model above. Whereas, the sampler configurations define parameters related to our MCMC sampler.\nWith the required ModelBuilder methods out of the way, let’s talk about some of the methods that we will override to accomodate our specific use case.\nFirst, since we are using a time-series model our predictions are generated differently than how you’d generate predictions from a typical regression or classification task. So we need to make changes to the sample_posterior_predictive() method to accomdate a time-series model. We take the approach of defining a new model specifically for forecasting purposes. Notice that we are calling _data_setter() to generate our future model input, which is our forecast horizon.\n# ModelBuilder Override method\ndef sample_posterior_predictive(self, n_ahead: int, extend_idata: bool, combined: bool, **kwargs):\n    self._data_setter(n_ahead)\n\n    with pm.Model() as self.model:\n        self.model.add_coord(\"obs_id\", self.train_time_range)\n        self.model.add_coord(\n            \"fourier_features\",\n            np.arange(len(self.train_fourier_series.to_numpy().T)),\n        )\n\n        t = pm.Data(\"time_range\", self.train_time_range, dims=\"obs_id\")\n        fourier_terms = pm.Data(\n            \"fourier_terms\", self.train_fourier_series.to_numpy().T\n        )\n\n        error = pm.HalfNormal(\"error\", self.model_config['error'])\n\n        # Trend component\n        amplitude_trend = pm.HalfNormal(\"amplitude_trend\", self.model_config['amplitude_trend'])\n        ls_trend = pm.Gamma(\"ls_trend\", alpha=self.model_config['ls_trend']['alpha'], beta=self.model_config['ls_trend']['beta'])\n        cov_trend = amplitude_trend * pm.gp.cov.ExpQuad(1, ls_trend)\n\n        gp_trend = pm.gp.HSGP(\n            m=[10], \n            c=5., \n            cov_func=cov_trend\n        )\n        trend = gp_trend.prior(\"trend\", X=t[:, None], dims=\"obs_id\")\n\n        # Seasonality components\n        beta_fourier = pm.Normal(\n            \"beta_fourier\", mu=self.model_config['beta_fourier']['mu'], sigma=self.model_config['beta_fourier']['sigma'], dims=\"fourier_features\"\n        )\n        seasonality = pm.Deterministic(\n            \"seasonal\", pm.math.dot(beta_fourier, fourier_terms), dims=\"obs_id\"\n        )\n\n        # Combine components\n        mu = trend + seasonality\n\n        pm.Normal(\n            \"temperature_normalized\",\n            mu=mu,\n            sigma=error,\n            observed=self.y_normalized,\n            dims=\"obs_id\",\n            )\n        \n        self.model.add_coords({\"obs_id_fut\": np.arange(self.start, self.end, 1)})\n\n        t_fut = pm.Data(\"time_range_fut\", np.arange(self.start, self.end, 1))\n        fourier_terms_fut = pm.Data(\"fourier_terms_fut\", self.test_fourier_series.to_numpy().T)\n\n        # Trend future component\n        trend_fut = gp_trend.conditional(\"trend_fut\", Xnew=t_fut[:, None], dims=\"obs_id_fut\")\n\n        # Seasonality components\n        seasonality_fut = pm.Deterministic(\n            \"seasonal_fut\", pm.math.dot(beta_fourier, fourier_terms_fut), dims=\"obs_id_fut\"\n        )\n\n        mu_fut = trend_fut + seasonality_fut\n\n        pm.Normal(\n            \"temperature_normalized_fut\",\n            mu=mu_fut,\n            sigma=error,\n            dims=\"obs_id_fut\",\n            )\n\n    with self.model:  # sample with new input data\n        post_pred = pm.sample_posterior_predictive(self.idata, var_names=[\"temperature_normalized_fut\"], predictions=True, **kwargs)\n        if extend_idata:\n            self.idata.extend(post_pred, join=\"right\")\n\n    posterior_predictive_samples = az.extract(\n        post_pred, \"predictions\", combined=combined\n    )\n\n    return posterior_predictive_samples\nWe also need to update the fit() method for a couple of reasons. The first is because we implemented a normalize_target method and we need to propogate that into the fit() method which calls our build_model() method. Second, because the defualt fit() method defines additional sampler configurations that we don’t want because we have already centralized all of our configurations to be handled by Kedro.\n# ModelBuilder Override method\ndef fit(self, X: pl.DataFrame, y: pl.Series, normalize_target: bool = False) -&gt; az.InferenceData:\n    \"\"\"\n    Fits the model to the provided dataset\n    ---\n    Params:\n        X: The dataset container predictor variables\n        y: The target variable\n        normalize_target: Whether to Z normalize the target variable\n    \"\"\"\n    self.build_model(X, y, normalize_target=normalize_target)\n    self.idata = self.sample_model(**self.sampler_config)\n\n    X_df = X.to_pandas()\n    combined_data = pd.concat([X_df, y.to_pandas()], axis=1)\n    assert all(combined_data.columns), \"All columns must have non-empty names\"\n    with warnings.catch_warnings():\n        warnings.filterwarnings(\n            \"ignore\",\n            category=UserWarning,\n            message=\"The group fit_data is not defined in the InferenceData scheme\",\n        )\n        self.idata.add_groups(fit_data=combined_data.to_xarray())\n\n    return self.idata\nLastly (I promise), we need to define a new method that will handle our model evaluations. We are going to use the root mean squared error as a metric to evaluate our model. We will also measure the coverage of our 80% Highest Density Interval (HDI). Additionally, it is a good idea to include in our model serialization the city the model is fitted to, and if we are standardizing the data then we should also include the mean and standard deviation of our target temperature values.\n# ModelBuilder override Init\ndef __init__(\n        self,\n        model_config: dict | None = None,\n        sampler_config: dict | None = None,\n        city: str = None\n    ):\n        \"\"\"\n        Initializes model configuration and sampler configuration for the model\n\n        Parameters\n        ----------\n        model_config : Dictionary, optional\n            dictionary of parameters that initialise model configuration. Class-default defined by the user default_model_config method.\n        sampler_config : Dictionary, optional\n            dictionary of parameters that initialise sampler configuration. Class-default defined by the user default_sampler_config method.\n        city: The Brazilian city we are modeling monthly average temperatures of\n        \"\"\"\n        sampler_config = (\n            self.get_default_sampler_config() if sampler_config is None else sampler_config\n        )\n        self.sampler_config = sampler_config\n        model_config = self.get_default_model_config() if model_config is None else model_config\n\n        self.model_config = model_config  \n        self.model = None  \n        self.idata: az.InferenceData | None = None\n        self.is_fitted_ = False\n        self.city = city # This is our Addition\n\n# ModelBuilder new method\ndef evaluate(self, y_true: pl.Series, forecasts: xr.Dataset, back_transform: bool = False) -&gt; dict:\n    \"\"\"\n    Evaluate our forecasts posterior predictive mean using the root mean squared error (RMSE) as the metric and evaluate our highest density interval's (HDI)s coverage\n    ---\n    Params:\n        y_true: The ground truth temperatures\n        forecasts: The forecasts\n        back_transform: Whether we need to transform our forecasts back to the original scale\n    \"\"\"\n    if back_transform:\n        try:\n            y_mean = self.y_mean\n            y_std = self.y_std\n        except AttributeError:\n            y_mean = self.idata.attrs['y_mean']\n            y_std = self.idata.attrs['y_std']\n        posterior_predictive_mean = forecasts[f'{self.output_var}_normalized_fut'].mean((\"chain\", \"draw\")).values * y_std + y_mean\n        hdi = az.hdi(forecasts[f'{self.output_var}_normalized_fut'], hdi_prob=0.8) * y_std + y_mean\n    else:\n        posterior_predictive_mean = forecasts[f'{self.output_var}_normalized_fut'].mean((\"chain\", \"draw\")).values\n        hdi = az.hdi(forecasts[f'{self.output_var}_normalized_fut'], hdi_prob=0.8)\n\n    error = y_true.to_numpy() - posterior_predictive_mean\n    RMSE = np.sqrt(\n        np.nanmean(\n            np.square(error)\n        )\n    )\n\n    coverage_df = pl.DataFrame(\n        {\n            \"hdi_lower\": hdi[f'{self.output_var}_normalized_fut'][:, 0].values,\n            \"hdi_upper\": hdi[f'{self.output_var}_normalized_fut'][:, 1].values,\n            \"y_true\": y_true\n        }\n    )\n\n    COVERAGE = (\n        coverage_df\n            .filter(\n                pl.col(\"y_true\").is_not_null()\n            )\n            .with_columns(\n                pl.when(\n                    (pl.col(\"y_true\") &lt;= pl.col(\"hdi_upper\")) &\n                    (pl.col(\"y_true\") &gt;= pl.col(\"hdi_lower\"))\n                )\n                .then(1.)\n                .otherwise(0.)\n                .alias(\"coverage\")\n            )\n            .select(pl.col(\"coverage\").mean()).item()\n    )\n    \n    return {\"RMSE\": RMSE, \"HDI_COVERAGE\": COVERAGE}\n\ndef _save_input_params(self, idata) -&gt; None:\n    \"\"\"\n    Saves any additional model parameters (other than the dataset) to the idata object.\n    \"\"\"\n    idata.attrs[\"city\"] = self.city\n    idata.attrs[\"y_mean\"] = self.y_mean\n    idata.attrs[\"y_std\"] = self.y_std\nCombining everything together we have our time-series model implemented as:\n# /src/climate_brazil/pipelines/ML/ts_model.py\nimport warnings\nfrom abc import abstractmethod\n\nimport arviz as az\nimport numpy as np\nimport pandas as pd\nimport polars as pl\nimport pymc as pm\nimport xarray as xr\nfrom pymc_extras.model_builder import ModelBuilder\n\n\nclass TSModel(ModelBuilder):\n    _model_type = \"TimeSeries\"\n    version = \"0.1\"\n\n    def __init__(\n        self,\n        model_config: dict | None = None,\n        sampler_config: dict | None = None,\n        city: str = None\n    ):\n        \"\"\"\n        Initializes model configuration and sampler configuration for the model\n\n        Parameters\n        ----------\n        model_config : Dictionary, optional\n            dictionary of parameters that initialise model configuration. Class-default defined by the user default_model_config method.\n        sampler_config : Dictionary, optional\n            dictionary of parameters that initialise sampler configuration. Class-default defined by the user default_sampler_config method.\n        city: The Brazilian city we are modeling monthly average temperatures of\n        \"\"\"\n        sampler_config = (\n            self.get_default_sampler_config() if sampler_config is None else sampler_config\n        )\n        self.sampler_config = sampler_config\n        model_config = self.get_default_model_config() if model_config is None else model_config\n\n        self.model_config = model_config  # parameters for priors etc.\n        self.model = None  # Set by build_model\n        self.idata: az.InferenceData | None = None  # idata is generated during fitting\n        self.is_fitted_ = False\n        self.city = city\n\n    def _generate_and_preprocess_model_data(self, X: pl.DataFrame, y: pl.Series, normalize: bool = False):\n        \"\"\"\n        Last mile data processing of inputs expected by the model\n        ---\n        Params:\n            X: The matrix of predictor variables expected by our model\n            y: The target variable\n            normalize: Whether to Z normalize the variables\n        \"\"\"\n        self.train_time_range = np.arange(X.shape[0])\n        self.n_modes = 10\n        periods = np.array(self.train_time_range) / (12)\n        self.train_fourier_series = pl.DataFrame(\n            {\n                f\"{func}_order_{order}\": getattr(np, func)(\n                    2 * np.pi * periods * order\n                )\n                for order in range(1, self.n_modes + 1)\n                for func in (\"sin\", \"cos\")\n            }\n        )\n        if normalize:\n            self.y_mean = np.nanmean(y)\n            self.y_std = np.nanstd(y)\n            self.y_normalized = (y - self.y_mean) / self.y_std\n        else:\n            self.y_normalized = y\n\n    def _data_setter(self, n_ahead: int):\n        \"\"\"\n        Generates required data for producing forecasts\n        ---\n        Params:\n            n_ahead: How many periods (months) to forecast future temperatures\n        \"\"\"\n        self.start = self.train_time_range[-1]\n        self.end = self.start + n_ahead\n\n        new_periods = np.arange(self.start, self.end, 1) / (12)\n        self.test_fourier_series = pl.DataFrame(\n            {\n                f\"{func}_order_{order}\": getattr(np, func)(\n                    2 * np.pi * new_periods * order\n                )\n                for order in range(1, self.n_modes + 1)\n                for func in (\"sin\", \"cos\")\n            }\n        )\n\n    def build_model(self, X: pl.DataFrame, y: pl.Series, normalize_target: bool = False, **kwargs):\n        \"\"\"\n        Defines the PyMC model structure\n        ---\n        Params:\n            X: Dataframe of features\n            y: Array of target values\n        \"\"\"\n\n        self._generate_and_preprocess_model_data(X, y, normalize=normalize_target)\n\n        with pm.Model() as self.model:\n            self.model.add_coord(\"obs_id\", self.train_time_range)\n            self.model.add_coord(\n                \"fourier_features\",\n                np.arange(len(self.train_fourier_series.to_numpy().T)),\n            )\n\n            t = pm.Data(\"time_range\", self.train_time_range, dims=\"obs_id\")\n            fourier_terms = pm.Data(\n                \"fourier_terms\", self.train_fourier_series.to_numpy().T\n            )\n\n            error = pm.HalfNormal(\"error\", self.model_config['error'])\n\n            # Trend component\n            amplitude_trend = pm.HalfNormal(\"amplitude_trend\", self.model_config['amplitude_trend'])\n            ls_trend = pm.Gamma(\"ls_trend\", alpha=self.model_config['ls_trend']['alpha'], beta=self.model_config['ls_trend']['beta'])\n            cov_trend = amplitude_trend * pm.gp.cov.ExpQuad(1, ls_trend)\n\n            gp_trend = pm.gp.HSGP(\n                m=[10],\n                c=5.,\n                cov_func=cov_trend\n            )\n            trend = gp_trend.prior(\"trend\", X=t[:, None], dims=\"obs_id\")\n\n            # Seasonality components\n            beta_fourier = pm.Normal(\n                \"beta_fourier\", mu=self.model_config['beta_fourier']['mu'], sigma=self.model_config['beta_fourier']['sigma'], dims=\"fourier_features\"\n            )\n            seasonality = pm.Deterministic(\n                \"seasonal\", pm.math.dot(beta_fourier, fourier_terms), dims=\"obs_id\"\n            )\n\n            # Combine components\n            mu = trend + seasonality\n\n            pm.Normal(\n                \"temperature_normalized\",\n                mu=mu,\n                sigma=error,\n                observed=self.y_normalized,\n                dims=\"obs_id\",\n                )\n\n    def sample_posterior_predictive(self, n_ahead: int, extend_idata: bool, combined: bool, **kwargs):\n        self._data_setter(n_ahead)\n\n        with pm.Model() as self.model:\n            self.model.add_coord(\"obs_id\", self.train_time_range)\n            self.model.add_coord(\n                \"fourier_features\",\n                np.arange(len(self.train_fourier_series.to_numpy().T)),\n            )\n\n            t = pm.Data(\"time_range\", self.train_time_range, dims=\"obs_id\")\n            fourier_terms = pm.Data(\n                \"fourier_terms\", self.train_fourier_series.to_numpy().T\n            )\n\n            error = pm.HalfNormal(\"error\", self.model_config['error'])\n\n            # Trend component\n            amplitude_trend = pm.HalfNormal(\"amplitude_trend\", self.model_config['amplitude_trend'])\n            ls_trend = pm.Gamma(\"ls_trend\", alpha=self.model_config['ls_trend']['alpha'], beta=self.model_config['ls_trend']['beta'])\n            cov_trend = amplitude_trend * pm.gp.cov.ExpQuad(1, ls_trend)\n\n            gp_trend = pm.gp.HSGP(\n                m=[10],\n                c=5.,\n                cov_func=cov_trend\n            )\n            trend = gp_trend.prior(\"trend\", X=t[:, None], dims=\"obs_id\")\n\n            # Seasonality components\n            beta_fourier = pm.Normal(\n                \"beta_fourier\", mu=self.model_config['beta_fourier']['mu'], sigma=self.model_config['beta_fourier']['sigma'], dims=\"fourier_features\"\n            )\n            seasonality = pm.Deterministic(\n                \"seasonal\", pm.math.dot(beta_fourier, fourier_terms), dims=\"obs_id\"\n            )\n\n            # Combine components\n            mu = trend + seasonality\n\n            pm.Normal(\n                \"temperature_normalized\",\n                mu=mu,\n                sigma=error,\n                observed=self.y_normalized,\n                dims=\"obs_id\",\n                )\n\n            self.model.add_coords({\"obs_id_fut\": np.arange(self.start, self.end, 1)})\n\n            t_fut = pm.Data(\"time_range_fut\", np.arange(self.start, self.end, 1))\n            fourier_terms_fut = pm.Data(\"fourier_terms_fut\", self.test_fourier_series.to_numpy().T)\n\n            # Trend future component\n            trend_fut = gp_trend.conditional(\"trend_fut\", Xnew=t_fut[:, None], dims=\"obs_id_fut\")\n\n            # Seasonality components\n            seasonality_fut = pm.Deterministic(\n                \"seasonal_fut\", pm.math.dot(beta_fourier, fourier_terms_fut), dims=\"obs_id_fut\"\n            )\n\n            mu_fut = trend_fut + seasonality_fut\n\n            pm.Normal(\n                \"temperature_normalized_fut\",\n                mu=mu_fut,\n                sigma=error,\n                dims=\"obs_id_fut\",\n                )\n\n        with self.model:  # sample with new input data\n            post_pred = pm.sample_posterior_predictive(self.idata, var_names=[f\"{self.output_var}_normalized_fut\"], predictions=True, **kwargs)\n            if extend_idata:\n                self.idata.extend(post_pred, join=\"right\")\n\n        posterior_predictive_samples = az.extract(\n            post_pred, \"predictions\", combined=combined\n        )\n\n        return posterior_predictive_samples\n\n    def fit(self, X: pl.DataFrame, y: pl.Series, normalize_target: bool = False) -&gt; az.InferenceData:\n        \"\"\"\n        Fits the model to the provided dataset\n        ---\n        Params:\n            X: The dataset container predictor variables\n            y: The target variable\n            normalize_target: Whether to Z normalize the target variable\n        \"\"\"\n\n        self.build_model(X, y, normalize_target=normalize_target)\n        self.idata = self.sample_model(**self.sampler_config)\n\n        X_df = X.to_pandas()\n        combined_data = pd.concat([X_df, y.to_pandas()], axis=1)\n        assert all(combined_data.columns), \"All columns must have non-empty names\"\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\n                \"ignore\",\n                category=UserWarning,\n                message=\"The group fit_data is not defined in the InferenceData scheme\",\n            )\n            self.idata.add_groups(fit_data=combined_data.to_xarray())  # type: ignore\n\n        return self.idata  # type: ignore\n\n    @staticmethod\n    def get_default_model_config() -&gt; dict:\n        model_config = {\n            \"error\": 0.2,\n            \"amplitude_trend\": 1.0,\n            \"ls_trend\": {\"alpha\": 48, \"beta\": 2},\n            \"beta_fourier\": {\"mu\": 0, \"sigma\": 0.5},\n        }\n        return model_config\n\n    @staticmethod\n    def get_default_sampler_config() -&gt; dict:\n        \"\"\"\n        Returns a class default sampler dict for model builder if no sampler_config is provided on class initialization.\n        The sampler config dict is used to send parameters to the sampler .\n        It will be used during fitting in case the user doesn't provide any sampler_config of their own.\n        \"\"\"\n        sampler_config= {\n            \"draws\": 1_000,\n            \"tune\": 1_000,\n            \"chains\": 4,\n            \"target_accept\": 0.95,\n        }\n        return sampler_config\n\n    @property\n    def output_var(self):\n        return \"temperature\"\n\n    @property\n    @abstractmethod\n    def _serializable_model_config(self) -&gt; dict[str, int | float | dict]:\n        \"\"\"\n        Converts non-serializable values from model_config to their serializable reversable equivalent.\n        Data types like pandas DataFrame, Series or datetime aren't JSON serializable,\n        so in order to save the model they need to be formatted.\n\n        Returns\n        -------\n        model_config: dict\n        \"\"\"\n        return self.model_config\n\n    def evaluate(self, y_true: pl.Series, forecasts: xr.Dataset, back_transform: bool = False) -&gt; dict:\n        \"\"\"\n        Evaluate our forecasts posterior predictive mean using the root mean squared error (RMSE) as the metric and evaluate our highest density interval's (HDI)s coverage\n        ---\n        Params:\n            y_true: The ground truth temperatures\n            forecasts: The forecasts\n            back_transform: Whether we need to transform our forecasts back to the original scale\n        \"\"\"\n        if back_transform:\n            try:\n                y_mean = self.y_mean\n                y_std = self.y_std\n            except AttributeError:\n                y_mean = self.idata.attrs['y_mean']\n                y_std = self.idata.attrs['y_std']\n            posterior_predictive_mean = forecasts[f'{self.output_var}_normalized_fut'].mean((\"chain\", \"draw\")).values * y_std + y_mean\n            hdi = az.hdi(forecasts[f'{self.output_var}_normalized_fut'], hdi_prob=0.8) * y_std + y_mean\n        else:\n            posterior_predictive_mean = forecasts[f'{self.output_var}_normalized_fut'].mean((\"chain\", \"draw\")).values\n            hdi = az.hdi(forecasts[f'{self.output_var}_normalized_fut'], hdi_prob=0.8)\n\n        error = y_true.to_numpy() - posterior_predictive_mean\n        RMSE = np.sqrt(\n            np.nanmean(\n                np.square(error)\n            )\n        )\n\n        coverage_df = pl.DataFrame(\n            {\n                \"hdi_lower\": hdi[f'{self.output_var}_normalized_fut'][:, 0].values,\n                \"hdi_upper\": hdi[f'{self.output_var}_normalized_fut'][:, 1].values,\n                \"y_true\": y_true\n            }\n        )\n\n        COVERAGE = (\n            coverage_df\n              .filter(\n                  pl.col(\"y_true\").is_not_null()\n              )\n              .with_columns(\n                  pl.when(\n                      (pl.col(\"y_true\") &lt;= pl.col(\"hdi_upper\")) &\n                      (pl.col(\"y_true\") &gt;= pl.col(\"hdi_lower\"))\n                  )\n                    .then(1.)\n                    .otherwise(0.)\n                    .alias(\"coverage\")\n              )\n              .select(pl.col(\"coverage\").mean()).item()\n        )\n\n        return {\"RMSE\": RMSE, \"HDI_COVERAGE\": COVERAGE}\n\n    def _save_input_params(self, idata: az.InferenceData) -&gt; None:\n        \"\"\"\n        Saves any additional model parameters (other than the dataset) to the idata object.\n        \"\"\"\n        idata.attrs[\"city\"] = self.city\n        idata.attrs[\"y_mean\"] = self.y_mean\n        idata.attrs[\"y_std\"] = self.y_std"
  },
  {
    "objectID": "posts/kedro-pymc-modelbuilder/index.html#training-node",
    "href": "posts/kedro-pymc-modelbuilder/index.html#training-node",
    "title": "Reproducibility & Scalability Part 2PyMC ModelBuilder",
    "section": "Training Node",
    "text": "Training Node\nNow that we have our model configured with ModelBuilder it is time to define our training node:\n#/src/climate_brazil/pipelines/ML/nodes.py\ndef train(training_dataset: pl.DataFrame, model_config: dict, sampler_config: dict) -&gt; dict:\n    \"\"\"\n    Train time-series models\n    ---\n    Params:\n        training_dataset: Training data\n        model_config: Model Configurations\n        sampler_config: MCMC sampler configurations\n    \"\"\"\n    city_models = {}\n    for city in training_dataset['city'].sort().unique(maintain_order=True):\n        city_dataset = training_dataset.filter(pl.col(\"city\")==city).sort(\"date\")\n        model = TSModel(model_config=model_config, sampler_config=sampler_config, city=city)\n        model.fit(X=city_dataset['date'], y=city_dataset['temperature'], normalize_target=True)\n        city_models[f\"{city}_model\"] = model\n\n    return city_models\n\n\n\n\n\n\nImportant\n\n\n\nDO NOT use this training node in production! We are going to refactor this code in the last part of this series to scale more efficiently.\n\n\nWe need to be sure to define catalog entries for our training output otherwise Kedro will designate our trained models as MemoryDataset’s. However, there is no built-in dataset in Kedro that knows how to store our model. So we need to define a custom Kedro dataset to accomodate our needs.\n\nCreating a Custom Kedro Dataset\nCreating a custom dataset can be quite simple. All we need to do is inheret from Kedro’s AbstractDataset and define the init, save, load, and describe methods. In our case, the ModelBuilder class has save and load methods that accept a filepath and we will use those methods in our custom dataset:\n# /src/climate_brazil/datasets/pymc_model_dataset.py\nfrom pathlib import Path, PurePosixPath\nfrom typing import Any\n\nfrom kedro.io import AbstractDataset\n\nfrom kedro_framework.pipelines.ML.ts_model import TSModel\n\n\nclass PyMCModelDataset(AbstractDataset):\n    \"\"\"\n    ``PyMCDataset`` loads / save PyMC models from a given filepath as a TSModel object.\n    \"\"\"\n\n    def __init__(self, filepath: str):\n        \"\"\"Creates a new instance of PyMCDataset to load / save PyMC models for a given filepath.\n\n        Args:\n            filepath: The location of the model netcdf file to load / save data.\n        \"\"\"\n        self._filepath = PurePosixPath(filepath)\n        self._path = Path(self._filepath.parent)\n\n    def load(self) -&gt; TSModel:\n        \"\"\"Loads data from the netcdf file.\n\n        Returns:\n            loaded TSModel\n        \"\"\"\n        model = TSModel.load(self._filepath)\n        return model\n\n    def save(self, model: TSModel) -&gt; None:\n        \"\"\"Saves PyMC model to the specified filepath.\"\"\"\n        self._path.mkdir(parents=True,  exist_ok=True)\n        model.save(self._filepath)\n\n    def _describe(self) -&gt; dict[str, Any]:\n        \"\"\"Returns a dict that describes the attributes of the dataset.\"\"\"\n        return dict(filepath=self._filepath)\n\n\nTrain Catalog Entry\nWith our custom dataset defined let’s define entries for our training and testing data as well as for our trained models.\n# conf/base/catalog.yml\n# Raw data ---------------------------------------\ncities_temperatures:\n  type: partitions.PartitionedDataset\n  path: data/01_raw/\n  dataset: \n    type: polars.LazyPolarsDataset\n    file_format: csv\n\n# Processed data ---------------------------------------\ncities_temperatures_processed:\n  type: polars.EagerPolarsDataset\n  file_format: parquet\n  filepath: data/02_intermediate/cities_temperatures_processed.pq\n\ncities_ts_plot:\n  type: plotly.JSONDataset\n  filepath: data/08_reporting/cities_ts_plot.json\n\n# Model inputs ------------------------------------------\ntraining_dataset:\n  type: polars.EagerPolarsDataset\n  file_format: parquet\n  filepath: data/05_model_input/training_dataset.pq\n\ntesting_dataset:\n  type: polars.EagerPolarsDataset\n  file_format: parquet\n  filepath: data/05_model_input/testing_dataset.pq\n\n# Models -----------------------------------------\nBelem_model:\n  type: climate_brazil.datasets.pymc_model_dataset.PyMCModelDataset\n  filepath: data/06_models/Belem_model.nc\n\nCuritiba_model:\n  type: climate_brazil.datasets.pymc_model_dataset.PyMCModelDataset\n  filepath: data/06_models/Curitiba_model.nc\n\nFortaleza_model:\n  type: climate_brazil.datasets.pymc_model_dataset.PyMCModelDataset\n  filepath: data/06_models/Fortaleza_model.nc\n\nGoiania_model:\n  type: climate_brazil.datasets.pymc_model_dataset.PyMCModelDataset\n  filepath: data/06_models/Goiania_model.nc\n\nMacapa_model:\n  type: climate_brazil.datasets.pymc_model_dataset.PyMCModelDataset\n  filepath: data/06_models/Macapa_model.nc\n\nManaus_model:\n  type: climate_brazil.datasets.pymc_model_dataset.PyMCModelDataset\n  filepath: data/06_models/Manaus_model.nc\n\nRecife_model:\n  type: climate_brazil.datasets.pymc_model_dataset.PyMCModelDataset\n  filepath: data/06_models/Recife_model.nc\n\nRio_model:\n  type: climate_brazil.datasets.pymc_model_dataset.PyMCModelDataset\n  filepath: data/06_models/Rio_model.nc\n\nSalvador_model:\n  type: climate_brazil.datasets.pymc_model_dataset.PyMCModelDataset\n  filepath: data/06_models/Salvador_model.nc\n\nSao_Luiz_model:\n  type: climate_brazil.datasets.pymc_model_dataset.PyMCModelDataset\n  filepath: data/06_models/Sao_Luiz_model.nc\n\nSao_Paulo_model:\n  type: climate_brazil.datasets.pymc_model_dataset.PyMCModelDataset\n  filepath: data/06_models/Sao_Paulo_model.nc\n\nVitoria_model:\n  type: climate_brazil.datasets.pymc_model_dataset.PyMCModelDataset\n  filepath: data/06_models/Vitoria_model.nc\n\n\n\n\n\n\nNote\n\n\n\nFor the sake of clarity, we added one entry for each city. You can use a Kedro’s dataset factory feature to capture all city models with one entry. We went over an example of how to use a dataset factory in part one of this series."
  },
  {
    "objectID": "posts/kedro-pymc-modelbuilder/index.html#forecasting-node",
    "href": "posts/kedro-pymc-modelbuilder/index.html#forecasting-node",
    "title": "Reproducibility & Scalability Part 2PyMC ModelBuilder",
    "section": "Forecasting Node",
    "text": "Forecasting Node\nFinally, we need to define how to generate our forecasts. We would like our forecasting node to have the ability to perform evaluations if we pass in the testing dataset as an argument and for the moment we just want to log out our evaluation metrics to stdout.\n# /src/climate_brazil/datasets/pymc_forecast_dataset.py\ndef forecast(model: TSModel, n_ahead: int, ground_truth: Optional[pl.DataFrame] = None) -&gt; xr.Dataset:\n    \"\"\"\n    Generates forecasts from trained time-series and produces evaluations if ground truth is passed in.\n    ---\n    Params:\n        model: The trained time-series model\n        n_ahead: The forecast horizon\n        ground_truth: The actual values to be compared with the forecasts\n    \"\"\"\n    forecasts = model.sample_posterior_predictive(n_ahead=n_ahead, extend_idata=True, combined=False)\n    if ground_truth is not None:\n        evaluations = model.evaluate(y_true=ground_truth.filter(pl.col(\"city\")==model.idata.attrs['city'])[\"temperature\"], forecasts=forecasts, back_transform=True)\n        logger.info(f\"{model.idata.attrs['city']} Evaluations: {evaluations}\")\n    return forecasts\nNotice that we need to pass in an n_ahead parameter that defines the forecast horizon. Now is a good time to add that parameter into /conf/base/parameters_ML.yml\ntesting_window: 12\nn_ahead: 12\n\nmodel_config:\n  error: 0.2\n  amplitude_trend: 1.0\n  ls_trend:\n    alpha: 48\n    beta: 2\n  beta_fourier:\n    mu: 0\n    sigma: 0.5\n\nsampler_config:\n  draws: 1000\n  tune: 1000\n  chains: 4\n  target_accept: 0.95\n  mp_ctx: spawn # might need this if you are running on MacOS\nOur forecast output will be an xarray dataset and we will go ahead and save that to disk in NETCDF format. Once again, we will create a custom Kedro dataset for that.\n# /src/climate_brazil/datasets/pymc_forecast_dataset.py\nfrom pathlib import Path, PurePosixPath\nfrom typing import Any\n\nimport xarray as xr\nfrom kedro.io import AbstractDataset\n\n\nclass PyMCForecastDataset(AbstractDataset):\n    \"\"\"\n    ``PyMCForecastDataset`` loads / save PyMC forecasts from a given filepath as an xarray Dataset.\n    \"\"\"\n\n    def __init__(self, filepath: str):\n        \"\"\"Creates a new instance of PyMCForecastDataset to load / save PyMC forecasts for a given filepath.\n\n        Args:\n            filepath: The location of the forecast netcdf file to load / save.\n        \"\"\"\n        self._filepath = PurePosixPath(filepath)\n        self._path = Path(self._filepath.parent)\n\n    def load(self) -&gt; xr.Dataset:\n        \"\"\"Loads data from the netcdf file.\n\n        Returns:\n            loaded forecasts\n        \"\"\"\n        return xr.load_dataset(self._filepath)\n\n    def save(self, forecast: xr.Dataset) -&gt; None:\n        \"\"\"Saves PyMC forecasts to the specified filepath.\"\"\"\n        self._path.mkdir(parents=True,  exist_ok=True)\n        forecast.to_netcdf(path=self._filepath)\n\n    def _describe(self) -&gt; dict[str, Any]:\n        \"\"\"Returns a dict that describes the attributes of the dataset.\"\"\"\n        return dict(filepath=self._filepath)"
  },
  {
    "objectID": "posts/kedro-pymc-modelbuilder/index.html#train",
    "href": "posts/kedro-pymc-modelbuilder/index.html#train",
    "title": "Reproducibility & Scalability Part 2PyMC ModelBuilder",
    "section": "Train",
    "text": "Train\nHere we are going to introduce tags, another Kedro feature that allows you to run specific nodes by tag category. At this stage our pipeline will be the training split node followed by the training node.\n# /src/climate_brazil/pipelines/ML/pipeline.py\nfrom kedro.pipeline import node, Pipeline, pipeline  # noqa\nfrom .nodes import train_test_split, train, forecast\n\ndef create_pipeline(**kwargs) -&gt; Pipeline:\n    split_train_nodes =  [\n        node(\n            func=train_test_split,\n            inputs=dict(\n                data = \"cities_temperatures_processed\",\n                testing_window=\"params:testing_window\"\n            ),\n            outputs=[\"training_dataset\", \"testing_dataset\"],\n            name=\"train_test_split\",\n            tags=[\"train\"]\n        ),\n        node(\n            func=train,\n            inputs=dict(\n                training_dataset=\"training_dataset\",\n                model_config=\"params:model_config\",\n                sampler_config=\"params:sampler_config\"\n            ),\n            outputs=dict(\n                Belem_model = \"Belem_model\",\n                Curitiba_model = \"Curitiba_model\",\n                Fortaleza_model = \"Fortaleza_model\",\n                Goiania_model = \"Goiania_model\",\n                Macapa_model = \"Macapa_model\",\n                Manaus_model = \"Manaus_model\",\n                Recife_model = \"Recife_model\",\n                Rio_model = \"Rio_model\",\n                Salvador_model = \"Salvador_model\",\n                Sao_Luiz_model = \"Sao_Luiz_model\",\n                Sao_Paulo_model = \"Sao_Paulo_model\",\n                Vitoria_model = \"Vitoria_model\"\n            ),\n            name=\"train\",\n            tags=[\"train\"]\n        )\n    ]\n\n\n\n\n\n\nCaution\n\n\n\nAlways make sure that your inputs and outputs match the keys you chose in catalog.yml"
  },
  {
    "objectID": "posts/kedro-pymc-modelbuilder/index.html#forecast",
    "href": "posts/kedro-pymc-modelbuilder/index.html#forecast",
    "title": "Reproducibility & Scalability Part 2PyMC ModelBuilder",
    "section": "Forecast",
    "text": "Forecast\nThis is where we are going to leverage the flexibility of tags. We are going to create two sub-pipelines for our forecasts. The first sub-pipeline will be for training purposes, this is when we have ground-truth data to pass in for evaluations. The second will be when we are using our time-series model in production and we do not have the ground-truth available. Our full ML pipeline will look like this:\n# /src/climate_brazil/pipelines/ML/pipeline.py\nfrom kedro.pipeline import node, Pipeline, pipeline  # noqa\nfrom .nodes import train_test_split, train, forecast\n\ncities = [\"Belem\", \"Curitiba\", \"Fortaleza\", \"Goiania\", \"Macapa\", \"Manaus\", \"Recife\", \"Rio\", \"Salvador\", \"Sao_Luiz\", \"Sao_Paulo\", \"Vitoria\"]\n\ndef create_pipeline(**kwargs) -&gt; Pipeline:\n    split_train_nodes =  [\n        node(\n            func=train_test_split,\n            inputs=dict(\n                data = \"cities_temperatures_processed\",\n                testing_window=\"params:testing_window\"\n            ),\n            outputs=[\"training_dataset\", \"testing_dataset\"],\n            name=\"train_test_split\",\n            tags=[\"train\"]\n        ),\n        node(\n            func=train,\n            inputs=dict(\n                training_dataset=\"training_dataset\",\n                model_config=\"params:model_config\",\n                sampler_config=\"params:sampler_config\"\n            ),\n            outputs=dict(\n                Belem_model = \"Belem_model\",\n                Curitiba_model = \"Curitiba_model\",\n                Fortaleza_model = \"Fortaleza_model\",\n                Goiania_model = \"Goiania_model\",\n                Macapa_model = \"Macapa_model\",\n                Manaus_model = \"Manaus_model\",\n                Recife_model = \"Recife_model\",\n                Rio_model = \"Rio_model\",\n                Salvador_model = \"Salvador_model\",\n                Sao_Luiz_model = \"Sao_Luiz_model\",\n                Sao_Paulo_model = \"Sao_Paulo_model\",\n                Vitoria_model = \"Vitoria_model\"\n            ),\n            name=\"train\",\n            tags=[\"train\"]\n        )\n    ]\n\n    forecast_training_nodes = [\n        node(\n            func=forecast,\n            inputs=dict(\n                model=f\"{city}_model\",\n                n_ahead=\"params:n_ahead\",\n                ground_truth=\"testing_dataset\"\n            ),\n            outputs=f\"{city}_forecasts_evaluation\",\n            name=f\"{city}_forecasts_evaluation\",\n            tags=[\"train\"]\n        )\n        for city in cities\n    ]\n\n    forecast_nodes = [\n        node(\n            func=forecast,\n            inputs=dict(\n                model=f\"{city}_model\",\n                n_ahead=\"params:n_ahead\"\n            ),\n            outputs=f\"{city}_forecasts\",\n            name=f\"{city}_forecast\",\n            tags=[\"forecast\"]\n        )\n        for city in cities\n    ]\n\n    return pipeline(split_train_nodes + forecast_training_nodes + forecast_nodes)"
  },
  {
    "objectID": "posts/kedro-pymc-modelbuilder/index.html#defining-custom-pipelines",
    "href": "posts/kedro-pymc-modelbuilder/index.html#defining-custom-pipelines",
    "title": "Reproducibility & Scalability Part 2PyMC ModelBuilder",
    "section": "Defining Custom Pipelines",
    "text": "Defining Custom Pipelines\nPreviously, we let Kedro automatically register our pipelines based on the project pipelines we created. However, we now have sub-pipelines that we would like to define based on our tags. We can do so in src/climate_brazil/pipeline_registry.py:\n#src/climate_brazil/pipeline_registry.py\n\"\"\"Project pipelines.\"\"\"\n\nfrom kedro.framework.project import find_pipelines\nfrom kedro.pipeline import Pipeline\n\nfrom climate_brazil.pipelines.data_processing import create_pipeline as dp\nfrom climate_brazil.pipelines.ML import create_pipeline as ml\n\ndef register_pipelines() -&gt; dict[str, Pipeline]:\n    \"\"\"Register the project's pipelines.\n\n    Returns:\n        A mapping from pipeline names to ``Pipeline`` objects.\n    \"\"\"\n    dp_pipeline = dp()\n    ml_pipeline = ml()\n    training_pipeline = ml_pipeline.only_nodes_with_tags(\"train\")\n    forecast_pipeline = ml_pipeline.only_nodes_with_tags(\"forecast\")\n\n    return {\n        \"data_processing\": dp_pipeline,\n        \"train\": dp_pipeline + training_pipeline,\n        \"forecast\": forecast_pipeline,\n        \"__default__\": dp_pipeline + training_pipeline\n    }\nNotice how we broke down our ML pipeline into two sub-pipelines. Also, notice how we defined the train pipeline to consist of both the data processing pipeline and the training pipeline. We also assigned the train pipeline to be the default pipeline. This means if we execute kedro run without specifying a specific pipeline it will run the train pipeline."
  },
  {
    "objectID": "posts/kedro-pymc-modelbuilder/index.html#training-pipeline",
    "href": "posts/kedro-pymc-modelbuilder/index.html#training-pipeline",
    "title": "Reproducibility & Scalability Part 2PyMC ModelBuilder",
    "section": "Training Pipeline",
    "text": "Training Pipeline\nYou can execute your training pipeline by either executing\nkedro run\nor\nkedro run --pipeline train\nLet’s go ahead and run one of the above to train the model and generate forecasts on the test set with evaluations."
  },
  {
    "objectID": "posts/kedro-pymc-modelbuilder/index.html#forecasting-pipeline",
    "href": "posts/kedro-pymc-modelbuilder/index.html#forecasting-pipeline",
    "title": "Reproducibility & Scalability Part 2PyMC ModelBuilder",
    "section": "Forecasting pipeline",
    "text": "Forecasting pipeline\nIf you already have a trained model and you don’t have the ground truth for evaluations you can execute\nkedro run --pipeline forecast"
  },
  {
    "objectID": "posts/kedro-pymc-modelbuilder/index.html#visualizing-forecasts",
    "href": "posts/kedro-pymc-modelbuilder/index.html#visualizing-forecasts",
    "title": "Reproducibility & Scalability Part 2PyMC ModelBuilder",
    "section": "Visualizing Forecasts",
    "text": "Visualizing Forecasts\nWe’ve seen our evaluation metrics logged to stdout, which by the way is written to /info.log, let’s also look at our fitted model and forecasts against the actual data. To do that let’s add a node that will handle generating these plots in our pipeline.\ndef forecast_plot(training_dataset: pl.DataFrame, testing_dataset: pl.DataFrame, model: TSModel, forecast: xr.Dataset) -&gt; go.Figure:\n    \"\"\"\n    Generates plot showing in-sample posterior predictive performance as well as out-of-sample forecasts\n    ---\n    Params:\n        training_dataset: The training split\n        testing_dataset: The testing split\n        model: the trained model\n        forecasts: the forecast from the trainined model\n    \"\"\"\n    # City specific data\n    city = model.idata.attrs['city']\n    city_training_dataset = training_dataset.filter(pl.col(\"city\")==city)\n    city_testing_dataset = testing_dataset.filter(pl.col(\"city\")==city)\n\n    # Model fit posterior predictive mean and HDI\n    posterior_mean_normalized = model.idata.posterior_predictive['temperature_normalized'].mean(('chain', 'draw'))\n    hdi_normalized = az.hdi(model.idata.posterior_predictive['temperature_normalized'], hdi_prob=0.8)\n    posterior_mean = posterior_mean_normalized * model.idata.attrs['y_std'] + model.idata.attrs['y_mean']\n    hdi = hdi_normalized * model.idata.attrs['y_std'] + model.idata.attrs['y_mean']\n\n    # Forecast posterior predictive mean and HDI\n    posterior_predictive_mean_normalized = forecast['temperature_normalized_fut'].mean(('chain', 'draw'))\n    posterior_predictive_hdi_normalized = az.hdi(forecast['temperature_normalized_fut'], hdi_prob=0.8)\n    posterior_predictive_mean = posterior_predictive_mean_normalized * model.idata.attrs['y_std'] + model.idata.attrs['y_mean']\n    posterior_predictive_hdi = posterior_predictive_hdi_normalized * model.idata.attrs['y_std'] + model.idata.attrs['y_mean']\n\n    fig = go.Figure()\n    fig.add_traces(\n        [\n            go.Scatter(\n                name=\"\", \n                x=city_training_dataset[\"date\"], \n                y=hdi[\"temperature_normalized\"][:, 1], \n                mode=\"lines\", \n                marker=dict(color=\"#eb8c34\"), \n                line=dict(width=0), \n                legendgroup=\"HDI\",\n                showlegend=False\n            ),\n            go.Scatter(\n                name=\"80% HDI\", \n                x=city_training_dataset[\"date\"], \n                y=hdi[\"temperature_normalized\"][:, 0], \n                mode=\"lines\", marker=dict(color=\"#eb8c34\"), \n                line=dict(width=0), \n                legendgroup=\"HDI\", \n                fill='tonexty', \n                fillcolor='rgba(235, 140, 52, 0.5)'\n            ),\n            go.Scatter(\n                x = city_training_dataset[\"date\"],\n                y = city_training_dataset[\"temperature\"],\n                mode=\"markers\",\n                marker_color=\"#48bbf0\",\n                name=\"actuals\",\n                legendgroup=\"actuals\"\n            ),\n            go.Scatter(\n                x = city_training_dataset[\"date\"],\n                y = posterior_mean,\n                marker_color=\"blue\",\n                name=\"posterior_mean\",\n                legendgroup=\"posterior_mean\"\n            ),\n            go.Scatter(\n                name=\"\", \n                x=city_testing_dataset[\"date\"], \n                y=posterior_predictive_hdi[\"temperature_normalized_fut\"][:, 1], \n                mode=\"lines\", \n                marker=dict(color=\"#eb8c34\"), \n                line=dict(width=0), \n                legendgroup=\"HDI\",\n                showlegend=False\n            ),\n            go.Scatter(\n                name=\"\", \n                x=city_testing_dataset[\"date\"], \n                y=posterior_predictive_hdi[\"temperature_normalized_fut\"][:, 0], \n                mode=\"lines\", marker=dict(color=\"#eb8c34\"), \n                line=dict(width=0), \n                legendgroup=\"HDI\", \n                fill='tonexty', \n                fillcolor='rgba(235, 140, 52, 0.5)',\n                showlegend=False\n            ),\n            go.Scatter(\n                x = city_testing_dataset[\"date\"],\n                y = city_testing_dataset[\"temperature\"],\n                mode=\"markers\",\n                marker_color=\"#48bbf0\",\n                name=\"\",\n                legendgroup=\"actuals\",\n                showlegend=False\n            ),\n            go.Scatter(\n                x = city_testing_dataset[\"date\"],\n                y = posterior_predictive_mean,\n                mode=\"lines\",\n                marker_color=\"yellow\",\n                name=\"\",\n                legendgroup=\"posterior_mean\",\n                showlegend=False\n            ),\n        ]\n    )\n    fig.update_layout(\n        title = f\"{city.title()} Temperature Forecast\",\n        xaxis=dict(\n                title=\"Date\",\n                rangeselector=dict(\n                    buttons=list(\n                        [\n                            dict(count=1, label=\"1y\", step=\"year\", stepmode=\"backward\"),\n                            dict(count=5, label=\"5y\", step=\"year\", stepmode=\"backward\"),\n                            dict(count=10, label=\"10y\", step=\"year\", stepmode=\"backward\"),\n                            dict(step=\"all\", label=\"All\"),\n                        ]\n                    )\n                ),\n                rangeslider=dict(visible=True),\n                type=\"date\",\n                rangeselector_font_color=\"black\",\n                rangeselector_activecolor=\"hotpink\",\n                rangeselector_bgcolor=\"lightblue\",\n                autorangeoptions=dict(clipmax=city_testing_dataset['date'].max() + timedelta(days=30), clipmin=city_training_dataset['date'].min() - timedelta(days=30))\n            ),\n        yaxis=dict(\n            title=\"Temperature\"\n        )\n    )\n    return fig\nLet’s make sure we define the outputs from our node so that we persist the figures to disk.\n# Forecast plots: This is a dataset factory that will save all of our figures\n\"{city}_plot\":\n  type: plotly.JSONDataset\n  filepath: data/08_reporting/{city}_plot.json\nNext, we add the node to our pipeline.\n# Add to /src/climate_brazil/pipelines/ML/pipeline.py\nplot_node = [\n    node(\n        func=forecast_plot,\n        inputs=dict(\n            training_dataset=\"training_dataset\",\n            testing_dataset=\"testing_dataset\",\n            model=f\"{city}_model\",\n            forecast=f\"{city}_forecasts_evaluation\"\n        ),\n        outputs=f\"{city}_plot\",\n        name=f\"{city}_forecast_plot\",\n        tags=[\"train\", \"forecast\"]\n    )\n    for city in cities\n]\n\nreturn pipeline(\n    split_train_nodes \n    + forecast_training_nodes \n    + forecast_nodes \n    + plot_node\n)\nGreat! Now, let’s only execute our forecast_plot node since we’ve already ran the pipeline that generates our models and forecasts.\nkedro run --nodes forecast_plot\nThat will generate the following figures:\n\n\n\nBelemCuritibaFortalezaGoianiaMacapaManausRecifeRioSalvadorSao LuizSao PauloVitoria\n\n\n\n\n                                                \n\n\n\n\n                                                \n\n\n\n\n\n\n                                                \n\n\n\n\n                                                \n\n\n\n\n\n\n                                                \n\n\n\n\n                                                \n\n\n\n\n\n\n                                                \n\n\n\n\n                                                \n\n\n\n\n\n\n                                                \n\n\n\n\n                                                \n\n\n\n\n\n\n                                                \n\n\n\n\n                                                \n\n\n\n\n\n\n                                                \n\n\n\n\n                                                \n\n\n\n\n\n\n                                                \n\n\n\n\n                                                \n\n\n\n\n\n\n                                                \n\n\n\n\n                                                \n\n\n\n\n\n\n                                                \n\n\n\n\n                                                \n\n\n\n\n\n\n                                                \n\n\n\n\n                                                \n\n\n\n\n\n\n                                                \n\n\n\n\n                                                \n\n\n\n\n\nLooks good! We now have our training and forecasting pipelines up and running."
  },
  {
    "objectID": "posts/embeddings_deepdive/index.html",
    "href": "posts/embeddings_deepdive/index.html",
    "title": "How Machines Comprehend LanguageIntroduction to Vector Embeddings",
    "section": "",
    "text": "In this first post of a series on Natural Language Processing (NLP), we introduce the foundational techniques for converting textual language into numerical representations that can be used in a wide range of downstream tasks, including text classification, machine translation, question answering, chatbots, and more."
  },
  {
    "objectID": "posts/embeddings_deepdive/index.html#sparse-embeddings",
    "href": "posts/embeddings_deepdive/index.html#sparse-embeddings",
    "title": "How Machines Comprehend LanguageIntroduction to Vector Embeddings",
    "section": "Sparse Embeddings",
    "text": "Sparse Embeddings\nA sparse embedding is one in which the majority of the values of the vector are 0. The simplest case of a sparse embedding is a one-hot representation of a vocabulary.\nFor example, let’s assume that a given vocabular consists of \\(N = 30,000\\) words and we want to represent the words in the sentence “The quick brown fox jumps over the lazy dog.” with a sparse one-hot representation. We would represent each word as a vector of size \\(N = 30,000\\) where out of those 30,000 entries only one entry is non-zero; The position of the vector that corresponds with that specific word in the sentence. An example of a sparse vector can be seen in Table 1, here we assume our entire vocabulary is only the six words that compose the prior example sentence from above.\n\n\n\n\nTable 1: A sparse vector encoding the word fox\n\n\n\n\n\n\n\n\n\n\n\n\n\nbrown\ndog\nfox\njumps\nlazy\nover\nquick\nthe\n\n\n\n\nfox\n0\n0\n1\n0\n0\n0\n0\n0\n\n\n\n\n\n\n\n\n\n\n\nCo-occurrence Matrices\nOne of the earliest tools we had to understanding word relationships and semantics were these co-occurence matrices. These matrices represent how frequently words occur together.\nThere are a few flavors of a co-occurrence matrix:\n\nWindow-based co-occurrence: This is when the rows of the matrix are the words in your vocabulary, the columns are context words that are i positions adjacent to the target word, and the cells in the matrix represent the counts of co-occurrence Table 2.\nSentence-based co-occurrence: The matrix rows and columns are the same as with window-based, however, now the cells in the matrix represent the count of sentences the target word co-occurs with the context word.\nDocument-based co-occurrence: The words in the vocabulary still represent the rows of the matrix, however, now each column represents a target document in your corpus, and the cells represent the counts of occurence of the target word inside the target document.\n\n\n\n\n\nTable 2: A window based co-occurrence matrix. (Window of size 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\nbrown\ndog\nfox\njumps\nlazy\nover\nquick\nthe\n\n\n\n\nbrown\n0\n0\n1\n1\n0\n1\n1\n1\n\n\ndog\n0\n0\n0\n0\n1\n1\n0\n1\n\n\nfox\n1\n0\n0\n1\n0\n1\n1\n2\n\n\njumps\n1\n0\n1\n0\n1\n1\n1\n1\n\n\nlazy\n0\n1\n0\n1\n0\n1\n0\n1\n\n\nover\n1\n1\n1\n1\n1\n0\n0\n1\n\n\nquick\n1\n0\n1\n1\n0\n0\n0\n1\n\n\nthe\n1\n1\n2\n1\n1\n1\n1\n0\n\n\n\n\n\n\n\n\n\n\nAs you may have already guessed, using the raw frequencies of words occuring besides words, in sentences, or in documents is not adequate in measuring the relationships of words. There are algorithms that improve upon raw frequency co-occurence.\n\n\nPostive Pointwise Mutual Information\nFor window and sentence based co-occurence, we can, generally, get better word relationships by using the Positive Pointwise Mutual Information (PPMI) algorithm. The PPMI algorithm is based on the Pointwise Mutual Information (PMI) equation:\n\\[\nPMI_{\\text(target\\_word, context\\_word)} = log(\\frac{P\\text(target\\_word, context\\_word)}{P\\text(target\\_word) \\times P\\text(context\\_word)})\n\\]\nThe PMI is a ratio of the joint probability of two words, in this case, and the product of the marginal probabilities of each individual word. If two words are independent then the ratio would be one due to \\[\nP(A, B) = P(A) \\times P(B)\n\\]\nThus, the PMI measures how much more or less two words co-occur than we would expect if they were independent.\nAs it turns out, figuring out how much less two words co-occur requires us to have a very large corpus which in many cases is not feesible and that is why we use the PPMI instead to only measure how much more two words co-occur than expected.\n\\[\nPPMI_{\\text(target\\_word, context\\_word)} = max(PMI_{\\text(target\\_word, context\\_word)}, 0)\n\\]\n\n\n\n\nTable 3: A mock example of a PPMI matrix\n\n\n\n\n\n\n\n\n\n\n\n\n\nbrown\ndog\nfox\njumps\nlazy\nover\nquick\nthe\n\n\n\n\nbrown\n2.061778\n0.000000\n0.000000\n0.000000\n3.098164\n0.000000\n1.081301\n0.612111\n\n\ndog\n2.781560\n1.348790\n0.235574\n0.518801\n0.000000\n0.821469\n0.000000\n1.544268\n\n\nfox\n3.613986\n0.000000\n0.000000\n0.000000\n0.095746\n4.239149\n1.178170\n0.000000\n\n\njumps\n0.000000\n0.000000\n1.569397\n0.000000\n0.000000\n0.154627\n0.000000\n0.707784\n\n\nlazy\n0.000000\n0.000000\n2.548816\n1.203726\n0.000000\n0.000000\n0.273880\n1.798835\n\n\nover\n2.127806\n0.883317\n0.000000\n1.176950\n1.817565\n0.000000\n0.208571\n0.000000\n\n\nquick\n0.000000\n0.973982\n0.799991\n0.000000\n1.615078\n1.297567\n0.000000\n1.089456\n\n\nthe\n5.043771\n1.903497\n3.815204\n1.683924\n0.000000\n0.000000\n0.000000\n3.389794\n\n\n\n\n\n\n\n\n\n\n\n\nTerm Frequency Inverse Document Frequency\nFor document based co-occurence, we can improve upon raw frequency co-occurence matrices by using the Term Frequency Inverse Document Frequency (TF-IDF) algorithm. The algorithm is quite simple, we compute the term frequency which is defined as:\n\\[\ntf_{term, document} =\n\\begin{cases}\n  1 + log_{10}(count(term, document)), & \\text{if } count(term, document) \\gt 0 \\\\\n  0, & \\text{otherwise}\n\\end{cases}\n\\]\nThe term frequency is a count of a given term in a specific document that is then squashed down with a \\(log_{10}\\) transformation. Next, we compute the inverse document frequency which is simply:\n\\[\nidf_{term} = \\log_{10}(\\frac{\\text{Total Number of Documents}}{\\text{Number of Documents that includes term}})\n\\]\nFinally, the TF-IDF weighted value for a particular term \\(w_{term}\\) is the product of the term frequency and the inverse document frequency:\n\\[\nw_{term} = tf_{term, document} \\times idf_{term}\n\\]\nThe TF-IDF algorithm is typically used in information retrieval when you are looking for a particular document based on some query composed of a handful of keywords.\n\n\n\n\nTable 4: A mock example of a TF-IDF matrix\n\n\n\n\n\n\n\n\n\n\n\n\n\ndocument1\ndocument2\ndocument3\n\n\n\n\nbrown\n3.895134\n3.277852\n2.221267\n\n\ndog\n3.085356\n4.432795\n5.084841\n\n\nfox\n2.901154\n2.434501\n4.365887\n\n\njumps\n2.638515\n3.290854\n1.547791\n\n\nlazy\n1.905576\n2.352809\n3.361879\n\n\nover\n2.986068\n3.196224\n4.278071\n\n\nquick\n4.035052\n4.113825\n2.908665\n\n\nthe\n3.921081\n3.286601\n3.428881"
  },
  {
    "objectID": "posts/embeddings_deepdive/index.html#dense-embeddings",
    "href": "posts/embeddings_deepdive/index.html#dense-embeddings",
    "title": "How Machines Comprehend LanguageIntroduction to Vector Embeddings",
    "section": "Dense Embeddings",
    "text": "Dense Embeddings\nIt turns out that we can represent word relationships and semantics with much shorter vectors that are real valued and dense. Unlike the previous sparse vectors we talked about these vectors are not mostly zero-counts and since they are not counts they can be negative. In addition, these dense vectors perform better than their sparse counterparts in almost every downstream Natural Language Processing (NLP) task. One thing to note is that these dense vectors no longer have a clear interpretation, unlike the sparse vectors that represent the co-occurence counts of words, words in sentences, or words in documents.\nThere are two types of dense embeddings:\n\nStatic Embeddings Table 5\nContextual/Dynamic Embeddings\n\n\n\n\n\nTable 5: A dense vector encoding of dimension 16 statically representing the word fox\n\n\n\n\n\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n\n\nfox\n-1.149424\n0.10549\n1.213764\n0.152841\n-1.39322\n0.787253\n0.973366\n2.149337\n0.80126\n0.051735\n0.229991\n1.382955\n1.005908\n0.458693\n-0.048577\n0.498065\n\n\n\n\n\n\n\n\n\n\n\nStatic Embeddings\nWhen we say an embedding is static we mean that each word in our vocabulary only has one vector associated with it. You may already notice an obvious limitation with this approach when a word has multiple meanings. For example, the word ship can refer to the action verb of shipping an item or to the noun of a water vessel. A static embedding would represent both meanings of the word “ship” with the same vector. The implication is that a static embedding can not disambigouate different meanings of the same word.\n\n\n\n\n\n\nNote\n\n\n\nWords that have multiple word senses are called polysemous words\n\n\nLimitations put aside, there are two popular algorithms to generate dense static embedding for NLP tasks are:\n\nWord2Vec\n\nContinuous Bag-of-word\nSkip-gram with negative sampling\n\nGlobal Vectors for Word Representation (GloVE)\n\n\nWord2Vec - Skip-Gram with Negative Sampling\nWord2Vec is the name of a software that produces static embeddings. Word2Vec has two different algorithms to produce these embeddings:\n\nContinuos Bag-of-Words\nSkip-gram with Negative Sampling\n\nWe will focus on the skip-gram method because, generally, it performs better. Skip-gram with negative sampling is a self-supervised method that works by training a logistic regression to predict if a context word is likely to occur with a target word. The true samples are generated by using a sliding window around each target word, while the negative samples are randomly sampled words from the corpus. The prediction task, however, is not the main outcome the key is that the weights that the classifier learns for each token are taken to be the embeddings. The implication is that if the prediction task is successful then the weights (each word has its own weight) encode a sufficiently good relationship between the words to achieve this task.\n\n\n\n\n\n\nNote\n\n\n\nThis section is purposely sparse because we will have an entire post dedicated to diving deep into Skip-gram with negative sampling word2vec embeddings including implementing them from scratch\n\n\n\n\nGlobal Vectors for Word Representation\nGlobal Vectors (GloVE) generates embeddings by leveraging ratios of conditional probabilities derived from a word-word co-occurence matrix. Unlike skip-gram that generates embeddings locally around a sliding window, GloVE embeddings are global. Also unlike skip-gram, GloVE learns the embeddings directly by optimizing word vectors against the log of the co-occurence counts between the two words the vectors are representing. Both GloVE and Word2Vec generate good static embeddings, however when it comes to interpratibility GloVE has the advantage due to the following properties:\n\nYou can directly inspect matrix relationships which are composed directly of probabilistic meaning\nGloVE learns the embeddings based on probability ratios which encourage semantic analogies; like the famous example of king - man + woman = queen\n\n\n\n\n\n\n\nNote\n\n\n\nThis section is purposely sparse because we will have an entire post dedicated to diving deep into GloVE embeddings including implementing them from scratch\n\n\n\n\n\nContextual Embeddings\nA step up from static embeddings are contextual embeddings. These embeddings capture the meaning of words in the given context as opposed to the general meaning of the word that we get with static embeddings. Generally, contextual embeddings are learned by transformer models that have multiple attention heads. These embeddings are also dynamic in that unlike static embeddings that have a fixed embedding layer, contextual embeddings are computed on the fly using a combination of a static embedding layer, a segment embedding layer to differentiate between input sentences and a positional embedding layer used to pass through information on the position of a word within a sentence.\n\n\n\n\n\n\nNote\n\n\n\nWe will do a deep dive on transformer models and how they work in a dedicated post\n\n\n\nBidirectional Encoder Respresentation Transformer (BERT)\nArguably the most popular transformer model that produces contextual embeddings is BERT. The key properties that allow BERT to learn contextual embeddings are:\n\nBidirectional self-attention which allows contextualization over the entire sentence instead of only from the previous words.\nMultiple self-attention heads to capture different linguistic relationships like syntax, co-reference, or long-range interactions.\n\nThe model is pre-trained on masked language modeling (MLM) and next sentence prediction (NSP) tasks. Word level representation is mostly learned via the MLM task. Whereas, the NSP task was designed to encode sentence relationship into the model to be used for downstream tasks such as paraphrase detection, detecting if two sentences agree or disagree, or detecting if a sentence coherently follows another sentence. However, after further research it seems that NSP is not very effective and is not actually required to be able to use BERT or its descendants for sentence relationship tasks.\n\n\n\n\n\n\nNote\n\n\n\nThis section is purposely sparse because we will have an entire post dedicated to diving deep into the BERT architecture including implementing it from scratch"
  },
  {
    "objectID": "posts/embeddings_deepdive/index.html#embedding-projections",
    "href": "posts/embeddings_deepdive/index.html#embedding-projections",
    "title": "How Machines Comprehend LanguageIntroduction to Vector Embeddings",
    "section": "Embedding Projections",
    "text": "Embedding Projections\nTo inspect our embeddings visually we are going to project from our high-dimensional space of ~35,000 down to 3-dimensions. In order to acheive this projection we are going to use the UMAP (Uniform Manifold Approximation and Projection) algorithm. The algorithm works by constructing a weighted graph in the original space and then learns via stochastic gradient descent (minimizes the cross entropy loss) to project a space in the reduced space that maintains local and global distances of points.\n\n\n\n\n\n\nCaution\n\n\n\nYou might be thinking to yourself “Oh, we are going from a space of ~35,000 to a latent space of 3, so we are generating an embedding of an embedding!” and you would be correct. However, remember that embeddings are not created equal. The TF-IDF embeddings are constructed to emphasize symantic representation of the words across the documents while the UMAP embeddings are constructed to capture the structure of the data in the high dimensional space and maintain it in the latent space.\n\n\n\nUMAP Details (optional)\nThe first step of the algorithm is to find a set of \\(k\\) nearest neighbors for each point in \\(x_{i} \\in \\mathbb{R^{D}}\\) (in our case the dimension is the number of documents) using a distance metric \\(d(x_{i}, x_{j})\\) (we will use cosine as our distance metric).\nNext, for each neighbor \\(x_{j}\\) of \\(x_{i}\\) we compute a weighted edge\n\\[\nw_{ij} = exp( - \\frac{d(x_{i}, x_{j}) - \\rho_{i}}{\\sigma_{i}} )\n\\]\nWhere \\(\\rho_{i}\\) is the local connectivity which represents the distance to the nearest neighbor of \\(x_{i}\\) and \\(\\sigma_{i}\\) is a smoothness parameter that is chosen to induce the entropy of the distribution to be \\(log(k)\\)\nAt this point we have a directed weighted graph from each point \\(x_{i}\\) to its neighbor \\(x_{j}\\), but a directed graph by definition violates mutual similarity \\(w_{ij} \\ne w_{ji}\\) which is why UMAP converts the graph to an undirected graph. To convert our directed graph to an undirected one we use a probabilistic union of edges\n\\[\nP_{ij} = w_{ij} + w_{ji} - w_{ij}w_{ji}\n\\]\nNext we initialize our coordinates in low-dimensional space \\(y_{i} \\in \\mathbb{R^{d}}\\) here you can use random initialization or you can start with PCA (Principal Component Analysis) or some other advanced method (we will use PCA).\nUMAP defines edge weights in the low dimensional space using a parametric curve\n\\[\nQ_{ij} = \\frac{1}{1 + a||y_{i} - y_{j}||^{2b}}\n\\]\nwhere \\(||y_{i} - y_{j}||\\) is the euclidean distance and \\(a,b\\) are learned constants that influence how tight/spread out clusters are and is controlled by the min_dist argument, which defaults to 0.1. \\(a, b\\) are estimated such that as the distance \\(d\\) increases the similarity function \\(f(d)\\) decays toward zero.\n\\[\nf(d) = \\frac{1}{ 1 + ad^{2b}}\n\\]\nThe function satisfies \\(f(0) = 1\\) representing maximal similarity, and controls how rapidly points are pulled together or pushed apart in the embedding space.\nFinally, UMAP uses stochastic gradient descent to minimize the cross entropy between \\(Q_{ij}\\) and \\(P_{ij}\\) where the cross entropy loss is defined as\n\\[\nL = \\sum_{i &lt; j} [-P_{ij}logQ_{ij} - (1-P_{ij})log(1 - Q_{ij})]\n\\]"
  },
  {
    "objectID": "posts/embeddings_deepdive/index.html#visualize-embeddings-in-3d-space",
    "href": "posts/embeddings_deepdive/index.html#visualize-embeddings-in-3d-space",
    "title": "How Machines Comprehend LanguageIntroduction to Vector Embeddings",
    "section": "Visualize Embeddings in 3D Space",
    "text": "Visualize Embeddings in 3D Space\nBelow you can clearly see that the TF-IDF embeddings encode a representation that is able to discriminate between the three filtered topics in our toy dataset.\n\n\n\n\n\n\nImportant\n\n\n\nIt is not necessary that the topics be seperable at the reduced dimensional space that our UMAP algorithm generates for these topics to be seperable in their original (TF-IDF) dimension. You may encounter cases where algorithms like Multinomial Regression or Multinomial Naive Bayes can still classify the texts even when your UMAP representation can’t separate the classes.\n\n\n\nimport umap\numap_model = umap.UMAP(n_components=3, metric=\"cosine\", init=\"pca\")\nprojected_embeddings = umap_model.fit_transform(train_tf_idf.T)\n\n\nlabels_mapping = {k: v for k, v in zip(range(3), newsgroups_train.target_names)}\nprojected_embeddings_df = pd.DataFrame(projected_embeddings)\nprojected_embeddings_df.columns = ['x', 'y', 'z']\nprojected_embeddings_df['document_class'] = [labels_mapping.get(l) for l in newsgroups_train.target]\n\n\nimport plotly.express as px\nfig = px.scatter_3d(projected_embeddings_df, x='x', y='y', z='z', color=\"document_class\")\nfig.update_layout(\n    shapes=[\n        dict(\n            type='rect',\n            xref='paper',\n            yref='paper',\n            x0=0.0, x1=1.0,\n            y0=0.0, y1=1.0,\n            line={'width': 3}\n        )\n    ]\n)"
  },
  {
    "objectID": "posts/kedro-mlflow/index.html",
    "href": "posts/kedro-mlflow/index.html",
    "title": "Reproducibility & Scalability Part 3MLFlow Integration",
    "section": "",
    "text": "In part three of this series, we configure MLFlow for production by defining our tracking server as a containerized PostgreSQL database and our artifact store as containerized MinIO storage. We then integrate MLFlow into our Kedro project in order to start tracking our model development."
  },
  {
    "objectID": "posts/kedro-mlflow/index.html#backend-store",
    "href": "posts/kedro-mlflow/index.html#backend-store",
    "title": "Reproducibility & Scalability Part 3MLFlow Integration",
    "section": "Backend Store",
    "text": "Backend Store\nAs we mentioned earlier, we are going to use PostgreSQL to store our metrics, parameters, and other metadata. In a docker compose file, let’s define our PostgreSQL service. You can name the service however you like; we will name it mlflow-logs. We also need to define a username, password, and database name for our service. We will name our database mlflowdb. We are going to pass in the username and password as an environment variable using secrets. For that you need to make sure you set those variables in your environment. On Linux/MacOS you can set an environment variable like so:\nexport POSTGRES_USER=some_user_name\nOn Windows using PowerShell you can set it like this:\n$env:POSTGRES_USER = \"some_user_name\"\nIn addition, we need to make sure that what we log into our database persists after the container is brought down. We will define a docker volume and call it pg_mlflow_db. Our docker-compose.yml file looks like this so far:\n#./docker-compose.yml\nservices:\n  mlflow-logs:\n    image: postgres:16.4\n    secrets:\n      - POSTGRES_USER\n      - POSTGRES_PASSWORD\n    environment:\n      - POSTGRES_USER_FILE=/run/secrets/POSTGRES_USER\n      - POSTGRES_PASSWORD_FILE=/run/secrets/POSTGRES_PASSWORD\n      - POSTGRES_DB=mlflowdb\n    ports:\n      - 5432:5432\n    volumes:\n      - pg_mlflow_db:/var/lib/postgresql/data\n\nvolumes:\n  pg_mlflow_db:\n\nsecrets:\n  POSTGRES_USER:\n    environment: POSTGRES_USER\n  POSTGRES_PASSWORD:\n    environment: POSTGRES_PASSWORD\n\n\n\n\n\n\nCaution\n\n\n\nNote how we are using POSTGRES_USER_FILE instead of POSTGRES_USER. This is a convention used by many docker images for passing in secrets."
  },
  {
    "objectID": "posts/kedro-mlflow/index.html#artifact-store",
    "href": "posts/kedro-mlflow/index.html#artifact-store",
    "title": "Reproducibility & Scalability Part 3MLFlow Integration",
    "section": "Artifact Store",
    "text": "Artifact Store\nWe need a place to store larger file objects like our models, figures, or artifacts. For that, we are going to define another service in our docker-compose.yml that will host a MinIO S3-compatible object store. Again, we define a username and password like we did before. Let’s also define a default bucket that will be created on creation of the service where we will store our artifacts. Again, like we did for our backend store our artifact store needs a persistant volume. So far we have:\n#./docker-compose.yml\nservices:\n  mlflow-logs:\n    image: postgres:16.4\n    secrets:\n      - POSTGRES_USER\n      - POSTGRES_PASSWORD\n    environment:\n      - POSTGRES_USER_FILE=/run/secrets/POSTGRES_USER\n      - POSTGRES_PASSWORD_FILE=/run/secrets/POSTGRES_PASSWORD\n      - POSTGRES_DB=mlflowdb\n    ports:\n      - 5432:5432\n    volumes:\n      - pg_mlflow_db:/var/lib/postgresql/data\n  # New MinIO service\n  minio:\n    container_name: minio\n    image: 'bitnami/minio:latest'\n    ports:\n      - '9000:9000'\n      - '9001:9001'\n    secrets:\n      - AWS_ACCESS_KEY_ID\n      - AWS_SECRET_ACCESS_KEY\n    environment:\n      - MINIO_ROOT_USER_FILE=/run/secrets/AWS_ACCESS_KEY_ID\n      - MINIO_ROOT_PASSWORD_FILE=/run/secrets/AWS_SECRET_ACCESS_KEY\n      - MINIO_DEFAULT_BUCKETS=mlflow\n    volumes:\n      - minio_data:/bitnami/minio/data\n\nvolumes:\n  pg_mlflow_db:\n  minio_data:\n\nsecrets:\n  AWS_ACCESS_KEY_ID:\n    environment: AWS_ACCESS_KEY_ID\n  AWS_SECRET_ACCESS_KEY:\n    environment: AWS_SECRET_ACCESS_KEY\n  POSTGRES_USER:\n    environment: POSTGRES_USER\n  POSTGRES_PASSWORD:\n    environment: POSTGRES_PASSWORD"
  },
  {
    "objectID": "posts/kedro-mlflow/index.html#tracking-server",
    "href": "posts/kedro-mlflow/index.html#tracking-server",
    "title": "Reproducibility & Scalability Part 3MLFlow Integration",
    "section": "Tracking Server",
    "text": "Tracking Server\nWe are now ready to define the MLFlow tracking-server and configure it so that it can communicate with our backend store and artifact store. The key here is to define the proper environment variables:\n\nMLFLOW_BACKEND_STORE_URI: This will point to our PostgreSQL service which has the form postgresql://username:password@service_name:port/database_name\nMLFLOW_ARTIFACTS_DESTINATION: This will point to our storage bucket which has the form s3://bucket_name\nMLFLOW_S3_ENDPOINT_URL: This will point to our MinIO service and which the form http://service_name:port\nAWS_ACCESS_KEY_ID: This is the access key ID to our MinIO service. It can be the root user (not recommended) or bucket specific access key\nAWS_SECRET_ACCESS_KEY: This is the secret access key ID to our MinIO service. It can be the root password (not recommended) or bucket specific secret access key\n\nLet’s go ahead and add our tracking-server service to our compose file:\n#./docker-compose.yml\nservices:\n  mlflow-logs:\n    image: postgres:16.4\n    secrets:\n      - POSTGRES_USER\n      - POSTGRES_PASSWORD\n    environment:\n      - POSTGRES_USER_FILE=/run/secrets/POSTGRES_USER\n      - POSTGRES_PASSWORD_FILE=/run/secrets/POSTGRES_PASSWORD\n      - POSTGRES_DB=mlflowdb\n    ports:\n      - 5432:5432\n    volumes:\n      - pg_mlflow_db:/var/lib/postgresql/data\n\n  minio:\n    container_name: minio\n    image: 'bitnami/minio:latest'\n    ports:\n      - '9000:9000'\n      - '9001:9001'\n    secrets:\n      - AWS_ACCESS_KEY_ID\n      - AWS_SECRET_ACCESS_KEY\n    environment:\n      - MINIO_ROOT_USER_FILE=/run/secrets/AWS_ACCESS_KEY_ID\n      - MINIO_ROOT_PASSWORD_FILE=/run/secrets/AWS_SECRET_ACCESS_KEY\n      - MINIO_DEFAULT_BUCKETS=mlflow\n    volumes:\n      - minio_data:/bitnami/minio/data\n  # new tracking server service\n  mlflow-tracking-server:\n    image: ghcr.io/mlflow/mlflow:v2.19.0\n    command: &gt;\n      bash -c \"\n          pip install -U pip\n          pip install psycopg2-binary boto3\n          mlflow server --host 0.0.0.0 --port 5000 --workers 1\n      \"\n    ports:\n      - 5050:5000\n    environment:\n      - MLFLOW_BACKEND_STORE_URI=postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@mlflow-logs:5432/mlflowdb\n      - MLFLOW_ARTIFACTS_DESTINATION=s3://mlflow\n      - MLFLOW_S3_ENDPOINT_URL=http://minio:9000\n      - MLFLOW_S3_IGNORE_TLS=true\n      - AWS_ACCESS_KEY_ID\n      - AWS_SECRET_ACCESS_KEY\n    depends_on:\n      - minio\n      - mlflow-logs\n    stop_grace_period: 1s\n\nvolumes:\n  pg_mlflow_db:\n  minio_data:\n\nsecrets:\n  AWS_ACCESS_KEY_ID:\n    environment: AWS_ACCESS_KEY_ID\n  AWS_SECRET_ACCESS_KEY:\n    environment: AWS_SECRET_ACCESS_KEY\n  POSTGRES_USER:\n    environment: POSTGRES_USER\n  POSTGRES_PASSWORD:\n    environment: POSTGRES_PASSWORD\n\n\n\n\n\n\nImportant\n\n\n\nEnvironment variables that aren’t passed in using the /run/secrets method can be viewed by echoing that variable in the container’s shell!"
  },
  {
    "objectID": "posts/kedro-mlflow/index.html#kedro-project-image",
    "href": "posts/kedro-mlflow/index.html#kedro-project-image",
    "title": "Reproducibility & Scalability Part 3MLFlow Integration",
    "section": "Kedro Project Image",
    "text": "Kedro Project Image\nThe final service we will be adding in this section is the image of our Kedro project. We set a profile of manual on this service so that the container isn’t brought up automatically. We do this because it is likely that you want this service to run on a schedule so we just let the scheduler handle bringing the service up. Notice that we set the environment variable MLFLOW_TRACKING_URI to the service name and port that is running our MLFlow tracking server. This will allow us to communicate from our Kedro project service.\n#./docker-compose.yml\nservices:\n  mlflow-logs:\n    image: postgres:16.4\n    secrets:\n      - POSTGRES_USER\n      - POSTGRES_PASSWORD\n    environment:\n      - POSTGRES_USER_FILE=/run/secrets/POSTGRES_USER\n      - POSTGRES_PASSWORD_FILE=/run/secrets/POSTGRES_PASSWORD\n      - POSTGRES_DB=mlflowdb\n    ports:\n      - 5432:5432\n    volumes:\n      - pg_mlflow_db:/var/lib/postgresql/data\n\n  minio:\n    container_name: minio\n    image: 'bitnami/minio:latest'\n    ports:\n      - '9000:9000'\n      - '9001:9001'\n    secrets:\n      - AWS_ACCESS_KEY_ID\n      - AWS_SECRET_ACCESS_KEY\n    environment:\n      - MINIO_ROOT_USER_FILE=/run/secrets/AWS_ACCESS_KEY_ID\n      - MINIO_ROOT_PASSWORD_FILE=/run/secrets/AWS_SECRET_ACCESS_KEY\n      - MINIO_DEFAULT_BUCKETS=mlflow\n    volumes:\n      - minio_data:/bitnami/minio/data\n\n  mlflow-tracking-server:\n    image: ghcr.io/mlflow/mlflow:v2.19.0\n    command: &gt;\n        bash -c \"\n            pip install -U pip\n            pip install psycopg2-binary boto3\n            mlflow server --host 0.0.0.0 --port 5000 --workers 1\n        \"\n    ports:\n      - 5050:5000\n    environment:\n      - MLFLOW_BACKEND_STORE_URI=postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@mlflow-logs:5432/mlflowdb\n      - MLFLOW_ARTIFACTS_DESTINATION=s3://mlflow\n      - MLFLOW_S3_ENDPOINT_URL=http://minio:9000\n      - MLFLOW_S3_IGNORE_TLS=true\n      - AWS_ACCESS_KEY_ID\n      - AWS_SECRET_ACCESS_KEY\n    depends_on:\n      - minio\n      - mlflow-logs\n    stop_grace_period: 1s\n  # New service added\n  climate-brazil:\n    profiles:\n      - manual\n    build: .\n    environment:\n      - MLFLOW_TRACKING_URI=http://mlflow-tracking-server:5000\n    depends_on:\n      - mlflow-tracking-server\n      - minio\n      - mlflow-logs\n\nvolumes:\n  pg_mlflow_db:\n  minio_data:\n\nsecrets:\n  AWS_ACCESS_KEY_ID:\n    environment: AWS_ACCESS_KEY_ID\n  AWS_SECRET_ACCESS_KEY:\n    environment: AWS_SECRET_ACCESS_KEY\n  POSTGRES_USER:\n    environment: POSTGRES_USER\n  POSTGRES_PASSWORD:\n    environment: POSTGRES_PASSWORD"
  },
  {
    "objectID": "posts/kedro-mlflow/index.html#kedro-hooks",
    "href": "posts/kedro-mlflow/index.html#kedro-hooks",
    "title": "Reproducibility & Scalability Part 3MLFlow Integration",
    "section": "Kedro Hooks",
    "text": "Kedro Hooks\nHooks are a great way to add additional functionality to your pipelines at specific points during, before or after execution. Common use cases of hooks are:\n\nInjecting additional logging behavior before/after pipeline/node execution\nValidating input data is in the correct format before executing a node\nDebugging a node/pipeline\nTracking/Profiling resource utilization\nCustomizing load and save methods\n\nWe are going to use hooks to inject additional logging and customize saving methods by logging metrics and parameters, and saving models/artifacts using MLFlow.\nTo define hooks in Kedro we need to create a new file hooks.py inside of /src/climate_brazil/ and create a class that will contain our hook’s logic as it’s methods. The scaffolding will look like this:\n# /src/climate_brazil/hooks.py\nfrom typing import Any\n\nfrom kedro.framework.hooks import hook_impl\n\nclass ModelTrackingHooks:\n    def __init__(self):\n        pass\n\n    @hook_impl\n    def after_node_run(\n        self, node: Node, outputs: dict[str, Any], inputs: dict[str, Any]\n    ) -&gt; None:\n        pass\n\n    @hook_impl\n    def after_pipeline_run(self) -&gt; None:\n        pass\n\n\n\n\n\n\nNote\n\n\n\nA lot of times you don’t actually need to define a __init__() method for you hook class. We will define it in our use case so that we can initialize MLFlow."
  },
  {
    "objectID": "posts/kedro-mlflow/index.html#configure-mlflow",
    "href": "posts/kedro-mlflow/index.html#configure-mlflow",
    "title": "Reproducibility & Scalability Part 3MLFlow Integration",
    "section": "Configure MLFlow",
    "text": "Configure MLFlow\nBefore we start experiment tracking with MLFlow, let’s set up some configuration variables inside of conf/base/parameters_ML.yml. First, let’s have the option to either use or not use MLFlow. Second, artifact logging can use up quite a bit of storage, so let’s make that optional as well. Finally, let’s set an experiment name and tags that will help differentiate experiment runs.\n#conf/base/parameters_ML.yml\nmlflow_config:\n  use_mlflow: true\n  log_artifacts: true\n  experiment_name: climate-brazil\n  tags:\n    model: structural time-series\n    trend_component: Gaussian Process\n    seasonal_component: Fourier"
  },
  {
    "objectID": "posts/kedro-mlflow/index.html#initialize-mlflow",
    "href": "posts/kedro-mlflow/index.html#initialize-mlflow",
    "title": "Reproducibility & Scalability Part 3MLFlow Integration",
    "section": "Initialize MLFlow",
    "text": "Initialize MLFlow\nNow we need to initialze MLFlow whenever we start a run of our pipeline. As a reminder we are going to do that using the Kedro hook we defined scaffolding for earlier.\n# /src/climate_brazil/hooks.py\nfrom typing import Any\n\nfrom kedro.framework.hooks import hook_impl\n\nclass ModelTrackingHooks:\n    def __init__(self):\n1        self.run_ids = {}\n2        conf_path = \"./conf\"\n        conf_loader = OmegaConfigLoader(conf_source=conf_path)\n        self.params = conf_loader.get('parameters')\n3        if self.params['mlflow_config']['use_mlflow']:\n            # set the experiment and start an MLFlow run\n            mlflow.set_experiment(\n                experiment_name=self.params['mlflow_config']['experiment_name']\n            )\n            mlflow.start_run()\n            # Save the run ID so that we can nest in the individual city models\n4            self.run_ids[\"parent_run_id\"] = mlflow.active_run().info.run_id\n            # If you have tags set for the run set them in MLFlow\n5            if self.params['mlflow_config']['tags']:\n                mlflow.set_tags(self.params['mlflow_config']['tags'])\n\n1\n\nInitialize a dictionary to store run IDs\n\n2\n\nLoad configurations we defined from inside our parameters files\n\n3\n\nIf we are using MLFlow set the experiment name we defined and start a run\n\n4\n\nStore the run ID inside the dictionary object we initialized earlier\n\n5\n\nIf tags are defined set them to the current run\n\n\n\n\n\n\n\n\nTip\n\n\n\nYou can click on the above circled numbers to highlight the corresponding lines in the code cell directly above them."
  },
  {
    "objectID": "posts/kedro-mlflow/index.html#tracking-prior-sampler-configurations",
    "href": "posts/kedro-mlflow/index.html#tracking-prior-sampler-configurations",
    "title": "Reproducibility & Scalability Part 3MLFlow Integration",
    "section": "Tracking Prior & Sampler Configurations",
    "text": "Tracking Prior & Sampler Configurations\nAs soon as we start an MLFlow run we can start logging the parameters for our priors and our sampler configurations that we defined in /conf/base/parameters_ML.yml. Let’s go ahead and log those with MLFlow inside our __init__() method:\n# /src/climate_brazil/hooks.py\nfrom typing import Any\n\nimport mlflow\nfrom kedro.framework.hooks import hook_impl\n\nclass ModelTrackingHooks:\n    def __init__(self):\n        self.run_ids = {}\n        conf_path = \"./conf\"\n        conf_loader = OmegaConfigLoader(conf_source=conf_path)\n        self.params = conf_loader.get('parameters')\n        if self.params['mlflow_config']['use_mlflow']:\n            # set the experiment and start an MLFlow run\n            mlflow.set_experiment(\n                experiment_name=self.params['mlflow_config']['experiment_name']\n            )\n            mlflow.start_run()\n            # Save the run ID so that we can nest in the individual city models\n            self.run_ids[\"parent_run_id\"] = mlflow.active_run().info.run_id\n            # We can log our model and sampler configs early\n1            mlflow.log_params(self.params['model_config'])\n2            mlflow.log_params(self.params['sampler_config'])\n            # If you have tags set for the run set them in MLFlow\n            if self.params['mlflow_config']['tags']:\n                mlflow.set_tags(self.params['mlflow_config']['tags'])\n\n1\n\nWe log our model configuration, which if you recall, holds the parameters for our priors\n\n2\n\nHere we log our sampler configurations. This is useful to log for when we have divergences, our sampling is slow, or when our effective sampling size is too small."
  },
  {
    "objectID": "posts/kedro-mlflow/index.html#tracking-model-specifications",
    "href": "posts/kedro-mlflow/index.html#tracking-model-specifications",
    "title": "Reproducibility & Scalability Part 3MLFlow Integration",
    "section": "Tracking Model Specifications",
    "text": "Tracking Model Specifications\nPyMC allows you to plot a graphical representation of the model using GraphViz. These plots can facilitate comparing different model specifications across experiment runs without needing to read through code. So, we will generate these figures and log them as artifacts. It’s also a good idea to define one run of our experiment as running through all of the cities. So, we will create nested runs for each city within our top-level parent run.\n# /src/climate_brazil/hooks.py\nfrom typing import Any\n\nimport mlflow\nfrom kedro.framework.hooks import hook_impl\n\ncities = [\n    \"Belem\", \"Curitiba\", \"Fortaleza\", \"Goiania\", \"Macapa\", \"Manaus\", \n    \"Recife\", \"Rio\", \"Salvador\", \"Sao_Luiz\", \"Sao_Paulo\", \"Vitoria\"\n]\n\nclass ModelTrackingHooks:\n    def __init__(self):\n        self.run_ids = {}\n        conf_path = \"./conf\"\n        conf_loader = OmegaConfigLoader(conf_source=conf_path)\n        self.params = conf_loader.get('parameters')\n        if self.params['mlflow_config']['use_mlflow']:\n            # set the experiment and start an MLFlow run\n            mlflow.set_experiment(\n                experiment_name=self.params['mlflow_config']['experiment_name']\n            )\n            mlflow.start_run()\n            # Save the run ID so that we can nest in the individual city models\n            self.run_ids[\"parent_run_id\"] = mlflow.active_run().info.run_id\n            # We can log our model and sampler configs early\n            mlflow.log_params(self.params['model_config'])\n            mlflow.log_params(self.params['sampler_config'])\n            # If you have tags set for the run set them in MLFlow\n            if self.params['mlflow_config']['tags']:\n                mlflow.set_tags(self.params['mlflow_config']['tags'])\n\n1    @hook_impl\n    def after_node_run(\n        self, node: Node, outputs: dict[str, Any], inputs: dict[str, Any]\n    ) -&gt; None:\n        \"\"\"\n        Here we are going to pull outputs from specific nodes and log them with MLFlow\n        ---\n        Params:\n            node: Attributes of the node that just ran\n            outputs: Outputs of the node that just ran\n            inputes: Inputes of the node that just ran\n        \"\"\"\n2        if node.name == \"train\":\n3            for city in cities:\n                if self.params['mlflow_config']['use_mlflow']:\n                    # Start a nested run\n4                    mlflow.start_run(\n                        run_name=city,\n                        nested=True,\n                        parent_run_id=self.run_ids['parent_run_id']\n                    )\n                    # Store city specific run ids for later\n5                    self.run_ids[f\"{city}_run_id\"] = mlflow.active_run().info.run_id\n                    # If you want to log artifacts log them here\n6                    if self.params['mlflow_config']['log_artifacts']:\n                        local_path = f\"./data/08_reporting/{city}_model_graph.png\"\n                        outputs[f\"{city}_model\"].model.to_graphviz(\n                            save=local_path, \n                            figsize=(12,8)\n                        )\n                        # log graph representation of model\n                        mlflow.log_artifact(local_path=local_path, artifact_path=\"figures\")\n                        # Define the inputs the model expects when forecasting\n7                    mlflow.end_run()\n\n1\n\nDefine a hook that will run after a Kedro node completes execution.\n\n2\n\nIf the node that completes is named train then fire the hook.\n\n3\n\nFor each city prepare to log city specific attributes.\n\n4\n\nStart a nested run within our current active run.\n\n5\n\nStore the nested run ID.\n\n6\n\nCreate the graphical representation of the model and log it as an artifact in our artifact store under the directory of figures/\n\n7\n\nClose the active nested run."
  },
  {
    "objectID": "posts/kedro-mlflow/index.html#logging-divergences-and-infrence-data-attributes",
    "href": "posts/kedro-mlflow/index.html#logging-divergences-and-infrence-data-attributes",
    "title": "Reproducibility & Scalability Part 3MLFlow Integration",
    "section": "Logging Divergences and Infrence Data Attributes",
    "text": "Logging Divergences and Infrence Data Attributes\nIt is also useful to log any divergences you come across during sampling because as you fit more and more models it can be difficult to keep track of any sampling problems. We will also log our Inference Data attributes which will include the model ID that ModelBuilder gives a new instance of our model, the city name, and the mean and standard deviation of our average temperatures for each city.\n# /src/climate_brazil/hooks.py\nfrom typing import Any\n\nimport mlflow\nfrom kedro.framework.hooks import hook_impl\n\ncities = [\n    \"Belem\", \"Curitiba\", \"Fortaleza\", \"Goiania\", \"Macapa\", \"Manaus\", \n    \"Recife\", \"Rio\", \"Salvador\", \"Sao_Luiz\", \"Sao_Paulo\", \"Vitoria\"\n]\n\nclass ModelTrackingHooks:\n    def __init__(self):\n        self.run_ids = {}\n        conf_path = \"./conf\"\n        conf_loader = OmegaConfigLoader(conf_source=conf_path)\n        self.params = conf_loader.get('parameters')\n        if self.params['mlflow_config']['use_mlflow']:\n            # set the experiment and start an MLFlow run\n            mlflow.set_experiment(\n                experiment_name=self.params['mlflow_config']['experiment_name']\n            )\n            mlflow.start_run()\n            # Save the run ID so that we can nest in the individual city models\n            self.run_ids[\"parent_run_id\"] = mlflow.active_run().info.run_id\n            # We can log our model and sampler configs early\n            mlflow.log_params(self.params['model_config'])\n            mlflow.log_params(self.params['sampler_config'])\n            # If you have tags set for the run set them in MLFlow\n            if self.params['mlflow_config']['tags']:\n                mlflow.set_tags(self.params['mlflow_config']['tags'])\n\n    @hook_impl\n    def after_node_run(\n        self, node: Node, outputs: dict[str, Any], inputs: dict[str, Any]\n    ) -&gt; None:\n        \"\"\"\n        Here we are going to pull outputs from specific nodes and log them with MLFlow\n        ---\n        Params:\n            node: Attributes of the node that just ran\n            outputs: Outputs of the node that just ran\n            inputes: Inputes of the node that just ran\n        \"\"\"\n        if node.name == \"train\":\n            for city in cities:\n                if self.params['mlflow_config']['use_mlflow']:\n                    # Start a nested run\n                    mlflow.start_run(\n                        run_name=city, \n                        nested=True, \n                        parent_run_id=self.run_ids['parent_run_id']\n                    )\n                    # Store city specific run ids for later\n                    self.run_ids[f\"{city}_run_id\"] = mlflow.active_run().info.run_id\n                    # If you want to log artifacts log them here\n                    if self.params['mlflow_config']['log_artifacts']:\n                        local_path = f\"./data/08_reporting/{city}_model_graph.png\"\n                        outputs[f\"{city}_model\"].model.to_graphviz(\n                            save=local_path, \n                            figsize=(12,8)\n                        )\n                        # log graph representation of model\n                        mlflow.log_artifact(local_path=local_path, artifact_path=\"figures\")\n                    # log divergences and inference data attributes\n1                    mlflow.log_param(\n                        \"divergences\",\n                        (\n                            outputs[f\"{city}_model\"]\n                              .idata.sample_stats.diverging.sum()\n                              .values.item()\n                        )\n                    )\n2                    mlflow.log_params(outputs[f\"{city}_model\"].idata.attrs)\n                    mlflow.end_run()\n\n1\n\nLog divergences if any occured during sampling.\n\n2\n\nLog Inference Data attributes."
  },
  {
    "objectID": "posts/kedro-mlflow/index.html#logging-metrics",
    "href": "posts/kedro-mlflow/index.html#logging-metrics",
    "title": "Reproducibility & Scalability Part 3MLFlow Integration",
    "section": "Logging Metrics",
    "text": "Logging Metrics\nObviously, we would want to log some metrics that give us an indication of how well our model is performing. In the previous post, we decided to evaluate our model using the root mean square error (RMSE) and the 80% Highest Density Interval (HDI) coverage. Let’s go ahead and log those metrics.\n# /src/climate_brazil/hooks.py\nfrom typing import Any\n\nimport mlflow\nfrom kedro.framework.hooks import hook_impl\n\ncities = [\n    \"Belem\", \"Curitiba\", \"Fortaleza\", \"Goiania\", \"Macapa\", \"Manaus\", \n    \"Recife\", \"Rio\", \"Salvador\", \"Sao_Luiz\", \"Sao_Paulo\", \"Vitoria\"\n]\n\nclass ModelTrackingHooks:\n    def __init__(self):\n        self.run_ids = {}\n        conf_path = \"./conf\"\n        conf_loader = OmegaConfigLoader(conf_source=conf_path)\n        self.params = conf_loader.get('parameters')\n        if self.params['mlflow_config']['use_mlflow']:\n            # set the experiment and start an MLFlow run\n            mlflow.set_experiment(\n                experiment_name=self.params['mlflow_config']['experiment_name']\n            )\n            mlflow.start_run()\n            # Save the run ID so that we can nest in the individual city models\n            self.run_ids[\"parent_run_id\"] = mlflow.active_run().info.run_id\n            # We can log our model and sampler configs early\n            mlflow.log_params(self.params['model_config'])\n            mlflow.log_params(self.params['sampler_config'])\n            # If you have tags set for the run set them in MLFlow\n            if self.params['mlflow_config']['tags']:\n                mlflow.set_tags(self.params['mlflow_config']['tags'])\n\n    @hook_impl\n    def after_node_run(\n        self, node: Node, outputs: dict[str, Any], inputs: dict[str, Any]\n    ) -&gt; None:\n        \"\"\"\n        Here we are going to pull outputs from specific nodes and log them with MLFlow\n        ---\n        Params:\n            node: Attributes of the node that just ran\n            outputs: Outputs of the node that just ran\n            inputes: Inputes of the node that just ran\n        \"\"\"\n        if node.name == \"train\":\n            for city in cities:\n                if self.params['mlflow_config']['use_mlflow']:\n                    # Start a nested run\n                    mlflow.start_run(\n                        run_name=city, \n                        nested=True, \n                        parent_run_id=self.run_ids['parent_run_id']\n                    )\n                    # Store city specific run ids for later\n                    self.run_ids[f\"{city}_run_id\"] = mlflow.active_run().info.run_id\n                    # If you want to log artifacts log them here\n                    if self.params['mlflow_config']['log_artifacts']:\n                        local_path = f\"./data/08_reporting/{city}_model_graph.png\"\n                        outputs[f\"{city}_model\"].model.to_graphviz(\n                            save=local_path, \n                            figsize=(12,8)\n                        )\n                        # log graph representation of model\n                        mlflow.log_artifact(local_path=local_path, artifact_path=\"figures\")\n                    # log divergences and inference data attributes\n                    mlflow.log_param(\n                        \"divergences\", \n                        (\n                            outputs[f\"{city}_model\"]\n                              .idata.sample_stats.diverging.sum()\n                              .values.item()\n                        )\n                    )\n                    mlflow.log_params(outputs[f\"{city}_model\"].idata.attrs)\n                    mlflow.end_run()\n\n1        if node.name in [f\"{city}_forecasts_evaluation\" for city in cities]:\n2            city = re.search(r\"(.*)(?=_forecasts_evaluation)\", node.name).group(1)\n            if self.params['mlflow_config']['use_mlflow']: \n                # start up again our city specific runs to log metrics\n3                mlflow.start_run(\n                    run_id=self.run_ids[f\"{city}_run_id\"],\n                    run_name=city,\n                    nested=True,\n                    parent_run_id=self.run_ids['parent_run_id']\n                )\n4                mlflow.log_metrics(outputs[f'{city}_evaluation'])\n5                mlflow.end_run()\n\n1\n\nExecute the hook only if the node corresponds to our evaluation nodes.\n\n2\n\nGrab the city name using Regex.\n\n3\n\nRe-activate the nested run corresponding to the city.\n\n4\n\nLog the RMSE and coverage metrics\n\n5\n\nClose the nested city run again."
  },
  {
    "objectID": "posts/kedro-mlflow/index.html#saving-a-custom-mlflow-model",
    "href": "posts/kedro-mlflow/index.html#saving-a-custom-mlflow-model",
    "title": "Reproducibility & Scalability Part 3MLFlow Integration",
    "section": "Saving a Custom MLFlow Model",
    "text": "Saving a Custom MLFlow Model\nMLFlow has several flavors of model frameworks already built-in that allow you to save your trained model into an easily deployable MLFlow model object. Behind the scenes MLFlow defines how the model should be loaded and how predictions are to be made using the model. Since we have built a custom solution using the Probabilistic Programming Language (PPL) PyMC we need to tell MLFlow how to load and use our model during forecasting time.\n# src/climate_brazil/pipelines/ML/mlflow_model_wrapper.py\nimport json\n\nfrom mlflow.pyfunc import Context, PythonModel\n\nfrom kedro_framework.pipelines.ML.ts_model import TSModel\n\nclass PyMCModelWrapper(PythonModel):\n1    def load_context(self, context: Context) -&gt; None:\n        \"\"\"\n        Loads the trained model from the given context.\n        ---\n        Params:\n            context: The context object containing artifacts, including the model.\n        \"\"\"\n        self.model = TSModel.load(context.artifacts['model'])\n\n2    def predict(self, context: Context, n_ahead: int) -&gt; str:\n        \"\"\"\n        Makes predictions using the model for the specified number of time steps ahead.\n        ---\n        Params:\n            context: The context object containing artifacts, including the model.\n            n_ahead: The number of future time steps to predict.\n        \"\"\"\n        preds_normalized = self.model.sample_posterior_predictive(\n            n_ahead=n_ahead.iloc[:, 0].values.item(), \n            extend_idata=False, \n            combined=False\n        )\n        preds = preds_normalized * self.model.idata.attrs['y_std'] + \n                self.model.idata.attrs['y_mean']\n        preds = preds.rename_vars(\n            {\"temperature_normalized_fut\": \"temperature_fut\"}\n        )\n        return json.dumps(preds.to_dataframe().reset_index().to_dict('records'))\n\n1\n\nLoad the trained model\n\n2\n\nGenerate forecasts using the trained model\n\n\nNow that we can log our model with MLFlow let’s add that logic to our hook:\n# /src/climate_brazil/hooks.py\nfrom typing import Any\n\nimport mlflow\nfrom kedro.framework.hooks import hook_impl\n\ncities = [\n    \"Belem\", \"Curitiba\", \"Fortaleza\", \"Goiania\", \"Macapa\", \"Manaus\", \n    \"Recife\", \"Rio\", \"Salvador\", \"Sao_Luiz\", \"Sao_Paulo\", \"Vitoria\"\n]\n\nclass ModelTrackingHooks:\n    def __init__(self):\n        self.run_ids = {}\n        conf_path = \"./conf\"\n        conf_loader = OmegaConfigLoader(conf_source=conf_path)\n        self.params = conf_loader.get('parameters')\n        if self.params['mlflow_config']['use_mlflow']:\n            # set the experiment and start an MLFlow run\n            mlflow.set_experiment(\n                experiment_name=self.params['mlflow_config']['experiment_name']\n            )\n            mlflow.start_run()\n            # Save the run ID so that we can nest in the individual city models\n            self.run_ids[\"parent_run_id\"] = mlflow.active_run().info.run_id\n            # We can log our model and sampler configs early\n            mlflow.log_params(self.params['model_config'])\n            mlflow.log_params(self.params['sampler_config'])\n            # If you have tags set for the run set them in MLFlow\n            if self.params['mlflow_config']['tags']:\n                mlflow.set_tags(self.params['mlflow_config']['tags'])\n\n    @hook_impl\n    def after_node_run(\n        self, node: Node, outputs: dict[str, Any], inputs: dict[str, Any]\n    ) -&gt; None:\n        \"\"\"\n        Here we are going to pull outputs from specific nodes and log them with MLFlow\n        ---\n        Params:\n            node: Attributes of the node that just ran\n            outputs: Outputs of the node that just ran\n            inputes: Inputes of the node that just ran\n        \"\"\"\n        if node.name == \"train\":\n            for city in cities:\n                if self.params['mlflow_config']['use_mlflow']:\n                    # Start a nested run\n                    mlflow.start_run(\n                        run_name=city, \n                        nested=True, \n                        parent_run_id=self.run_ids['parent_run_id']\n                    )\n                    # Store city specific run ids for later\n                    self.run_ids[f\"{city}_run_id\"] = mlflow.active_run().info.run_id\n                    # If you want to log artifacts log them here\n                    if self.params['mlflow_config']['log_artifacts']:\n                        local_path = f\"./data/08_reporting/{city}_model_graph.png\"\n                        outputs[f\"{city}_model\"].model.to_graphviz(\n                            save=local_path, \n                            figsize=(12,8)\n                        )\n                        # log graph representation of model\n                        mlflow.log_artifact(local_path=local_path, artifact_path=\"figures\")\n\n                        # Define the inputs the model expects when forecasting\n1                        input_schema = mlflow.types.Schema(\n                            [\n                                mlflow.types.ColSpec(\n                                    name=\"n_ahead\",\n                                    type=mlflow.types.DataType.integer\n                                ),\n                            ]\n                        )\n\n                        # Log the model\n2                        with tempfile.TemporaryDirectory() as tmpdir:\n                            file_path = os.path.join(tmpdir, f\"{city}_model.nc\")\n                            outputs[f\"{city}_model\"].save(file_path)\n                            mlflow.pyfunc.log_model(\n                                artifact_path=\"model\",\n                                python_model=PyMCModelWrapper(),\n                                artifacts={\"model\": file_path},\n                                signature=ModelSignature(inputs=input_schema),\n                                conda_env=\"./environment.yml\"\n                            )\n                    # log divergences and inference data attributes\n                    mlflow.log_param(\n                        \"divergences\", \n                        (\n                            outputs[f\"{city}_model\"]\n                              .idata.sample_stats.diverging.sum()\n                              .values.item()\n                        )\n                    )\n                    mlflow.log_params(outputs[f\"{city}_model\"].idata.attrs)\n                    mlflow.end_run()\n\n        if node.name in [f\"{city}_forecasts_evaluation\" for city in cities]:\n            city = re.search(r\"(.*)(?=_forecasts_evaluation)\", node.name).group(1)\n            if self.params['mlflow_config']['use_mlflow']: \n                # start up again our city specific runs to log metrics\n                mlflow.start_run(\n                    run_id=self.run_ids[f\"{city}_run_id\"],\n                    run_name=city,\n                    nested=True,\n                    parent_run_id=self.run_ids['parent_run_id']\n                )\n                mlflow.log_metrics(outputs[f'{city}_evaluation'])\n                mlflow.end_run()\n        \n3    @hook_impl\n    def after_pipeline_run(self) -&gt; None:\n        \"\"\"Hook implementation to end the MLflow run\n        after the Kedro pipeline finishes.\n        \"\"\"\n        if mlflow.active_run():\n            mlflow.end_run()\n\n1\n\nDefine the input schema that our model will expect during forecasting\n\n2\n\nLog the model under /model with the defined input schema and dependency requirements defined within environment.yml\n\n3\n\nAfter the pipeline executes close the MLFlow active run"
  },
  {
    "objectID": "posts/kedro-mlflow/index.html#activating-hooks",
    "href": "posts/kedro-mlflow/index.html#activating-hooks",
    "title": "Reproducibility & Scalability Part 3MLFlow Integration",
    "section": "Activating Hooks",
    "text": "Activating Hooks\nIt is quite simple to activate a Kedro hook. We just need to import our hook class into /src/climate_brazil/settings.py and add the to the file the line:\n# /src/climate_brazil/settings.py\n# Hooks are executed in a Last-In-First-Out (LIFO) order.\nfrom climate_brazil.hooks import ModelTrackingHooks\nHOOKS = (ModelTrackingHooks(),)"
  },
  {
    "objectID": "posts/kedro-mlflow/index.html#executing-our-pipeline",
    "href": "posts/kedro-mlflow/index.html#executing-our-pipeline",
    "title": "Reproducibility & Scalability Part 3MLFlow Integration",
    "section": "Executing our pipeline",
    "text": "Executing our pipeline\nOkay, it is showtime! Let’s re-build our pipeline image and run our pipeline.\ndocker compose build climate-brazil\ndocker compose up -d\ndocker compose up -d climate-brazil\n\n\n\n\n\n\nNote\n\n\n\nRemember that our pipeline service has a profile set to manual. We need to explicitly bring up the service.\n\n\nIf you checkout the MLFlow User Interface (UI) running on localhost:5050, you should see something like this:\n\n\n\nMLFlow User Interface"
  },
  {
    "objectID": "posts/kedro-mlflow/index.html#parameters-metrics",
    "href": "posts/kedro-mlflow/index.html#parameters-metrics",
    "title": "Reproducibility & Scalability Part 3MLFlow Integration",
    "section": "Parameters & Metrics",
    "text": "Parameters & Metrics\nWe can see more details about our nested runs by clicking on them in the UI. Here you can see both the parameters and metrics that we logged for the Brazilian city Belem.\n\n\n\nCity Specific Parameters & Metrics"
  },
  {
    "objectID": "posts/kedro-mlflow/index.html#artifacts",
    "href": "posts/kedro-mlflow/index.html#artifacts",
    "title": "Reproducibility & Scalability Part 3MLFlow Integration",
    "section": "Artifacts",
    "text": "Artifacts\nFinally, navigating to the artifacts tab in the UI we see the logged model, under /model, along with the expected input and some sample code to validate it produces the expected output.\n\n\n\nCity Specific Artifacts\n\n\nUnder the /figures path, we can find the model graph figure that we logged.\n\n\n\nCity Specific Model Graph\n\n\nThe artifacts are visible/accesible via the MLFlow UI, but remember that these objects are stored in our MinIO service. You can access the storage directly on localhost:9001 and locate the objects in our default bucket:\n\n\n\nMinIO Artifact Storage"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to My Blog",
    "section": "",
    "text": "How Machines Comprehend LanguageIntroduction to Vector Embeddings\n\n\n\nNatural Language Processing\n\n\n\n\n\n\n\n\n\nJun 21, 2025\n\n\nJonathan Dekermanjian\n\n\n\n\n\n\n\n\n\n\n\n\nReproducibility & Scalability Part 4Scaling With Ray\n\n\n\nEngineering\n\n\n\n\n\n\n\n\n\nApr 17, 2025\n\n\nJonathan Dekermanjian\n\n\n\n\n\n\n\n\n\n\n\n\nReproducibility & Scalability Part 3MLFlow Integration\n\n\n\nEngineering\n\n\n\n\n\n\n\n\n\nApr 12, 2025\n\n\nJonathan Dekermanjian\n\n\n\n\n\n\n\n\n\n\n\n\nReproducibility & Scalability Part 2PyMC ModelBuilder\n\n\n\nEngineering\n\nAnalytics\n\n\n\n\n\n\n\n\n\nApr 2, 2025\n\n\nJonathan Dekermanjian\n\n\n\n\n\n\n\n\n\n\n\n\nReproducibility & Scalability Part 1The Kedro Framework\n\n\n\nEngineering\n\n\n\n\n\n\n\n\n\nMar 26, 2025\n\n\nJonathan Dekermanjian\n\n\n\n\n\nNo matching items"
  }
]